{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57iFV6-0N99k"
      },
      "outputs": [],
      "source": [
        "!pip install modelscope addict aiohttp requests tqdm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "86b8cdba72c0487487691e7bf541b1da",
            "c787dce02b5f406692bc483d867f1b9d",
            "edd173faeff74ad283b6b7373b9c391e",
            "3d52615582bb4accbaccf10b62388699",
            "eab29b89038e497ea1782cb05638243b",
            "33785dea9e2142078e20af4cb7430256",
            "7d0ca61de917451fa1f0b9c8372943c4",
            "7e734d02abd249e781e687b7cce75c4b",
            "5592bcbe6c614971a65754d9bd371fc7",
            "5e3b6a79621c48d48167036b3f7b42b9",
            "6ade4aff31f54202b82204a185a6627f",
            "16d17455c28049ae8a00661e4cb400f9",
            "d7001087bf05439182343a1927ca553a",
            "93aebab0e618451da2d89b13722cc4f9",
            "721383979a7c45399264d6439a6b3639",
            "b4766cdd10724ac3826f4caf25c40099",
            "d92c221d8c91439793757d4f04012b2e",
            "2c1f53b43ffa44b492ccac077d69f74a",
            "3210c647eca74de7b36817b751c5c28a",
            "19d8e43fbd8041f3967bf8fbb1211a82",
            "0cbf4452135f4c1db96f27ed051b7346",
            "4a31ef1c2129427a807a1b5639f8eaa7",
            "6fedca28a9bd441ca9c68265fef30ba8",
            "e719034506984aee8a365a98dc55ed6f",
            "e3076e36d87c45d48ee719db0ef56b92",
            "61069e89dada4e3399f8e30b365b4877",
            "edd2c66315c44c4bb496283dc69dfe06",
            "a063d9706a1c4e2caa09eece741462d0",
            "dd1f47f01e17425c8c16dfaa774bd412",
            "0fd196ad3cc64212a310a52ba497d5fb",
            "fe6e414b5596422c97771e3212cb9f8c",
            "0a6f5088348e428594e0c6e0f1439d21",
            "e59a905deeee4b12bafe99a934e214e8",
            "1773390e287247c7a11bba965974b737",
            "fe2911ab94654deab8eefa851a84d992",
            "5396cb9938d946d6a3e911c0dcf1c38c",
            "aaecebc7516f4767a096969e33879d18",
            "26199fbc942441da9324ad112cca4d34",
            "cad8999a891d4311bc8e99be3999c01b",
            "c2ae98e8c1ec40cb96850dd0ecd2be9b",
            "a3706210875742a392e5507e973dd029",
            "472e685d98b74fb68274eb2c893b386d",
            "09d715bc644949f79c78ea3a663a2176",
            "3c4766483ab74a3baa08eba6ac03d65c",
            "d054666105634112a70bd3aa59713a36",
            "549a8b0d237b49e980984ee1a09a9f08",
            "17b9dbe38e1741308ffa09dba84a7861",
            "65a4d32d4f5f4171b4baffa9fe042e1a",
            "7158f9ded03248e08251c11beedc0d95",
            "39d766177faf4f68bedee1bdf9b7d5d8",
            "5ee6ff0aec2f490781066c0cab49f16b",
            "6241aedf9e10400ca80a6028829f0fd3",
            "71bb93edc2ae4a0aa239175773e40047",
            "6ed5f0792a0e40d996dae340ecf21f46",
            "3f70355aa35a4067ae35cd6d3f272dc8",
            "8ac39827c83040fe86552c7e6dbcd488",
            "490fa60f449d4a53a7daa8faf04cdeb4",
            "a49580a7d86c43e5a113fbbb5961c075",
            "5192a52a52214369b17cc706dbc9f94e",
            "012bfec4c75f4e45917e35934a70793c",
            "d84b4bf0fdbd44878afd843ab639cfd2",
            "ac2952f5d24245789649586fda6d3285",
            "35ef6876004e47fbb0d43cc8306000e9",
            "2473253d855d4bff885e5994f7dc11b5",
            "1759c96001a94b0c9b2c704763e25763",
            "0e90ca79a4c6466bafb46c2850cf4ed0",
            "b0fc373b7a914d87b472958fcd0cb91f",
            "7e448796fd0c401d9b2d6bcc61d189e1",
            "40b50e03915142c2b68f9ddc6d12ad21",
            "7b8e7b3ca0f34eb89eb8fbcbbb409169",
            "d28e4c0863204f5296604e016af9418d",
            "f73cf3d9c0d745ab8b172dc68868a946",
            "fe692706abbb4c4d90166714db6b388f",
            "bdd4746cb35a4e1fa3f1ed0d896ba415",
            "b50f14fa80b3428f9951007d60c9c41d",
            "6ebe014bbf704c31b2aa1cd38b4eb39a",
            "5e4e91713bb74892be7320691ded1773",
            "2e52ef4addf3479d96baabe991237861",
            "519a89157ae34414b23ba151c3be0d0b",
            "c6bd9b2c59884fb79c33a9eb08fefe05",
            "45035a0f58ab45cabc2729a38d8d7933",
            "dc1edb786a4b43fcbd8963ffd9c95757",
            "adf4047907c44fdaab9283e6f191a8ab",
            "5c959cff98944fcba99f3a720fadc656",
            "c9d8e2c35a03420cb285e7cefe8cce1d",
            "2f431d07e04c4de98c19ce92553defd8",
            "462087b584f742da84800f42f72638fc",
            "13babc9a8a2242519f660d9c9b232f11",
            "572a4b3599c441cd9376fc4b1bbb01f9",
            "0655360d0d60492dbbba1e8df3c5dc9d",
            "8342d04350b14f10aea8baeabffef4dd",
            "7c198bc26b0b4167b1cfd7bccb5e6070",
            "0da1fbd111e14c95ad09d0d062d9e0f1",
            "39e1a279c1da41e8a81d65c3ca2812b7",
            "e47062a0f9da44cdb8071a7f5c7a1c3c",
            "f41fa98825f841a98b5e4bdd8fd9e8c5",
            "c459c0bef2e1408a8437c16300c1d0a9",
            "a8e2383c7d6046a4bd4afadb033d3d45",
            "c9e84972fdc54df4b1615a30e2734447",
            "41f87dec136b4891bd00d4bafe09e4b8",
            "04ff58fbd51e4335bf0fccd7bf778e03",
            "d3bcdc103bb646678383751221c6dfde",
            "3beb70d3bbe842739774339225d2a42c",
            "4404c8895d9240a288f0a4c316ee83a6",
            "9b8b976a237c466ba617da7a5e616955",
            "dd121d51834c4adf94fff006c0922817",
            "7606b668154b498cb6a14faf520d1489",
            "4321ea6fb103495ea155e50adb4e79a1",
            "d9e012111d634802b9c654cb09aab716",
            "93fa188741784837be98c7cb79e8b377",
            "f1c7f7183465492d803e47b58b59a3c7",
            "349a2cd6777940c5806354c6cefa1f49",
            "732f4c0d8ed0485caab688a89bed4fd7",
            "b374cc48c3574d27b03e02fc5250d047",
            "3198cbacd8a9400680ee4a9e138b9721",
            "e7f8c93897044bad874a79494d296647",
            "3d0817013e5049ab8eac2e46f52766ac",
            "91bd3462fbc34a4e8a1dc226a0f6dc99",
            "bd4db3a38ef343838896e1440c1ec976",
            "7ee52f2ebfc94b638b1b443c6c284b06",
            "4e755447389744658b4f44c518069eed",
            "f713aefb8c2f40f7abe60e0ab4b2068d",
            "6168a184cc8a4771812f0e3b4ed08687",
            "dd402f789cb54479ae12f347fa4bc2ae",
            "a628453aa18d4053baca7eebcdb7009b",
            "20fe5430471d4eb39a4e0d03530e0079",
            "9d8c0134c0ac4d568e6215c71f485312",
            "6bc1ec17defa431cb1be6b2857b21cc1",
            "fad6ac6f264e4ebbb6ffd856a00d4043",
            "74e83d3ea80748f4916ab91bf6e9fdb2",
            "1d9d112daa4a4ec58f3980518518f394",
            "af9d0b21066c43fd993b8d5bde261c02",
            "8eff4035a1e747e6bbc7e37c408e699b",
            "696c607a088e4d8a924dd7e7a9c808f7",
            "7d0e29dced1b4befbeec87d8398e2b9e",
            "674d2320910d4b8cb54085f8229e77c9",
            "e57b5ef904584d758b82f6bba122b9b8",
            "c22317f7712b4efaac0daadf68535c72",
            "2ab99f52c5bb4af6ad473104994f0f80",
            "b7bf4d7b392946a7aa0fa3ced1ff5043",
            "6870473e7db44da7a950b2b2a1b11de8",
            "ae958630f99647df817229d15ae09239",
            "2b0d207e58204933afe05f227a86fd04",
            "439d2860d0bb487fb9ce58f41b8ef819",
            "bec8233b54db429399b930d408fe8804",
            "80e5368351ad47dbb6629cd584e05492",
            "89ca8fb1b43e43fca2d1261b976e0b85",
            "0379a703b8d1426e9927268728b3f96e",
            "b455ce518b02401d95c2dfed5ef14810",
            "ff0e1feee78e41c7a8667f559ead63df",
            "a76b4e8438754885ad7c99327a6749b9",
            "bf07a0b8bbb44796b3aaffdafef8a9cb",
            "aa0804b6a8864dcc9a3d8f57ee80ce20",
            "4b0a1043763045269b43b29552d7a71c",
            "0d9cccefa60c449689e9cabd04857fab",
            "a2994e2991c44db48cf94852b22b1142",
            "0e77ea7d5ec7499a9c4d007aa3b6efa9",
            "90ce8df4723b4a0f9f3b68ab92beef2f",
            "7e067139bacc4f05a7573610abe098f3",
            "48f598586ab94ec49003ba4e92bc8481",
            "d66f62907aa045a8afcbc88fbf7a19d8",
            "eb1b45c3e48c46aa92844961abd15913",
            "adca45cd41014108b0d1c2585d6dbfea",
            "121deeca9bb546d28224240a3d1a6dc1",
            "803dc72b746547d1823f7d9d17ccf8fd",
            "52d0960525114c0391be1eae1fee89a7",
            "d6d24c4617174a1784e2226d86489949",
            "a8403440ef3c42e19e7b7feb11eedd2d",
            "c55c9b6114974f0a8187f30b938d247c",
            "4aff4aeaa89c4d8580f25d7898cf242f",
            "65bfc9e3caec48468940d297b1b32163",
            "aca35e59e80f4c7f9e8b52a4d63adf1b",
            "8ef87f87155440d58d8d81b63abef220",
            "af35af701f7742adbdb872701b802911",
            "5e2d8f90e361410f91431e7013a6f874",
            "784a48bb43514f3cb13325f2cbb330b6",
            "bdfba7a5d6ef4028972097fcc5be8fff",
            "fd0800131eac4e96a03c2a7bc1f3211e",
            "5cc7e66bddf54993b60ee4e6814d14cd",
            "c8979407dc4849bfb94475a8e754311c",
            "c2186525a96b456b9996e1c88f6b76ca",
            "970013d0191f4451ac109cf222c58c21",
            "12cb178f70fe43108dddfcf88bcd1329",
            "85bb7c017edc43f2837999e17492bd1c",
            "e15c010dd8154598aaa58014c22b015c",
            "edf205ffe7e84ac8ba550e950d73b414",
            "565431068186428fb96dc61d6722f3ea",
            "584ef6902c3e49458f498845787ab292",
            "ad09ccbe47224a1cb785c16e7be0d1f3",
            "7ecdd079ffc64672b7c5b7332167e8f8",
            "0ca6cbb4a862470bba7a7d3408f5f684",
            "c13dd9d3ff264c3fa3c0f553c9d5f695",
            "2f4874cc43aa478d80fc3d5cd999c0d9",
            "6f8fc573535f457ab04d81402a8a57a5",
            "844925304432433db9adb51157004858",
            "51c4c765782247a3bfc1e5ef2df604a0",
            "f3f0ca91814b4b4aa466a88a084b0e1a",
            "a14a469ffd9f4fce9c2b10aabba793bb",
            "5f427f67545a49989a53f70f10cf51b3",
            "cbc5b5bcbf3b4a43a5ee855d48e1903c",
            "3f8efe9366ea47e1a16eba3dfded0337",
            "5c21dfbf1ae44041a348dc9176c6014e",
            "69ba7d7f60af4f27803adac0dc73b8b7",
            "58b145be5b5348a680012261428c8720",
            "87f3177c15c84cbca6c8985d40350095",
            "3e8fd5194efe4d4f975e261cdb397341",
            "721455084e1b437a865bcd17d99244bf",
            "b46de432861941ba8fef64f8b10677c8",
            "da27c6d4c412440eb9816d73b5cda70f",
            "d6516ffc11974feab2959b88551ea028",
            "8b3877c6710d446d8e67c98bbd9af5b2",
            "729e4df970634927a3d7d2b72607d000",
            "dca0c4d39e6f49fa841d5fc583b2ce3d",
            "934bae43e2694a9dbc5bc03fa11a6ab0",
            "7df1f19b7c8b4d8aa65f96064f3d8353",
            "994031e2a5b94d759325417f88a61aa8",
            "71dd2ac0bf1f48acb2299e01f673405f",
            "023f04357113484fa2936430eb888454",
            "d6cbff4a212f4da79a32d97dfef6c53b",
            "4ee8d9603a8e4118a2888ca4bad6889f",
            "bf82f0110b8c458d9547b0253ed15347",
            "1af2e45500f14a8087dc014df39e2fbd",
            "f023ac40c6cb4a30b6a2209bbdf1927e",
            "42c65d5520884173b3dd3fe0eaa68a98",
            "6bffc27dfb294a94b9902b84338fc4ae",
            "4d8e36904e174a22ba66ca4cdfc3471e",
            "3b1378620bbb418bb05a1a1f9a607b47",
            "591cabc6e4c34d36a9870d5713f1b8e9",
            "6de60d8e82144c8a8a9767ab53f4f689",
            "2ff895dfacbd4f1d87b0681b6dd545b7",
            "6e0a9476cad04eca953789470f7f2ac0",
            "0a19f9ed65e44862b193d3a761967cd7",
            "d0e7e707eb8f401d91bb5771036cb83e",
            "52cb77a2def74218af8e4f2de339ccb9",
            "e41caf959306471ebb32bf31c6e665b2",
            "f6739cce2d8843a8a48691717abb2277",
            "7920d21ab0ac489b8234fb26248c12e7",
            "5c155b47fb3348c4b349185b33e6027b",
            "447280b5d3684b32bfc66e6591e0ca95",
            "4a8734466a8c4ed6a1ba3afbbbb68ff3",
            "8ef4d90f20a64cd18349af605aa9717e",
            "607bb6a94ca14c04bd7f9c6b2ba15f77",
            "8c13dc62bbab41f092a3ade2d2c8c61e",
            "66f31e15a37e45778ac20eef49de6b50",
            "d47bed531c4448ca8241cbea9866371a",
            "c5a63cb25e514430bb9aa444a917f2b1",
            "c9350c6b76ed42d681e390ebf5eddcd0",
            "9258682f4c44440eb3fa1f33df2655e7",
            "3a3417e229664d50b791accd5c788a63",
            "8f6be2f006a047a3acaacaf836d2b0ae",
            "601829b443aa4b3ca7d7f93001e5d9ce",
            "bb18395af3b2403c8094f5d057257b8f",
            "2311373a86c645cc8da4e90bde6f295e",
            "f3590b7cb91044f0b84ec8b4d8240065",
            "eb46e718676d4b4591e2d0bd3963eeff",
            "ecf054e57eac449e8378892f85419c0f",
            "cde2755689a344bbbb0c19d11cbcbd0c",
            "527f5e3d0bb44bd4962e76b1cc643fc4",
            "8136b90da8fe4cdc812d8ddcf5350ba3",
            "4d06949235904c56add415de70e827bb",
            "304112ad685a45f5a29ff23e870d5def",
            "e18db17aa91c4c5594648b1d0bcd276c",
            "a8b4769d77a04ca68626ee98d12d6db0",
            "75b90a22efc949c49ed1855b9fe2c1d9",
            "1ddf8fccdcd0489eaeb661467b3c4425",
            "402a99abc7e54df686e48301bf0b502c",
            "1137435065b240929e551ac436a614e5",
            "564041ced0c44d69bc35e8f9a6346e3d",
            "82359936ea6a409e9ab645b92ea23853",
            "5bb163f12c39432683915cd4f29f8c80",
            "e9977e277b664d8dbd5f51210ec43f0b",
            "dec69aa4fd654d5bb23b5f6229a7f357",
            "d3b838e2e4a04977be608f22b6b858db",
            "acb5e6891fa44f63a2dfe5cd5c3bdf67",
            "a92da005cc684a1baa963293d303f37f",
            "c77e1316bdde48679e54314e3f658196",
            "706e3a95bed3413080faa2f23faf66ea",
            "a75e30e0ddcb46678de9a7eee0bc426b",
            "672c6d9e827c41f1a0b884e2d5160ab2",
            "f9eb26f0971c4e01b088419e8bab2840",
            "7eedf618b5344ba7890eb6b5c0eb9cdd",
            "f2c7fbbaa0374caabedeebd22cdbeacf",
            "2695419ccb9f41c997972ca73ee2ff16",
            "65535083ec1743d7a581fceada8026e8",
            "ecd475bdf2c24a23bd926ae53be192d0",
            "b4c7fe646741437cba9292e921db8c59",
            "f1eed59e2a9b49f6a0f392d65e7133d9",
            "812883ffb2774195906c864aaf179d38",
            "e9bae617bf534efab9c5a82fb4605942",
            "45148cf2f5ea4a708125fdc409395950",
            "d59037e3864545bd8492c2f0c4bce614",
            "f2b57055c40d4eb2b9f1f1c9e1b11503",
            "481772e766534b7f962e6f476e230ea6",
            "e090a1b5185044bca0aa3c23a9ce9f2d",
            "ba16508ffd3a4433be3fce6ababb307d",
            "418b357b28e047dca04f8f17f3fc375c",
            "d96cc789a364484cacbf109a98238709",
            "c1d98febf1f449eeaa0a24e439b0f7d9",
            "933719bf83d747ecbf2a9d7ca8c5b5b4",
            "f0a3e7b7eb0447c4b04d1829cd19d88e",
            "9344c5102dde4a2da6b9ae48bded44f5",
            "4f9b90fc3132458b8253dfe59c622d4d",
            "61127971a60d4c9d963f33e3bd984153",
            "c45516f7e361494cba1b817ffa90f469",
            "0c0a809e783e483fbc3387b51b2a5abe",
            "e8b84c0daac54d319d63cbfcaae75f58",
            "d3745e9cec4c4b18a725d6bd9a6a318d",
            "1a338f9a15ad43ccbce0cf5aab1704f0",
            "e0027836565e4d30a6e4ed539a7d5f7b",
            "81d8b690f4d7483e9d3b1d6ee6a16943",
            "caf71772ade44d5bae5ea14af062a2e1",
            "d32bd5192e964190b96c4ac5993bef17",
            "f5ce8be99c12477da37c829d529ecad6",
            "39608cd9847442f3a5726bf48635ce21",
            "6603aa49e7944973888371d3962b57ac",
            "b61fa56772cb42428d3d9cb848f966b8",
            "afc13849d4d74e74a10d8b472f81eaa0",
            "13104e2205654db6bef189fc2314d20d",
            "c35726c91afc4a1b84f3d4ee2411acd8",
            "e5998566c5564751aa2c8234ebefc554",
            "3b692e76478e4d11842900add1448c72",
            "4c39c23af87e4daebb8779671dbb67fc",
            "557beb0d27474172905c15595ab28910",
            "072cae87b93a4d5c8dc445dd4e3d6e88",
            "61a24e54bfe14d50bd293eebb5299b9f",
            "335af0924d014d40a056ee195fb21612",
            "ddae853cd6c142eb9a5632c5af9362e6",
            "a9e7fe53b70444f58c2542597dca0526",
            "cb223591ba7241a497ba356a74d44690",
            "db48e8e9044847f6b5ce902584783d22",
            "7a5fa4c0507d4ca580ea705a3db34f18",
            "9ab00b6f21c641289aab6f1598754b89",
            "4b682354338a42c69ae71167fee89580",
            "66e29d4978d247dd8fee3b26ddcadfaf",
            "04b6e817c3af4f63a13b9fd12684fe77",
            "de96312514014a969717aa18717aef90",
            "7ac2a62e47d64e989d665d0bbe51ab97",
            "f2e447c0b7a045808bff215b13e87a38",
            "bdc3ceda0c354b2c92bc7b57d21ba996",
            "665ea9af71db414e80705c4fe387bfaa",
            "e1f5d8be639d49978e839265c04be031",
            "fbc3f1de86a94757abc915710f06de49",
            "d04f1f15c7f648bf965660b95f2bec65",
            "0b5ec895ecd547f19fb6bcddfe5c6c80",
            "6364585109b6411eb17b805e2ec3b369",
            "a0f83e0acd6e4a4d8b18921ce0ae8ece",
            "3521cf83dc6140d48cb085eb5a877b81",
            "578dbfa43d834111a89b659b6c26aa72",
            "d062b042c92b4a8197ab115bb61d7158",
            "caa698276a7748eea26bce40b44512c2",
            "b68ee46368de4b83abd5f087e5089697",
            "f584ce5a43bf4bfca768fc0d43597994",
            "31d75d2b530d4a0483133aa0c679c596",
            "ab65b4ad51bc47249d842912d024d2f3",
            "a471b8d5bf0b4b92b9b45239910ee70f",
            "7e5b2669a4fa4a7eab17d6db8101522a",
            "af571e291ba9431cabf40c0fe8b5c91e",
            "721d22af5052436eb1ccdf9911caa074",
            "9413a45d8ee84e868bf4783b4d44bee0",
            "5ec8dddb4b89459e970d64ff4e72a657",
            "f14d3a97a9fe44be912e9e55c90f186b",
            "d108c9cf11ee4b34946ba2960f212da8",
            "035247d2aff746e091c59883732dc1d9",
            "743c49668dd64fbe8c1a87354e80a6ba",
            "9b3275e844114ec2b0923fe793bfa2f7",
            "0a5c7ffac7d84fc0b62b7496362309fd",
            "f859cfbbfb2b4340be4c5badc2c569c0",
            "9ab776b973c14a72a0cd8b5546ffed9b",
            "1a03a4d9af0240d299a2864c94821a7b",
            "050b00824a714e5eadb351e4caf2b607",
            "0fff43324809462d906600d21b86e7ff",
            "f8d65df63bdf43ffbc834cf3d8379487",
            "4e7fbcc3469f48e1a9ebc73f7aee4f33",
            "623b36d1793f417e97a35d02ab01fe3f",
            "ae42c739afad45389d3bbe630c32cc99",
            "d4d5da567a32485ab0d8210c58fa4a05",
            "fc8d9920e31f4eedb1bbddb82275c12d",
            "002e98318e1f4188873f9148083a441a",
            "ee6e5ae9953a4dd79659df4010d5f0e4",
            "2a6501c92294442ea4462114695906c2",
            "a0be5ff8bfa045688d5aec49eb157b3a",
            "e6f213a2a6184f3ab3401731d6d9690b",
            "f5fa316359404931b06425a616dd754c",
            "64b18477b46747d297eb9c09082508dd",
            "84e25370886c4477a89ef99b71767a4d",
            "2e95e76997e3429193ce410e3cbea01c",
            "7c5f244f24eb466da7cf4fd3a126b484",
            "b890fbb77e024761b917c79dc6047b86",
            "93650f3e4d864114b989179929250948",
            "5c6a04a1dce34f4da5f2d533e78c3002",
            "e021fafe2d17436a9ae9ef88d1b776e0",
            "fed5c8542b494ae6b1add92418a2b0bc",
            "693716292b7e4f2ca76fb2efe0ca8955",
            "08f9e20118164693b1161f56f82a888d",
            "5a7d163c2d364f6aa2b9bdc7f36ea946",
            "5e0ab1cf49504ad2afb0970d2b1f13c7",
            "ba46758b66e24061b2026d911d88e33b",
            "7ad50d33235f4fac9dc1f4f64960eafc",
            "aaaeed62e4054505af6ed055a4dd3af3",
            "038af559a4df4dff867cc3eeab6f866a",
            "58e67342a7bb461c835888998349d662",
            "3c998bacc1d245d0b2298a52404d2a64",
            "0b6951f1264244eb8147d4e32a4dfe84",
            "273b09e7cc2b416eb5db565d501fa805",
            "7eaa89c3621c41628ece753c9220be5d",
            "40c0b7b34d604a6a9f5525a0dd48157c",
            "edea68cdf5d942098f18e162fd996a68",
            "de8473b916c2499f9c72f8154190bd94",
            "d075fffe056e4aa9902435f2552f9e2f",
            "527a701d969647558797f6f1b10766aa",
            "8e38d6f60659414ab12f6fa54859908f",
            "1d1ce762b0f24629856d91b148503e73",
            "1b2293f9dbdc4ff7bdf9ce757b4e41b4",
            "f33ccec5b77447a790714a51bc388d8a",
            "7ffb58735ed5432c9df05fe5646b1e2e",
            "10d25175e79f4f3b8bf3d90ac8b883ad",
            "76cf2dadfdcb420e961584ea80a23261",
            "51b3e90661da40cba555b6aba7519ee0",
            "7e47973e045646ffa600958e581f3845",
            "45cfca5b10b945f4a1ac0bb975aebc03",
            "59cb9bf6ea074f74b22bf88648245013",
            "b324866114b1436ea25c7bbd61b8caed",
            "c92862695c27465abf2fb3db80714f0d",
            "5ee6476343494893bee2a375c24f2040",
            "985d7cba227047b9845aad66b3d33e19",
            "2c8bd145eea248988deefd97df1f73ed",
            "90bdb4d60ab846b89b81f4ca6441eb8c",
            "2bf0e5c1bff84b08900159f20a47e504",
            "254053cdb8a042df91004671bd08a4ca",
            "fd1ef440f6654de19c627d85213556c5",
            "cf308a88b5ab4dd4900ba23adf8f8331",
            "e299a2733da44b9793e031da8bc3bd9f",
            "74a29070aa0e400bafc6f05f12fb0b7e",
            "0aaf47e231ac4d798d55e1ae6493879e",
            "af0122a3fee84cbb85424407cf7a7581",
            "f0707922911a416d8ae86a39637184ca",
            "2eab081484424f40b03a241b68d70584",
            "91e2ac58f18843a494fd5759f7ea2f5a",
            "05354ab1f9254b41b9ac9e26295e83c9",
            "439592cf8575406c9ac60c6385f17224",
            "808854da31b04db29cc5b912ca57e35a",
            "18148b5b771e469dadf1d1e77371606a",
            "71a6136d94ff45938a65ef8d69c9c3c4",
            "b19cfe681a754a27a5f77add87547527",
            "c40cc7ccf5af4fb5845daebe041cc873",
            "a59635b5aa2741418880f7f7bad1d110",
            "486570f741134bf9a951185baee02393",
            "56e0a91637c9422099f67e7a91d03f87",
            "ca66eb8bce6a45818b897c3f70b1e1c2",
            "c6c2fd82f3a04af189259caea928afb7",
            "30e60f278b20411eb481c38300d258f9",
            "da220582afd74ce7a86147528d7819bf",
            "2bb03acb2e244e94975548accc82d063",
            "04ead14343a747e1a24cd7c78a2cfaef",
            "a74bc62fd14b47a691c62d4617e4c107",
            "188f9b7af1044717b0de58efc8567e00",
            "023d4e8adc3a477b9a484b19d5e51171",
            "3092fd091ad54160942f421d13ae3721",
            "6ffc654e12e3403fa364b52125e5a30d",
            "fa0f1c7af95c473dbd17efa8177edd62",
            "9be23aa970a142f3a0f96da607cbf378",
            "deb5afc36a4448d98278b50f9011ba7c",
            "d69b5359deaf412f999cd83cd380a358",
            "d7c5b0b124b74388a7a059a0a1d69560",
            "044f21ed61424b1195823013d68d680f",
            "5207724128d44f60bced38302e130961",
            "b8ec81cf4df340dba0362657e12586e3",
            "6ecae4e1107d41ca9a2587ff1f060db8",
            "83cb9e7bfcab4d099d6d29b4ef181dde",
            "2e45fe764c6a4a29bea85ed63048cdee",
            "499af40f2a724aa6b68f8d11ec01e599",
            "c749f822062b47b1a5f44c5b1bf80a5b",
            "9751fb094c5f43dbb55181033dc95b6c",
            "0e19b2fdbb14419bb9a7e5cb80e85cc8",
            "83abbbe2fc874dc1985edf756c4364c2",
            "dbcc0c23d94742a88c00553261dbfd93",
            "dcfff566048840c08210805e690c8b0c",
            "93af1c5299534a788a16ef462bb80427",
            "5ee304a35b0640f6a746def6ad48ae89",
            "44c469928a1c4311975e23f8ba0a6775",
            "7568cd143d0f47b298a6ec83985d8b45",
            "77bbaf79cb4b4128ae11bd8d9c47c5ec",
            "cd0238bd8f974863aff2505bf4753adb",
            "ca2da5fbdb994a9a9a2d06f192084c7d",
            "3d065227a70d4903b3073bf8eb68af58",
            "655d3390fb0d47baaed9ee33fd13c74c",
            "83045cbb91e04c69bb6a8345fc617590",
            "43a4309129bd4b2fa6785745183b2bb8",
            "0ab80f6bf9cb4f30b6e2614d45107efa",
            "e9a05907526a42eea3d00555d5765418",
            "69d30e62a11544c5869cd0966ed709e0",
            "c46f61627dce4b61be0fcb1a5bbce6ab",
            "f655efb40dc54423a2d59aec8ac95782",
            "f5047f7a0fe9409cb40abe6ef3404ffe",
            "0964f6c7be3243618c5ec0282463ced3",
            "10c25c9a4a14445d91ad0a89ec850b4e",
            "c88fdbc5af5049d290b25ac9956c8016",
            "25a818a66ec14710a2d0a8a41bf712a4",
            "56c25f6e4c8b419ea020fd0dae681a09",
            "2d54054a61b246e9b7be96a1d90ea764",
            "a7b1f60d75854a0fbb5a17f36af9afc4",
            "9e9f13688d7b4ca4a126dd8f55347063",
            "3fb46b70322b40f9a3808edc859a334a",
            "fb601496f3684f138e379a7ef0f159e6",
            "c0438b53dd5742198bcc4f31990f5928",
            "b71cffd7f3214094b467350636f30c20",
            "befd6da40dee497bb4ae3ad948dc391d",
            "a4ef2bb0dd2f4784846b06341f79525b",
            "c7f2cb1e0f304f70abebaa4d73ab802e",
            "64c44599ab9a4ebda6a270f5f3d938be",
            "55479a07b48b48f1ac5d8c736fd051b9",
            "155492956989422a97a34d9a3371a7f4",
            "d508744b736d43bf84c49a350ca3598f",
            "e8de4d61278842c897adc77126e1bf66",
            "e85db8aaae73437e885af6f832cbbe85",
            "715b1990d3814ad29902ef7feff8c5ee",
            "6e878f9f97ad452b9e0a6fa6ada7cf3c",
            "f232eddf0b474cf39b3fa29ccc3c4681",
            "cedd38e0768f4fc3a530aa2203ad849d",
            "51c6a4a4ab9a4500b07c8019f86d29e6",
            "b6b4ec790f3349b1877a35f0cf8d581d",
            "9e3a680752334e96a13f007c5073cce2",
            "72f57cc38eef4739bdee12162a09db61",
            "256bbb1035ea4d29970a47d55c1d6cee",
            "ee29d82a1fd04d24961b8a1ca95b82ba",
            "2b2a80e57ecc4f268c11508e61ca5808",
            "3988f39e200242f9b6a1bc2595295c01",
            "2593a1f747d8427c8cdfa489e2a87b6b",
            "adbaee126343429c8343316716d48bca",
            "aa1d0dd742c34c45a13552abecba153c",
            "bbb0b513680141a88b61a92246861e3e",
            "f6801e9fa06841b995530bd7395461f7",
            "17b080ff18024e25b0e22d8f1522dbdf",
            "4ef20d2158fa427dad7227469a690099",
            "303be2e8dcee4c97a2167652c4137529",
            "1b50620b43c54d79aefb14de592936d8",
            "add5c6b67a0c478daaab25c4308339da",
            "a7bff63d6b5246e685e7ea5e8f7af403",
            "46286a88b8ca4cbebe99594e05163c05",
            "e6c3ec3956ad4d18bfe1cf00d9462f77",
            "a2f482e1a9ad435aaa223463ea42e264",
            "3d07736d18f74795b45e3c86fb32fcdd",
            "cf557807219b4f7abd0d8bc3ccb41702",
            "f5e35e0a28804c41a217d43d9e12fd58",
            "f8614384512b426d978e55a4f8f12b85",
            "0e48cbe8df5a41b9a0a4142f34859db7",
            "2bd7126ba7b04474966736cfc88717fc",
            "7467e9bde9494565bd053f153e4bbdf6",
            "95b60efd809e47a6be8c8be22769a8ff",
            "76ceadea5ec54ba3b91bc45fe7750413",
            "730cb697502b4b389b09d65d3f1789d3",
            "075261f66e5a41b696a51553262d0d68",
            "7d9f2c7d6576426e94919023095945a3",
            "0c6b80f9422b487aa78d272f91285321",
            "cae0177279f240f5a3a32d1c4c153298",
            "68208ef15ec64189921227c619f88b76",
            "a47eaed8224142899e3d195e3b12e99e",
            "ba68460048014697b44e2140487e7093",
            "a356cc24f20441848d00847c91dec1c5",
            "652bf8af766c4b3b93a153ffba2da202",
            "c22b9c86a5454ec88ea803b72ea2f2ae",
            "608d8dcb31254dd1bdea275742c746dc",
            "52ecbab86efb4ba0b911850fa1f68526",
            "cdd90ca6ef0d4b9aae3951620112291f",
            "ee053b038dbc44bbaa0776345ea7e437",
            "a0efe76cdc234cf29cb823aebe7b438c",
            "f0b26c3e2efd47ff8f022f096ac1adee",
            "5d97c216e99847ee8d2f9cc35531ec19",
            "4e781128c74641b4b55bb4d37f1cb347",
            "e4748a6a6b6641d3836effe6a57d4d69",
            "9c3206976fce4c3f8d0bde8110e683c4",
            "a7fe13207ca842fa8c268a9b97db5b67",
            "ad241810bb0e48e8bd01d689c793f9b6",
            "7993bb4f24f148e9893a55c443689ec6",
            "8bdb140957dc4cfc801c65e09b6cee8e",
            "9d9104542e7e4184aa7503d1b3a70e3a",
            "2c58e331be3142f2a9ff6b8dbb4775d7",
            "5dfd29d223304e81875b144a88fae11e",
            "880b2827b2b74777a757230ad05a2370",
            "0a0bed94338241a8a38122aa263167b9",
            "072b7b4b4f8e4155a30344a01487ce00",
            "c638ead9ded748c493a6c7107246ddf7",
            "047913100e204459aa7ffe9d03688d0b",
            "018982629b294bc1b26b9062e3216370",
            "1e99d78fceee4785b49de84cb5b81275",
            "5555dc4a35df4672aff58f9f02f273f2",
            "37c1ecc7a57e4578adde0493adc570ad",
            "9a95ef09ed0f4563a9952936f844e430",
            "beac7da3b3f2441590daefb331289256",
            "82daef89f74442f993380d83196be2f0",
            "20311e97bb3b45e6ae899826074bcfa4",
            "d6ed5d8fb3df4fd99392f3200f27b4e2",
            "eff6e3c4dc694ca6bc4366b6fe234fb8",
            "c7bcd64db42f44b5813024715e3a954f",
            "e91d0b48af9d43ae9aa19007deef2a8f",
            "78edd019b89a4b3f94a058a3d7f9165b",
            "60caafd857354c7681ae35c933ff130a",
            "9c313bcbc0f741f3b0a723da6bd5b83e",
            "5178cd7c967b40f6aa06b326ff65534b",
            "b7340af60a0249fba4494b35d189fb44",
            "878634f684a24292a86e2b7d3b671416",
            "a7ca470386ed4b07b739d7b4172edb69",
            "13524c0c8e9d44ff96e7ffe6b7e05c64",
            "439179fe810649a8b55a0859fb38dbea",
            "5fde4ebd394d46a4a2e5bc1a1e77ce18",
            "c071e066e11d4da1b646b83d4b1870db",
            "32bda37c20024511a5042bb4c2a48b67",
            "ac277180d4c94f478f65f8c5aa56a37b",
            "56e382d1943d4990891314c98480663a",
            "9047f7d04c224adba2cd76ea69f565fa",
            "847b5cf923bc40f1a4d35a0852915f22",
            "25d9169aef0d4395b26e8ebc5d526fd5",
            "28a3744d84b04db6829425dbf791aa3e",
            "0b366897863341bc902aa35e92dfe9b2",
            "6dc892b851f441d4a0d54df3d9ca20fd",
            "add484f9e2ce4ae2854898b88863e93b",
            "11fd5a389843487082f83a63327302bf",
            "2d4c195c59ee48cfb62cd153bff752d8",
            "d856e6038687496f83319882f62cc188",
            "87e2326b64e04116bde463995c2ade1b",
            "74117064d79f406d9bd669a49b826a11",
            "bb0ea45f8c7949cbb567676fac340962",
            "4b002cd733a649ba941aa29d1d584b8b",
            "6c346f76a51a4becbf2ddfcf6160e599",
            "51368a1bdc574e5b91d4a64446851127",
            "e4c1aa24d61540319f7f8afda4ba118d",
            "b5bd7aa9ed8243e4a2576530ff0ecf15",
            "3131227f6fb84ff1b706c6f012a9b619",
            "5722b0115fce4f03ab8da0a0a4ddac04",
            "b4fb41df76ba47a3a5a035252b3fc965",
            "e9e75f26184e4b69bdfc11e7627a90d9",
            "00334c0b2c614a9e9c30c4ba52e54dcd",
            "a381e784e97846309d90b49e7aa17812",
            "35dc3cf6fe5348b3bcff011fd269547a",
            "aa977cd78abf48ee99b86e86d8dc8a57",
            "4605a8c9aef649b894a57dac614163f4",
            "ecac637d93774b53b7c8f9c8b6dce4ed",
            "0dd2e9fbbd5a45cb919cb53b63968f65",
            "4147e4bf3a304099ac0bb6d6ecdb4479",
            "066a0c4fce264782acdf10f6f6e3622c",
            "07a2aa1d002e4412b94ef72319a350f9",
            "a6106767a2eb40ab8ddcc6be7941a93f",
            "92b9e4ff38b343aeb50c5f50066aa267",
            "9fbddcd9e35341abbe25009ea1f1303c",
            "c51d43c099884903867e2b1086fe7ed2",
            "190e220e7adf4bcf9474c834f35dc54e",
            "f10dc417c644442789840938328a84a4",
            "168057af343348cbb415c9730da44bfb",
            "a43d408cd3604ef0aae3ad8aba861e3e",
            "71abedd3ea6543bd9e02215965e1bd60",
            "43a94077573a4ba090c8f2af73215d9c",
            "cdf662205e2d4d168694c620e368f499",
            "8c081a391b7b40d58a1b57c7a4899e81",
            "9e22cce8a8154ac7a37eb8d6eabd64ca",
            "e78e2ce18e4c436991ada9b4f790ce58",
            "98044f2a74a145f6b74a1b9f35863a51",
            "df381598e4ee4e92bba3f8dd7aac040f",
            "31904a1d1cee42328afe1a25706a7667",
            "7481ff21352c46e1bd2bb0bf496ffec8",
            "8386e7a55cb54f3d9cdcfbe67bacf90e",
            "5f2949b9ad064d09bac9b390ab5fa7c4",
            "a342890e08674977b6a61706d45f055e",
            "2da35df8f2864db4afff409abdec5e9b",
            "6858abc8c4ff41c7acfef3e13ff2648c",
            "c0c49ae9f76f446c86bf3502479511c2",
            "5bdee5065fda467585a2017d9ebb240b",
            "82546c80f8504f72af066aeb750b2f9b",
            "186859350d8a4db3b194d4633d795506",
            "9d814cd564d94b389f1e12018fc08f55",
            "d0b5f548e91946f083ab50b5267dddd2",
            "c42946b231f9451eaaa09c4002647076",
            "4ab56d3720a5410b87e4a0e71e698522",
            "1d348d48850245c09de3e3787dbf13c4",
            "921389b39fbc4e3b97956f41b6199fe6",
            "8fefddf659334043be219829f9cdd582",
            "3cfaf1ebcae745fbad379ec1e9adbbf8",
            "6b9e9ef86ed948ea83f520730eadf1e4",
            "27b26df4cd514533831a95a8699e6b81",
            "848974a8ad4443e5b529a54d8565d2c5",
            "5f2b7466b714466282c0f90915215c25",
            "d901cfd4c4ed46e48b950fe06a0479ac",
            "c76f6a9a5fe548aea1e49c5765cffdbc",
            "f69dfa9253cd4e0e966868a569954ef3",
            "2aee65d7bf304ee8ac88eb1550217c56",
            "17846ac1d87e4acb9db044dd96197345",
            "2a15cadc1c9b4eeb9b40da6925b3fa45",
            "fcc512fd88d946648dbd81ec99f2e5a5",
            "f752b8d2e55d41a4910541ea6803879c",
            "e69857e738b24aa5ae36559fea20504b",
            "c2dd711adab9463bbdea128a4439a138",
            "df3bb6c7935d430b9f3f3b29144dd391",
            "f69ec6544e9046f898a943d0ac6a0ca6",
            "b6c5d635f7174548b7a28264c2bb2ba9",
            "84797b95c67242a6aae1c80f0141dc67",
            "eda8fcba58134affac690b01244be73b",
            "2fccd67badc142c3931d23ff28646a01",
            "a44e89e031e14172acb68d12d6aab78a",
            "c6aa0f6b08394fd2a96b79808ba69b19",
            "6ad9203774a74a7ea61cf32d9621d58a",
            "876ba3e195e2452fb43ac1d383a7a058",
            "09243d18aef041b3bc1cf05435e67ab4",
            "36bec5b206644178a28705016f57006c",
            "5a7af02bc5b04afe840bf09e5e85d66b",
            "74b2c2f347544ad3a490924920c420b3",
            "a1cd43556384444184d49646151bbaa3",
            "cf21a9057e564960bda8ff4b5b3e34fc",
            "3ef9643f5c564deab9ac6b549bc86dbd",
            "7f1910cd4ac745d09ff1bb09c66c975c",
            "8d3f436e4e9341a7b676d5d9f4328d6e",
            "e013cb5f81ea41c38267a1f8b875344e",
            "97d69a2f68f04a7890924dae30ab01ef",
            "9b14b799c11043078a7f110920e6f32f",
            "6ffe980df3ce46afbbb9b3eb612d2787",
            "d491bc06c85b4f70a0c5d57c2de91482",
            "483611d1fd7d4680b415d0b140ac2e99",
            "a06cd463710442329642d58afaba5bb5",
            "085e513ab0664259b706f91d55b33556",
            "9e1ad5401367433ab70aac38bd84aa0b",
            "dac71f140ad540b89429c349b1554824",
            "bc972d28b60d4949b5e89b86232b2bc0",
            "57093d32b92c4d7c99f90924ebd94efd",
            "ff4eef111cf84098a02006a002a3e8cd",
            "81a9a725bb4740b0a6c662428f35ae1b",
            "669a2772073a4288ad1ce136568f0e74",
            "6ec18a56e9a24b53b6b797f81545e554",
            "a3c46b0e86cb4ae6a924454f04a27fd6",
            "3328ced3b825465798aefb3475c031d8",
            "3435c25b254742a7a82d9de5f17988d8",
            "36364a311b354120a02e2b841b6f2858",
            "2a57c6e41e24432cb7729f992320a8fa",
            "1412c248a50b4aba833992cad365eff8",
            "a1c00b95fe9f436d8b390d9a31962f5b",
            "77e3d0726193424a808e3fa494307a6d",
            "294543597d8044849d23477fa9138178",
            "d2d5d97dbbaa4a5bb1a65ec42ff18c0f",
            "aa9e9cc4b93f4a4d9f0af0272dc79b70",
            "dde1173b23e440c0ab7087d8f58bf261",
            "54fe1627eec94605bde27ded2f171fb4",
            "d609d45438534d4198916869bd32a700",
            "012655270da748158cc040c47f973159",
            "4b65fe6a5c184634b2e67d35af7c78db",
            "104801c45ec047019388b3f18f243b40",
            "40d5144411c74c44ac65999ddc12ce63",
            "40a5a19395ab4371a429438892caab54",
            "03234eabb31d4ceabff8a4e84448817b",
            "357fc0a6b3a74ae5b67c0dc97da53f74",
            "d41ca7bf21d74aedbcae34a69b7d0bdd",
            "fd4e22dd29154990bd22fc05936b6471",
            "8ec8343e996a4a9d8f9358f5b16a4e06",
            "aae7c1e081284a11b70ce350622bf561",
            "303d650c7fc64aee8f0660e60b0b53a4",
            "79427544a58143b581438e35bf00fab3",
            "37e54b1fbf2443c7a64442ab02da654a",
            "2e2c3bb30fd348d890c28e35a5b3342a",
            "d0c2d343d3e6415eab4e4f88774ce4b1",
            "f3dfb245de7f43baa44d9ff261712f86",
            "2f74dbfbaefb425f887a7c322577f844",
            "b094d5571ded4398a0eded5d1003f5dc",
            "6d689903d59f4768abe7680480f75c56",
            "e47860f69cad401abf1799ba4a6710b3",
            "b2d64a0497da4b4c8cf0f5deb5f5f0a0",
            "9dbab2f5805e4ee9ad2f1e1813714116",
            "c37e85323d5c40e99eda9dc7d324c32f",
            "d284a716e42d425e984b87f78e4f644d",
            "3eca679cd1924d6fbd7292698fc76279",
            "27f7d5a291cb4cca8b2a600673c401d8",
            "d695bb5cc57343c38e86a2958ec676da",
            "bdabbfaf96e14884bf4c7c22f9abc6bd",
            "f279f39c74e84b0dbcbe0e3cc307f019",
            "a6139ea9dca140d88b274eff9252136a",
            "d8554998c9154dcc9d22dcc7579a255d",
            "791b2f5791d3420d82af98c5d0cbb1a5",
            "99a02d4e278648ef9a1c9fc1bf928f71",
            "101ac4c7ccc64a22ac2b9459240087cf",
            "37e25cbbd7604619a5b588cebf2efd80",
            "88bfe626212a45c08028153bbc807f36",
            "0060a45aba4e44d18cb406730a7cc0be",
            "4a9359d785ec48a8b9e6673531fd49af",
            "108dc838b7e54caca61e8b1907bb2ac9",
            "4810245d716e4ebb99cbbd95dbe28f5b",
            "564a36ad5dea4535b737feea5e3dad18",
            "d47c808876e4471e8932d97d540c27ea",
            "3a384672df804e2eb302f2c262ac1b34",
            "d88b2a209b7c4e5aa827923100b360e2",
            "48bd381f73554708a85608ab8333e639",
            "6bdf888eec664801adc41955f5a1be23",
            "86c07d6b29554e64ad5ac43bb51eefda",
            "a01012fc829d430f96f001834b45ceef",
            "400408cf4b064cff8f114132d923cc7c",
            "a039accf745b42f6b59da7a44857c81c",
            "2cc50b3e3c7a4e0297d8c6d1bee75e54",
            "e3de5bac352a4ef1b393d8bd54f22497"
          ]
        },
        "id": "GOWMULwb23KS",
        "outputId": "1d320c6d-8d94-45cd-c610-825ed4137182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ—‘ï¸ Old AI folder deleted and recreated clean.\n",
            "AI_OUT: /content/drive/MyDrive/AudioModel/AI\n",
            "Found 40 configs:\n",
            "['split_20250821_122428', 'split_20250821_123446', 'split_20250821_125931', 'split_20250821_131935', 'split_20250821_132430', 'split_20250821_134435', 'split_20250821_140536', 'split_20250821_142935', 'split_20250821_144939', 'split_20250821_150431', 'split_20250821_155434', 'split_20250821_162429', 'split_20250821_170439', 'split_20250821_173006', 'split_20250821_175929', 'split_20250821_182033', 'split_20250821_184950', 'split_20250821_191635', 'split_20250821_194003', 'split_20250821_195516', 'split_20250821_200502', 'split_20250821_202505', 'split_20250821_204955', 'split_20250822_142242', 'split_20250823_072111', 'split_20250823_222115', 'split_20250824_202811', 'split_20250826_022022', 'split_20250827_142346', 'split_20250829_042001', 'split_20250830_162006', 'split_20250901_034338', 'split_20250902_152050', 'split_20250904_071958', 'split_20250905_222111', 'split_20250907_132045', 'split_20250909_052043', 'split_20250911_052041', 'split_20250913_072120', 'split_20250915_112047']\n",
            "\n",
            "============================\n",
            "Processing config: split_20250821_122428\n",
            "============================\n",
            "Rows in 'split_20250821_122428' train split: 10\n",
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_122428.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_122428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/1200] Saved ai_0001.mp4\n",
            "[2/1200] Saved ai_0002.mp4\n",
            "[3/1200] Saved ai_0003.mp4\n",
            "[4/1200] Saved ai_0004.mp4\n",
            "[5/1200] Saved ai_0005.mp4\n",
            "[6/1200] Saved ai_0006.mp4\n",
            "[7/1200] Saved ai_0007.mp4\n",
            "[8/1200] Saved ai_0008.mp4\n",
            "[9/1200] Saved ai_0009.mp4\n",
            "[10/1200] Saved ai_0010.mp4\n",
            "\n",
            "============================\n",
            "Processing config: split_20250821_123446\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86b8cdba72c0487487691e7bf541b1da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_123446.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16d17455c28049ae8a00661e4cb400f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_123446' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fedca28a9bd441ca9c68265fef30ba8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_123446.tar.gz:   0%|          | 0.00/232M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_123446.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_123446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_125931\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1773390e287247c7a11bba965974b737",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_125931.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d054666105634112a70bd3aa59713a36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_125931' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ac39827c83040fe86552c7e6dbcd488",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_125931.tar.gz:   0%|          | 0.00/147M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_125931.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_125931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_131935\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0fc373b7a914d87b472958fcd0cb91f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_131935.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e52ef4addf3479d96baabe991237861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_131935' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "572a4b3599c441cd9376fc4b1bbb01f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_131935.tar.gz:   0%|          | 0.00/95.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_131935.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_131935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_132430\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41f87dec136b4891bd00d4bafe09e4b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_132430.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1c7f7183465492d803e47b58b59a3c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_132430' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f713aefb8c2f40f7abe60e0ab4b2068d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_132430.tar.gz:   0%|          | 0.00/80.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_132430.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_132430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_134435\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eff4035a1e747e6bbc7e37c408e699b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_134435.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "439d2860d0bb487fb9ce58f41b8ef819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_134435' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d9cccefa60c449689e9cabd04857fab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_134435.tar.gz:   0%|          | 0.00/178M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_134435.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_134435\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_140536\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52d0960525114c0391be1eae1fee89a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_140536.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdfba7a5d6ef4028972097fcc5be8fff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_140536' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "584ef6902c3e49458f498845787ab292",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_140536.tar.gz:   0%|          | 0.00/180M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_140536.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_140536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_142935\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f427f67545a49989a53f70f10cf51b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_142935.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6516ffc11974feab2959b88551ea028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_142935' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf82f0110b8c458d9547b0253ed15347",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_142935.tar.gz:   0%|          | 0.00/163M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_142935.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_142935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_144939\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a19f9ed65e44862b193d3a761967cd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_144939.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c13dc62bbab41f092a3ade2d2c8c61e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_144939' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3590b7cb91044f0b84ec8b4d8240065",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_144939.tar.gz:   0%|          | 0.00/219M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_144939.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_144939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_150431\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ddf8fccdcd0489eaeb661467b3c4425",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_150431.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c77e1316bdde48679e54314e3f658196",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_150431' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1eed59e2a9b49f6a0f392d65e7133d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_150431.tar.gz:   0%|          | 0.00/108M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_150431.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_150431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100/1200] Saved ai_0100.mp4\n",
            "\n",
            "============================\n",
            "Processing config: split_20250821_155434\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1d98febf1f449eeaa0a24e439b0f7d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_155434.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0027836565e4d30a6e4ed539a7d5f7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_155434' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5998566c5564751aa2c8234ebefc554",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_155434.tar.gz:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_155434.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_155434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_162429\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a5fa4c0507d4ca580ea705a3db34f18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_162429.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbc3f1de86a94757abc915710f06de49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_162429' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31d75d2b530d4a0483133aa0c679c596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_162429.tar.gz:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_162429.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_162429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_170439\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "743c49668dd64fbe8c1a87354e80a6ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_170439.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae42c739afad45389d3bbe630c32cc99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_170439' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e95e76997e3429193ce410e3cbea01c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_170439.tar.gz:   0%|          | 0.00/149M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_170439.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_170439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_173006\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba46758b66e24061b2026d911d88e33b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_173006.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de8473b916c2499f9c72f8154190bd94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_173006' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e47973e045646ffa600958e581f3845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_173006.tar.gz:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_173006.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_173006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_175929\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd1ef440f6654de19c627d85213556c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_175929.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "808854da31b04db29cc5b912ca57e35a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_175929' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da220582afd74ce7a86147528d7819bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_175929.tar.gz:   0%|          | 0.00/91.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_175929.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_175929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_182033\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d69b5359deaf412f999cd83cd380a358",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_182033.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e19b2fdbb14419bb9a7e5cb80e85cc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_182033' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d065227a70d4903b3073bf8eb68af58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_182033.tar.gz:   0%|          | 0.00/125M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_182033.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_182033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_184950\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10c25c9a4a14445d91ad0a89ec850b4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_184950.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "befd6da40dee497bb4ae3ad948dc391d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_184950' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f232eddf0b474cf39b3fa29ccc3c4681",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_184950.tar.gz:   0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_184950.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_184950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_191635\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adbaee126343429c8343316716d48bca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_191635.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6c3ec3956ad4d18bfe1cf00d9462f77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_191635' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "730cb697502b4b389b09d65d3f1789d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_191635.tar.gz:   0%|          | 0.00/157M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_191635.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_191635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_194003\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "608d8dcb31254dd1bdea275742c746dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_194003.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad241810bb0e48e8bd01d689c793f9b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_194003' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "018982629b294bc1b26b9062e3216370",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_194003.tar.gz:   0%|          | 0.00/146M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_194003.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_194003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_195516\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e91d0b48af9d43ae9aa19007deef2a8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_195516.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c071e066e11d4da1b646b83d4b1870db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_195516' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11fd5a389843487082f83a63327302bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_195516.tar.gz:   0%|          | 0.00/187M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_195516.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_195516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[200/1200] Saved ai_0200.mp4\n",
            "\n",
            "============================\n",
            "Processing config: split_20250821_200502\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3131227f6fb84ff1b706c6f012a9b619",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_200502.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4147e4bf3a304099ac0bb6d6ecdb4479",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_200502' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71abedd3ea6543bd9e02215965e1bd60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_200502.tar.gz:   0%|          | 0.00/154M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_200502.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_200502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_202505\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f2949b9ad064d09bac9b390ab5fa7c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_202505.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ab56d3720a5410b87e4a0e71e698522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_202505' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f69dfa9253cd4e0e966868a569954ef3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_202505.tar.gz:   0%|          | 0.00/144M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_202505.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_202505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250821_204955\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84797b95c67242a6aae1c80f0141dc67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250821_204955.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1cd43556384444184d49646151bbaa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250821_204955' train split: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06cd463710442329642d58afaba5bb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250821_204955.tar.gz:   0%|          | 0.00/96.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250821_204955.tar.gz -> /content/bm_video_benchmarks_clean/split_20250821_204955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================\n",
            "Processing config: split_20250822_142242\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3328ced3b825465798aefb3475c031d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250822_142242.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54fe1627eec94605bde27ded2f171fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250822_142242' train split: 500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ec8343e996a4a9d8f9358f5b16a4e06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250822_142242.tar.gz:   0%|          | 0.00/7.41G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250822_142242.tar.gz -> /content/bm_video_benchmarks_clean/split_20250822_142242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[300/1200] Saved ai_0300.mp4\n",
            "[400/1200] Saved ai_0400.mp4\n",
            "[500/1200] Saved ai_0500.mp4\n",
            "[600/1200] Saved ai_0600.mp4\n",
            "[700/1200] Saved ai_0700.mp4\n",
            "\n",
            "============================\n",
            "Processing config: split_20250823_072111\n",
            "============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e47860f69cad401abf1799ba4a6710b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "split_20250823_072111.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8554998c9154dcc9d22dcc7579a255d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in 'split_20250823_072111' train split: 500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d47c808876e4471e8932d97d540c27ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "videos/split_20250823_072111.tar.gz:   0%|          | 0.00/5.80G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/.cache/huggingface/hub/datasets--bitmind--bm-video-benchmarks/snapshots/463dd64f80da11a9c2323ae809b31c7215161232/videos/split_20250823_072111.tar.gz -> /content/bm_video_benchmarks_clean/split_20250823_072111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-782756894.py:92: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[800/1200] Saved ai_0800.mp4\n",
            "[900/1200] Saved ai_0900.mp4\n",
            "[1000/1200] Saved ai_1000.mp4\n",
            "[1100/1200] Saved ai_1100.mp4\n",
            "[1200/1200] Saved ai_1200.mp4\n",
            "\n",
            "======================================\n",
            "ðŸŽ‰ CLEAN AI DATASET COMPLETE\n",
            "Total AI videos saved: 1200\n",
            "AI folder: /content/drive/MyDrive/AudioModel/AI\n",
            "======================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 0. Install dependencies\n",
        "# ============================================\n",
        "!pip install -q datasets huggingface_hub\n",
        "\n",
        "# ============================================\n",
        "# 1. Imports\n",
        "# ============================================\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "from huggingface_hub import hf_hub_download\n",
        "from google.colab import drive\n",
        "import tarfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# ============================================\n",
        "# 2. Mount Google Drive\n",
        "# ============================================\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# ============================================\n",
        "# 3. Configure AI output folder\n",
        "# ============================================\n",
        "AI_OUT = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "\n",
        "# === DELETE the entire folder and recreate it ===\n",
        "if os.path.exists(AI_OUT):\n",
        "    shutil.rmtree(AI_OUT)\n",
        "os.makedirs(AI_OUT, exist_ok=True)\n",
        "\n",
        "print(\"ðŸ—‘ï¸ Old AI folder deleted and recreated clean.\")\n",
        "print(\"AI_OUT:\", AI_OUT)\n",
        "\n",
        "# ============================================\n",
        "# 4. Dataset configs\n",
        "# ============================================\n",
        "DATASET_ID = \"bitmind/bm-video-benchmarks\"\n",
        "configs = sorted(get_dataset_config_names(DATASET_ID))\n",
        "\n",
        "print(f\"Found {len(configs)} configs:\")\n",
        "print(configs)\n",
        "\n",
        "# ============================================\n",
        "# 5. Build clean 1200 AI dataset\n",
        "# ============================================\n",
        "TARGET = 1200\n",
        "saved = 0\n",
        "video_counter = 1\n",
        "\n",
        "BASE_EXTRACT_DIR = \"/content/bm_video_benchmarks_clean\"\n",
        "os.makedirs(BASE_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\")\n",
        "\n",
        "# Iterate through ALL configs\n",
        "for cfg in configs:\n",
        "    if saved >= TARGET:\n",
        "        break\n",
        "\n",
        "    print(\"\\n============================\")\n",
        "    print(f\"Processing config: {cfg}\")\n",
        "    print(\"============================\")\n",
        "\n",
        "    # Load metadata\n",
        "    try:\n",
        "        ds = load_dataset(DATASET_ID, cfg)\n",
        "    except Exception as e:\n",
        "        print(\"!! Could not load config:\", e)\n",
        "        continue\n",
        "\n",
        "    train_split = ds[\"train\"]\n",
        "    print(f\"Rows in '{cfg}' train split:\", len(train_split))\n",
        "\n",
        "    # Download videos tar\n",
        "    try:\n",
        "        tar_path = hf_hub_download(\n",
        "            repo_id=DATASET_ID,\n",
        "            filename=f\"videos/{cfg}.tar.gz\",\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "    except:\n",
        "        print(\"!! Could not download TAR for\", cfg)\n",
        "        continue\n",
        "\n",
        "    # Extract videos\n",
        "    extract_dir = os.path.join(BASE_EXTRACT_DIR, cfg)\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {tar_path} -> {extract_dir}\")\n",
        "    try:\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            tar.extractall(extract_dir)\n",
        "    except:\n",
        "        print(\"!! Extraction failed:\", cfg)\n",
        "        continue\n",
        "\n",
        "    # Copy until we hit TARGET\n",
        "    for sample in train_split:\n",
        "        if saved >= TARGET:\n",
        "            break\n",
        "\n",
        "        video_ref = sample[\"video\"]     # \"cfg/filename.mp4\"\n",
        "        filename = os.path.basename(video_ref)\n",
        "        src_path = os.path.join(extract_dir, filename)\n",
        "\n",
        "        if not os.path.exists(src_path):\n",
        "            continue\n",
        "\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        dest_name = f\"ai_{video_counter:04d}{ext}\"\n",
        "        dest_path = os.path.join(AI_OUT, dest_name)\n",
        "\n",
        "        shutil.copy(src_path, dest_path)\n",
        "\n",
        "        saved += 1\n",
        "        video_counter += 1\n",
        "\n",
        "        if saved <= 10 or saved % 100 == 0:\n",
        "            print(f\"[{saved}/{TARGET}] Saved {dest_name}\")\n",
        "\n",
        "print(\"\\n======================================\")\n",
        "print(\"ðŸŽ‰ CLEAN AI DATASET COMPLETE\")\n",
        "print(f\"Total AI videos saved: {saved}\")\n",
        "print(\"AI folder:\", AI_OUT)\n",
        "print(\"======================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY1XU3K4KxXU",
        "outputId": "fddc9763-301b-497f-ccc9-544687c2481d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cpu\n",
            "/content/drive/MyDrive/genvid_ucf_1200/real_1200 -> 1200 videos (label 0)\n",
            "/content/drive/MyDrive/AudioModel/AI -> 1200 videos (label 1)\n",
            "/content/drive/MyDrive/soravideo/sora2aivideos -> 1250 videos (label 1)\n",
            "Total samples (before shuffle): 3650\n",
            "Manifest written to: /content/video_manifest.csv\n",
            "\n",
            "=== Cleaning manifest: checking video readability (OpenCV) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3650/3650 [23:23<00:00,  2.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kept 3649 / 3650 videos.\n",
            "Dropped 1 unreadable videos.\n",
            "Examples of bad videos: ['/content/drive/MyDrive/AudioModel/AI/ai_0030.mp4']\n",
            "Clean manifest has 1200 REAL and 2449 AI videos.\n",
            "Balanced subset written to: /content/video_manifest_balanced_1200.csv\n",
            " -> 600 REAL + 600 AI = 1200 total\n",
            "\n",
            "######## Training RAW model ########\n",
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:00<00:00, 147MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0290.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01023.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0dc42ae08191945e67e5bba3c273_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0975.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00185.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00510.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01121.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1165.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0876.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d321021e48191a2bee104e37e4c92_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fcbfa48191a5a96128b4db0c8e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c66fb45c48191bbc40de87bc1f26e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00076.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00461.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc031c2908191bde0923022b59ba6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690beac1527081919dfc8c2deef1d1fa_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1010.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d18526fc48191b4ed47f29a5c72bc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00465.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01013.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31deab1081918886ac9b8a15a12d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00959.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfa591dcc8191a0fdc980cb8965c1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01014.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0743.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00320.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0080.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00729.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00677.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0706.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00135.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00301.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d1725081919f11f48976aafffc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00099.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d511d88191b3b8a50cbecf5c7e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00448.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00146.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00642.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d03b6c819199eebcbefe34ed9b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00091.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [10/240] Loss: 0.5301\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0826.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c1129ca448191b450daf1e622aad5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c06e741d881919d6273ac1ba69a6a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00900.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3203041c8191871c6a4fe0209847_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00926.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3b0ff2a0819182970b5232541a88_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00138.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0720.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00698.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320320548191b1cce940aeb762c1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fdcd048191908aa0f794db9eb4_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00807.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00104.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00459.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0914.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00545.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5a4d7bb48191b11e935b15f5880b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00631.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00357.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01173.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00354.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00619.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00574.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00762.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00118.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00825.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01186.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1191.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5a2048191b02f1b41ebf1d949_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01175.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1195.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c25fe15608191a9c5e07053424c2d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00598.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd6b1111c8191a82a179a05bc9fc6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0119.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00857.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00294.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01141.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1094.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [20/240] Loss: 0.2798\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00035.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00749.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01142.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c097502908191b43c94ac837152dd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be98043ec81919d5636e456b7062a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00101.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0700.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00621.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00970.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0768.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00695.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00027.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00248.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e56ed88191b85f8f3568e005b7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a27bc91c481918f725ef9f621b02f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00767.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c21c31d0081919628575ebfee1410_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d9acec8191ba99fa72a75a1a41_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00344.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0538.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0684.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0537.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0840.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00732.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cf9c4faf48191a46cd41ac620202c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e34b6c8191af74c9d942c65538_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3213ee9c8191809d32b39609bef1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0471.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00722.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0501.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0521.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00405.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5494c81919859c21ad68433ce_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dc6b748191a5837c4ab5a375f1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0509.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00086.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5ba08819182f75c48f1275a43_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c342aa0d4819197e9c3d80159c841_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2374c4c88191aa34b9e3bfd4ad7e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00066.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [30/240] Loss: 0.1580\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00925.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00271.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00430.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c337869c48191a70424f62858f188_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00193.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2657d3ec81919d64c96fa3836419_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01144.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0144.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb3119db08191b3ccf2fa99331b54_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01010.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1181.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3d2714708191ae3c8f49459bc201_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00231.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00562.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32009eb8819190ba2909ae5d1002_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00888.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00803.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00763.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0831.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fea2108191b3ae584fc407e53d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00449.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00810.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0850.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dfda8081918e4c3f9d82ff281a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1173.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0444.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01090.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0761.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01051.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dcfa308191adc9ac32722d8002_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0055.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c588ef7ac8191ab2b75d419e99e10_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00250.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffce80819181ece6f20f03ae8a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0920.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00650.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00026.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00987.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00547.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0301.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [40/240] Loss: 0.1004\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d826b081918ad096103a74c833_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0862.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00837.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00593.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00963.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0434.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3b9cb19081918e2733f75bcc4d2c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01059.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0482.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320dc9608191ae1ba4a1e7faa182_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00589.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00219.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00524.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01176.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dbfcec8191a33d369cfd8ba585_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00335.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00208.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cbfc44c148191b2ec3adfb3848176_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00359.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a2c7d00d88191a03ab14f6d992153_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00191.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01190.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0369.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320abfb48191a5f7ab8720851d60_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3206b7c88191b39b13b11b97e3ea_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb2f955208191adf8b68551b51f3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d1a70c24c81919202a08a5f47d0ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0800.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00498.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00890.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00579.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1046.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ecb66481918ca9085b1e8a66a5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0640.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00366.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00015.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01113.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1096.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3d4b33d88191b31b3d3df2d5ba76_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3200ff6081918e304ed786b3de3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [50/240] Loss: 0.0710\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00170.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00313.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00145.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0572.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00216.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00381.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2e9ca81881919e2bf5c7fb519866_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01102.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00476.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1099.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00117.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01170.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01140.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00291.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00120.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0514.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01172.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01151.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00703.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1178.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d730348191a3826cd41b136512_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cae7d04308191925ceb62d7f35053_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2024f5c88191aa99341486b11b08_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0137.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bee3ee93c81919ce74df4be54ae19_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e720508191a160fb434385543d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00220.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00312.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01001.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fdcff881919d1169fad6a4fee7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0524.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01118.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6907b3d4c7ec81918ffc6e651bbd724d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0326.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00901.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00846.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00013.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1080.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00770.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0130.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [60/240] Loss: 0.0543\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320413648191927ab6e9e96727d7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00273.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01124.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0052.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00496.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01152.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00149.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c19d96a0c8191a74a189fb3281e1a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0263.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1055.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00331.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f302448191a2b21a173e3d5961_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0157.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00802.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00102.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00724.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00721.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c54ce0cf08191b52dd037279b1573_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00364.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cf866935c8191817f220f8cf6ec98_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0100.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00531.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0078.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0630.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e0887c8191b62343a462d9772e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00370.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ccef99d04819182e89672496d4782_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00372.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00860.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00911.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00700.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0813.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00358.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0719ae7c819191de49b0f4883dd2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31eceb6c8191b90aea6fb24b4519_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01132.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00167.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00720.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00986.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffa500819186e6096a15adbfa2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [70/240] Loss: 0.0439\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfa1efdf88191bd989133c3475432_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01094.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00418.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1109.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00420.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00048.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00623.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00936.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01067.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e55f7481918664817f266aa980_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01037.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0472.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00847.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00155.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0393.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00551.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00540.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00805.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01167.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0617.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00203.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01076.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00773.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00043.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00738.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0390.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0764.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0479.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cbcda45f88191a2df5e6919e95fdd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00327.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0f31866481919d2235f18d7a20b4_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00419.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0293.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f8d76c8191b11aa97e2782ebb1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0821.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00835.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1024.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01050.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00373.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00873.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [80/240] Loss: 0.0367\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00225.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4b263860819184babbd74a80e291_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320a53dc8191840e5ba71449bd92_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0579.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c84bd4d0481919f1075f8ff02d5fa_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d024848191ae2d9f5244d5b39c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00468.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0547.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320d4d148191a4f4aa792a3678fd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00417.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0883.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0232.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00820.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00071.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00495.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f945d48191be44594acbc58924_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00824.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f2166081919863a7aff996707d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c30fade908191b05e74f539582b65_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00169.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1065.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0255.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01180.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00222.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0731.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01095.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6909438bddb88191ae556a7e6236680b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0991.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0065.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00608.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fb681c81918774357846880fc9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0046.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d5faa88191a050064fbfe04c0f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00972.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00060.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0857.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00941.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ea94388191b2ed64c6c94d4c8e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0219.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00266.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [90/240] Loss: 0.0314\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00813.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0433.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00928.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00280.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00472.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00912.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0567.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00050.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00696.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00649.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d162962ac8191adba11e0c000f510_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cfd9848191b66173894b9e06c5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0a4bff7c8191a8a4f5d182e8f13c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00226.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01079.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01028.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00033.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0492.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0299.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0788f38481919532fc50fd9c8a78_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf688589081918784a569d0af3f0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0671.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00755.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00484.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00020.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0303.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0957.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e0005c8191b2a5d7b5c70e3e7a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01166.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00578.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0954.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0251.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0365.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00736.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c232b4d0c8191b254cb1afaeaa236_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00121.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc58d63fc819183c37bc3e3ee942f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a8106499c8191b0700f8147bfd24f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00515.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0942.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [100/240] Loss: 0.0273\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00181.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5c234f448191ad6b438db9d25841_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fb0e8c8191bbc4de4fcc6d9ece_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0535.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0583.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0223.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf4ed41d881918a83b095732727aa_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0923.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01055.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0319.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00512.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0209.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1076.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00798.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1113.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0441.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00636.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0964.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0827.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00016.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01043.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32023a008191b3925cbcef8d5c44_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0820.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00003.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d0fbf081919b73aa633c34ca64_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfea6dcec8191a4e2399f3f92722d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01065.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00884.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00743.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c8a63ef1c81918f8ede0b1e1eeecc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0911.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00147.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00881.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00645.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00437.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fd22008191a91c2348d4df73f2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ed99e481919a5890688e311fd5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c55ddd9f08191bad59970583017ab_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00796.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00883.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [110/240] Loss: 0.0241\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00288.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01015.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00499.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00942.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320e51b48191aca16b57ef9a83de_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00447.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2be26928819187fa68918a2c81d1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0990.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0784.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0848.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00078.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00414.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d17ae48191a299076c50a750ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dec3708191aaf86b7f4e75bc37_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00183.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0750.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00633.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01146.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0598.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00137.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0866.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c097878608191936ce982c577dc58_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00740.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb308f934819194641a1fc04845cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01056.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00151.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00843.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1187.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00946.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00067.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d163e911c8191a32ddcbd62a6e177_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00019.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00563.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00573.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e757bc8191a4c6a980b95eb650_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00163.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0495.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00446.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00517.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00716.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [120/240] Loss: 0.0215\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0043.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0431.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00886.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00180.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0076.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bee48d9b481918455a68444fcbdc3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00475.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320bd8a8819183ede25229a49524_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c9135e0548191b7af5e6bf480ec22_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1166.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01035.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1164.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00293.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00961.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2982d1d0819180e3c4c24d56c273_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c46d4ae948191a7e30e5e06eb7cd3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cecf278088191be1fdf878922aa34_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00853.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00746.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6907ad3ead188191a7f954de5b268712_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c13e96ea88191bdefb919323e9ae9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00694.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00055.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00537.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00238.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00513.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0b4f92a881918032848a7bdf32eb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3204275081918f1fe050a0568f0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0233.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be6ec75b88191b54b5345a05bc4cd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d55140819196f797fc71ce0e9b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01096.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01092.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0740.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cf71b081919d71b98e8a1ac657_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00937.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00927.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31db13b48191b7f63dae3f75b655_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0415.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00823.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [130/240] Loss: 0.0193\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01159.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0363.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00171.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00267.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00685.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00586.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00896.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00140.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00065.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0286.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00411.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd172cfac81919411c9a872bc035f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0529.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0386.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c91a2ed948191ae607522e5cdcd3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00283.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00760.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01000.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00125.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00202.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3205114c819183465de359d74e6c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0036.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1142.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01193.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00488.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1150.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0653.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1198.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0280.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00045.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0151.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00607.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00833.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0679.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be8eef6ac8191840f9d0b89259f68_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00527.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0192.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0802.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fb86f08191a4b7b16c8b24aae5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3208f37c8191985304b0df4b953c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [140/240] Loss: 0.0175\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0616.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb3022e148191b73aa4347866146f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0562.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00398.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00652.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0992.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00130.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1120.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00784.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0648.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1097.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec0ba0819191653324a90e898e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00296.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00727.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31db623c819187b843311a86ed27_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0686.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00305.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0412.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00507.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00483.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00105.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1040.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0453.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00304.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0237.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3210f50c8191ab69b415f40ffa0b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0557.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01105.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1038.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f4dbec8191ae5182965ab9a144_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f33adc8191946e1c0dbfede89d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00082.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00682.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00024.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00950.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01033.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0051.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00944.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00668.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caea294f08191af29811316888934_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [150/240] Loss: 0.0159\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0176.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00387.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00590.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00967.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01109.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31edf6b88191af9741ed7262b3cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1058.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce8542e18819190d5f3624c8faff6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c05fd87ac8191a7d0c6b2d0eb69c0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00907.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00787.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0461.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00848.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01053.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cefc1d85481918c53d091fd993b7b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0569.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0266.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01174.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0447.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0930.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00660.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0105.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01020.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00134.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0805.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f798d48191845cafae541f9b60_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0511.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0917.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0746.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00074.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00880.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01012.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1163.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00943.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c026f1c648191a0743501f0015d38_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0372.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01164.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c042dfd688191b5e7d3bff9a6b9c2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cdceafec88191adb2076c6f303768_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0056.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [160/240] Loss: 0.0146\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01130.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c41dae5e48191adc37588f487c71e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0781.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00678.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cf985c8191b43eccffde5b3170_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00822.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0830.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0986.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01019.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc512ae108191ba476e326d1d79ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3202297c81918d618e6d95984d40_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32032efc8191a1c06dddf1244c9f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0126.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01177.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00530.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0454.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0546.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0446.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00783.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00038.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00778.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320fcd04819193e607e3760e9b83_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00854.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00029.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00188.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01158.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0658.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01025.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1176.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0193.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffddc88191ad0cf7c2de5f8e48_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00764.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0527.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00480.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01123.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00189.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d086688191824e9b625e1ae5c9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00664.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690973289d808191ac95c78b519cebe3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00673.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [170/240] Loss: 0.0134\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0486.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00122.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00037.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00719.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00761.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00558.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00596.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dbd2e08191b9c053193a91da62_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00954.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00794.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0053.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ea0ec88191a4fcc527752e1541_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00529.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00148.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01160.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c57f50c5c81918c553a3571977d4b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00124.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0814.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0187.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00879.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3206c024819195970fd119c22bfb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0315.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c79035e9881919c94de5c877a8475_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00069.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320fd6e48191bf7e71e70d5cf075_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be69b5f148191974e9f2adb0943bf_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00514.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c36a059a08191a5d87880aec3e5c7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00142.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00929.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01085.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1141.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00789.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00389.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01171.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00522.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320c6c448191a88a676152f3c2fd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32140624819194c96c8e43f1661c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31da52188191ada8c028f83fe848_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0413.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [180/240] Loss: 0.0124\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00080.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00324.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00815.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4d69fb2881919807b3771c0231c2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00383.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0667.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce58a60a48191a01ed133fa61a444_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0488.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0350.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00201.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00477.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00340.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00333.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ee79488191a7a34fa4ae466e70_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00127.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00599.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d037a4204819194feb012bbcdcafb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1098.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0127.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0217.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00625.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0467.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00521.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31df2b708191b500f7ff41d6fdeb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00186.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0039.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0283.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e453648191ba8a2ffccb9b3edd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0623.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00036.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0585.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4c257db881919d1dba72aa1d1f1d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00509.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e26b108191bb363e1fcb64eb0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32049efc8191bdefd2517440829a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00924.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01115.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d2cc8881919911db5031f4f3e7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0322.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01071.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [190/240] Loss: 0.0115\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00249.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0481.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0519.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00409.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00457.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00647.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e1663081919f1ca66510d3058b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0560.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00150.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00903.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320f1944819189359b67cf953189_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0113.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d0f34e1e08191a8db412c8ee82d28_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf51ad1508191b9997552c74c3729_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00956.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01110.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0685.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00733.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1002.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c039a1a408191a8ed46ed10a83dbf_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00187.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c9d0e423c8191bf04cbe0f5d97a42_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0324.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00157.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00255.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00310.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00875.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00819.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e8c2108191af7257adcbc6e1cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01179.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01040.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00034.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2c373bb48191b00fa4da1267ceca_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0106.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caa289c8c8191ba6fef17064d9578_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0929.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0308.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bedb659ec8191a663bf4267144f56_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00584.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00338.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [200/240] Loss: 0.0107\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3213a6f88191970d989b0642efdc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00697.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0334.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00297.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0374.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00070.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1045.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00252.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce785c7688191ab806ba99501a6a0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00454.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01081.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01057.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf708dad88191a12793e2d2133b12_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01009.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01184.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec51a08191b0ad6f8ea3d9c303_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00490.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0962.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00211.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00768.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0332.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00302.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc298eb80819181cb0dea1570b9f3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0588.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0733.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01198.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00429.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00613.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf4ff6a8481919a61b0bc76847601_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00407.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00285.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0754.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00432.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0470.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01064.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00996.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc169ddf88191832982f0c750577b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ddcfb881919aadadcb5df9308b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0933.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be5174cc88191be82eedbddd84dd8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [210/240] Loss: 0.0100\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00493.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e000b08191ae0664f12317e198_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00413.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e09b8081919dce0c604f8c7cb5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00000.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1011.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00525.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00718.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00669.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1138.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00861.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d1b2b3f0c81918d18c9561ee5488b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0626.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00958.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01072.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f7935481918bd499bdfce8fe7d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5b1e59888191af5ce0654aceb2c8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01163.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf6d1f2b88191ac550089dbf5b638_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0947.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01044.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0931.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00439.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1158.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ccc82881c8191b7e34ec5a7dacfe1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd7aaa5d48191912c8a97dd22c3cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00889.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0111.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1033.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00962.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0247.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0983.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f9bdd48191901ccb03bcbe3cf8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3200effc8191957642ac76e79186_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00234.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31eddbe88191a9384a5a215e734c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00182.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00725.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c1bfa1be48191a55ece75704f4212_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00245.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [220/240] Loss: 0.0094\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0506.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c38020fd88191855d4bbe10861ad1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00840.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690931b556a4819199e18f0635b9de9c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00332.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0282.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01199.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00018.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00571.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00328.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c15ee5d4481918757faec9bef0ece_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01148.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3210f9d88191b41be7a4947d15f2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00256.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ed637881918088ee4407f05c77_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0661.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0791.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0666.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00485.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0201.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00200.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00844.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00023.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0014.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00108.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00717.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32052b7881919914cc5f7ccd6fa7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01104.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01083.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0150.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00270.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0528.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d02de8819190f09fc3b4832044_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01022.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00671.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0865.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0730.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00575.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00841.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0370.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [230/240] Loss: 0.0088\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00032.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1050.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0636.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00582.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00139.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01120.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c831875ec8191b180b0406f6b46e0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00632.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00554.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32113a1c81918e6f377d0a2514b0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0153.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e828888191955e2d6059c357d6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caf9f14f48191aa8af41ad2e8d261_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00850.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00451.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00388.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01088.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3f2ab71081918c1871f06168ceeb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c393a2b1c8191b41aa13c907d7e08_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00656.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ca39272048191bac43f2b58aca89d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0792.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00228.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0563.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e1cf148191b89ae619cf0cc79e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec96c08191b78e2d587ac1245c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1148.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00851.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1179.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00572.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0245.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0610.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c160c49bc8191886ec7c6d2cca9a1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00520.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00054.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0458.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1069.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01087.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00056.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c8517eac08191a9d8885a3edb25ea_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW] Epoch [1/1] Step [240/240] Loss: 0.0083\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d7b29881919a7f005b07fcd120_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1091.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0185.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01128.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00588.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0184.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0526.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00952.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0035.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01062.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00676.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00868.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0659.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0318.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00049.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d1bd1f36081918d20f1e23fc194d9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00902.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f9bab08191a678c908eafc7d51_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf7b3c0408191bd092f97b2638442_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00004.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1023.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3201da588191805287be2f2eeeca_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01181.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f1fe6c81918c28f6189738c50b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00092.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0497.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0512.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00921.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00470.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1088.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0175.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0751.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00592.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00207.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00976.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00769.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00887.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320eb41c8191a37b5bc7365fecc4_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00898.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c162969a88191b540befd9b7c08c6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0680.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00691.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0079.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00601.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00905.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00129.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00659.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0267.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00980.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01161.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1167.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01194.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2e8f90d881919b8edb53d3b88d12_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00159.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00869.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00427.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dd3ba48191b21b3f0906d0aa75_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00244.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c29690acc81919605703fffc9feb8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0870.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00968.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc208f780819185d087900ff2c6b9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01038.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0491.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fa55448191b4375bfbae1d4c08_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00460.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00567.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1092.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00670.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00422.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00728.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0619.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0448.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00110.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0452.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0314.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00938.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00274.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1032.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00330.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00380.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec28f88191b7ce0a8160ff41cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00423.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00410.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00626.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00010.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00662.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01150.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0252.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00908.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf2a2a0648191a2c0b2a36c3ecf7d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00940.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c456b565881919a0171763e8cee1c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00385.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00765.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31de21e881918829e8ca0bad3cf0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00581.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c322cceac8191943b3bb574d837d0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0140.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00247.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00107.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0564.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0907.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00953.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01185.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be52f8e4481919df33ddabdef8f62_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01063.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00644.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1043.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00406.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0890.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00360.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00192.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01187.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c36450eb081918a5544687163b831_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6908c76d86d88191a03ba2bc76ff9068_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00948.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00600.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00295.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00541.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01089.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00239.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320d352c81919fb6c4b7e3198bce_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0765d864819197a2ddd65176a4d7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00583.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1082.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00557.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0676.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00287.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00955.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01182.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ba9d1655081918fdd72725da25e8b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00713.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1107.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00179.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0325.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00362.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00741.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00438.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d59b9c819186a55bd59385b38d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce0df377c8191a2b87a2a9493fa0b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0d54a9648191b1c541db60a88cd4_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0271.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bb2e7acf0819182b9022210a5f65c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01147.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00706.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01134.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caf7a4d58819199913221e6b8ba42_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01031.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00863.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00931.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00299.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3210458881918433fbfb2cdd00d5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6905d93caf1c81918d1f0279bc2b94b7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00123.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00991.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00528.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00688.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0503.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00791.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bffe6bb6c81918f4e3d8698ad8a4e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00322.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f4f56c8191ac817c282617ac42_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0985.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a51c9bb30819193ce70bff9e0f7e3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cec3427b48191975a2937cf3cd59d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00957.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0023.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0728.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00221.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c9c2817788191bca46c9dcc07524a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0808.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320fb9b48191a4e4e61f74ae5008_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00486.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d1baf881919a10a6c16792fd23_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1119.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00063.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0832.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0633.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00093.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00083.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0605.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00614.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f4f36881919b9ba428ef828e7f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c53126f308191ac67a077cf200841_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0969.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00546.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01006.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0060.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01002.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00002.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00025.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1140.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fa61a481918cac419aa7a87e19_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0745.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0881.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00836.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690b9ea415a08191a84d477d9f256eec_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01045.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3204d9588191a37ba84b105f5566_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cf6ede1d88191a75cf64fb0962a68_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00253.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00684.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00595.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f302748191978937c63983bd4a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f377488191a0cb2f2ff9bf7772_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00265.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00394.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0212.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01027.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfe8f258081919447b910b44989c3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00052.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0896.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ff61b48191a2430bf5b085a8e7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31def98c819198bd8e0851a0f05e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c1b769984819192498f18845583e3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00292.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00665.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0620.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fc28848191818aa6fb75af963b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00661.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00241.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be79eed1081919c597edf8a293a2b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320f248c8191b07649bb6b1d9801_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c09c2fd3c81918cc32ad1095fab05_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00702.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0cda0ab08191861e8bd78914d2db_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32035c9c8191a1bb0789c26f8575_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00922.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0063.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c746cf30c819193b4d343df34c8cf_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00165.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0178.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0811.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0234.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cfbff08191acbc7ec36c939b25_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00367.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3206d74c81919c9ba2697fb93d4b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2f908c6c8191b0ebc605bce667be_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00355.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[RAW-VAL] Loss: 0.0087 | Acc: 1.0000\n",
            "\n",
            "######## Training MID model ########\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2982d1d0819180e3c4c24d56c273_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c06e741d881919d6273ac1ba69a6a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0929.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32049efc8191bdefd2517440829a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1191.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00589.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00228.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01056.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31db623c819187b843311a86ed27_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00760.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00203.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0308.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00485.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caf9f14f48191aa8af41ad2e8d261_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e000b08191ae0664f12317e198_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00414.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d1725081919f11f48976aafffc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0286.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0217.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0151.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0585.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf51ad1508191b9997552c74c3729_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1138.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3206b7c88191b39b13b11b97e3ea_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31edf6b88191af9741ed7262b3cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00582.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00080.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00946.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01120.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01174.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00163.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00050.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00807.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0100.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320a53dc8191840e5ba71449bd92_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32140624819194c96c8e43f1661c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00381.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00729.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c54ce0cf08191b52dd037279b1573_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0648.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [10/240] Loss: 0.3527\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00302.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01158.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01065.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0820.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ea94388191b2ed64c6c94d4c8e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00861.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01193.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01132.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e8c2108191af7257adcbc6e1cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00018.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01175.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00815.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0914.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00954.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0700.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0730.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0157.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0481.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c19d96a0c8191a74a189fb3281e1a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00038.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1150.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1163.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1076.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00056.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cbcda45f88191a2df5e6919e95fdd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00191.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00669.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3d2714708191ae3c8f49459bc201_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3200ff6081918e304ed786b3de3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00340.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0754.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0567.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d03b6c819199eebcbefe34ed9b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e720508191a160fb434385543d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0821.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e55f7481918664817f266aa980_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01199.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00850.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0461.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0322.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [20/240] Loss: 0.1814\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0990.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00881.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0792.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0883.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00554.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00150.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00070.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0458.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f7935481918bd499bdfce8fe7d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2c373bb48191b00fa4da1267ceca_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00398.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00907.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0431.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5ba08819182f75c48f1275a43_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00698.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00749.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0528.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01000.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f4dbec8191ae5182965ab9a144_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01142.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1011.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320413648191927ab6e9e96727d7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c9d0e423c8191bf04cbe0f5d97a42_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00798.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0137.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01121.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00271.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31deab1081918886ac9b8a15a12d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00820.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00625.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00875.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00515.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1109.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01057.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01059.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0247.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0105.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0486.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1033.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0720.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [30/240] Loss: 0.1033\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01180.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01023.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3f2ab71081918c1871f06168ceeb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0263.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0588.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0080.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d55140819196f797fc71ce0e9b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00633.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01083.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00903.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00950.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01076.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0495.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfea6dcec8191a4e2399f3f92722d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0290.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0434.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d1b2b3f0c81918d18c9561ee5488b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0251.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0992.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0153.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00678.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00134.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1141.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0363.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00065.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01043.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00477.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f2166081919863a7aff996707d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00778.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00762.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0563.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00642.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0527.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1058.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00373.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0043.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00124.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690931b556a4819199e18f0635b9de9c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01173.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1148.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [40/240] Loss: 0.0670\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0524.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320fd6e48191bf7e71e70d5cf075_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00755.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0471.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00631.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0280.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00370.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1113.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00069.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00696.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01146.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0583.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00927.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf4ff6a8481919a61b0bc76847601_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0850.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690caea294f08191af29811316888934_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be8eef6ac8191840f9d0b89259f68_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec51a08191b0ad6f8ea3d9c303_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0562.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00171.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0488.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00835.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00331.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0876.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00682.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0447.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cefc1d85481918c53d091fd993b7b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5494c81919859c21ad68433ce_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00305.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01171.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00695.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00810.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00167.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00048.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00125.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c55ddd9f08191bad59970583017ab_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6909438bddb88191ae556a7e6236680b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0051.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00283.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0315.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [50/240] Loss: 0.0483\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00285.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01113.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0653.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0492.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d18526fc48191b4ed47f29a5c72bc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0454.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0446.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0957.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01179.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00510.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d511d88191b3b8a50cbecf5c7e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c232b4d0c8191b254cb1afaeaa236_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00524.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00770.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0319.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00517.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00280.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01067.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1195.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fb681c81918774357846880fc9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d162962ac8191adba11e0c000f510_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01190.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00572.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00045.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01085.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc58d63fc819183c37bc3e3ee942f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00000.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0964.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0126.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0800.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c8a63ef1c81918f8ede0b1e1eeecc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00996.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00725.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00520.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0113.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00328.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00338.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00890.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00372.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00496.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [60/240] Loss: 0.0376\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00847.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00717.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cbfc44c148191b2ec3adfb3848176_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01095.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00819.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00961.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ea0ec88191a4fcc527752e1541_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0470.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d9acec8191ba99fa72a75a1a41_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00889.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0636.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0453.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00843.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0232.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3208f37c8191985304b0df4b953c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e34b6c8191af74c9d942c65538_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31eddbe88191a9384a5a215e734c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01001.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00130.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c57f50c5c81918c553a3571977d4b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00480.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00673.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d17ae48191a299076c50a750ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0911.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01141.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1046.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00188.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0324.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01144.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690973289d808191ac95c78b519cebe3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00359.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3213ee9c8191809d32b39609bef1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01176.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01166.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f9bdd48191901ccb03bcbe3cf8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0572.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00987.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00335.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3203041c8191871c6a4fe0209847_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00220.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [70/240] Loss: 0.0306\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00900.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00805.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00959.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dbfcec8191a33d369cfd8ba585_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00155.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0283.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c831875ec8191b180b0406f6b46e0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0920.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00147.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1178.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00841.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01102.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00645.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be98043ec81919d5636e456b7062a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0671.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0056.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00121.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0365.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00598.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01020.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a8106499c8191b0700f8147bfd24f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0848.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00186.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00146.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0616.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0537.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00608.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0f31866481919d2235f18d7a20b4_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fea2108191b3ae584fc407e53d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01123.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00718.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00573.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0511.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31db13b48191b7f63dae3f75b655_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0369.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00270.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c36a059a08191a5d87880aec3e5c7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1069.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00411.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0176.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [80/240] Loss: 0.0258\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c337869c48191a70424f62858f188_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00118.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00250.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01160.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00840.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00883.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00170.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c393a2b1c8191b41aa13c907d7e08_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00219.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4c257db881919d1dba72aa1d1f1d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00596.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00574.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00803.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320c6c448191a88a676152f3c2fd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00579.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0667.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00102.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01186.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01110.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00700.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0233.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0386.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e26b108191bb363e1fcb64eb0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00929.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320bd8a8819183ede25229a49524_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0706.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00540.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00169.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0661.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00967.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00420.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0036.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd6b1111c8191a82a179a05bc9fc6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0052.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32113a1c81918e6f377d0a2514b0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00607.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00522.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00429.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00860.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0332.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [90/240] Loss: 0.0222\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0415.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c097878608191936ce982c577dc58_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00632.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00449.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c66fb45c48191bbc40de87bc1f26e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1142.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01092.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31da52188191ada8c028f83fe848_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01040.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01013.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00623.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0509.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1099.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00202.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00071.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0686.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00032.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f798d48191845cafae541f9b60_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0467.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e56ed88191b85f8f3568e005b7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00912.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c30fade908191b05e74f539582b65_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e0887c8191b62343a462d9772e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bee3ee93c81919ce74df4be54ae19_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00383.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00796.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc512ae108191ba476e326d1d79ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00255.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0144.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0187.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0127.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00035.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00527.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0791.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0a4bff7c8191a8a4f5d182e8f13c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2024f5c88191aa99341486b11b08_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffddc88191ad0cf7c2de5f8e48_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a27bc91c481918f725ef9f621b02f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cf866935c8191817f220f8cf6ec98_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00354.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [100/240] Loss: 0.0194\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4d69fb2881919807b3771c0231c2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320f1944819189359b67cf953189_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00140.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00613.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00545.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cae7d04308191925ceb62d7f35053_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00746.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1164.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01167.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01159.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00364.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00926.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0831.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00499.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0444.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01170.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0393.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00962.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00296.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00537.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00943.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00256.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0482.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3202297c81918d618e6d95984d40_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cfd9848191b66173894b9e06c5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00389.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01055.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0546.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00245.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0991.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00148.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d321021e48191a2bee104e37e4c92_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00273.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0209.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0521.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0519.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0301.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00802.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00970.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00719.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [110/240] Loss: 0.0172\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00685.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dc6b748191a5837c4ab5a375f1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00901.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1098.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00490.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00813.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd172cfac81919411c9a872bc035f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01035.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00636.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d1a70c24c81919202a08a5f47d0ff_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00291.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00043.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00105.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00033.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0813.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00181.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00310.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5a4d7bb48191b11e935b15f5880b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0658.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00468.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0334.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00027.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0557.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1096.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00767.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0046.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f945d48191be44594acbc58924_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00222.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01109.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f33adc8191946e1c0dbfede89d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0299.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00942.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00037.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0719ae7c819191de49b0f4883dd2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01064.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cf985c8191b43eccffde5b3170_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00447.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c039a1a408191a8ed46ed10a83dbf_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1176.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01053.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [120/240] Loss: 0.0154\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c1129ca448191b450daf1e622aad5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00127.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0764.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c21c31d0081919628575ebfee1410_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00483.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320fcd04819193e607e3760e9b83_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00313.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320dc9608191ae1ba4a1e7faa182_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfa591dcc8191a0fdc980cb8965c1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c46d4ae948191a7e30e5e06eb7cd3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00054.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c160c49bc8191886ec7c6d2cca9a1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31cf71b081919d71b98e8a1ac657_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00853.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00886.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00854.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00151.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1158.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d5faa88191a050064fbfe04c0f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00727.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ca39272048191bac43f2b58aca89d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31df2b708191b500f7ff41d6fdeb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00013.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01124.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3205114c819183465de359d74e6c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0931.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00484.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0433.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1080.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00135.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3210f50c8191ab69b415f40ffa0b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1173.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01033.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32032efc8191a1c06dddf1244c9f_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00060.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e1663081919f1ca66510d3058b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00888.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00584.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c79035e9881919c94de5c877a8475_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00823.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [130/240] Loss: 0.0139\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00461.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1187.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01044.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e09b8081919dce0c604f8c7cb5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00924.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ed99e481919a5890688e311fd5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00784.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0962.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1045.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00145.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00671.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1181.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f302448191a2b21a173e3d5961_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0666.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01028.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320abfb48191a5f7ab8720851d60_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00193.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00721.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00187.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2e9ca81881919e2bf5c7fb519866_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0193.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00200.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0412.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00294.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0303.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ecb66481918ca9085b1e8a66a5_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00182.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01148.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0802.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00120.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00668.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0326.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00722.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec0ba0819191653324a90e898e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00547.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00137.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00320.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00357.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0623.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00015.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [140/240] Loss: 0.0126\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e1cf148191b89ae619cf0cc79e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d163e911c8191a32ddcbd62a6e177_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0733.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00034.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00562.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00446.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00293.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00266.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fdcff881919d1169fad6a4fee7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bfa1efdf88191bd989133c3475432_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00076.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00495.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00936.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cdceafec88191adb2076c6f303768_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01072.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00138.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00972.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d0f34e1e08191a8db412c8ee82d28_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00768.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf4ed41d881918a83b095732727aa_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00551.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0282.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fb0e8c8191bbc4de4fcc6d9ece_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01088.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00231.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0569.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00003.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00327.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ccc82881c8191b7e34ec5a7dacfe1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00521.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690a2c7d00d88191a03ab14f6d992153_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00794.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0219.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c91a2ed948191ae607522e5cdcd3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00880.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00677.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00944.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2657d3ec81919d64c96fa3836419_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00457.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00078.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [150/240] Loss: 0.0115\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be6ec75b88191b54b5345a05bc4cd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00142.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1097.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0078.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c13e96ea88191bdefb919323e9ae9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0130.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0192.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0788f38481919532fc50fd9c8a78_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5c234f448191ad6b438db9d25841_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00226.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0917.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00086.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00225.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00649.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01071.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf6d1f2b88191ac550089dbf5b638_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0983.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0065.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c1bfa1be48191a55ece75704f4212_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0111.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00619.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c84bd4d0481919f1075f8ff02d5fa_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00475.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320320548191b1cce940aeb762c1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be5174cc88191be82eedbddd84dd8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce58a60a48191a01ed133fa61a444_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00720.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c0dc42ae08191945e67e5bba3c273_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0626.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3206c024819195970fd119c22bfb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0640.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1179.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00738.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ec96c08191b78e2d587ac1245c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0743.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01022.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00928.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fcbfa48191a5a96128b4db0c8e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0293.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0740.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [160/240] Loss: 0.0105\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320e51b48191aca16b57ef9a83de_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00660.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0746.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00621.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0942.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0390.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0975.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0223.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0055.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01105.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bf688589081918784a569d0af3f0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0830.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00884.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00139.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0150.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01172.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0370.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1120.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00789.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00301.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01037.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00430.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01152.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0201.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00074.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6907b3d4c7ec81918ffc6e651bbd724d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c8517eac08191a9d8885a3edb25ea_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00763.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00512.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0923.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d320d4d148191a4f4aa792a3678fd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00476.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0826.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb308f934819194641a1fc04845cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01163.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0947.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0535.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1024.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00514.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0514.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [170/240] Loss: 0.0097\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00448.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00941.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3b9cb19081918e2733f75bcc4d2c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f8d76c8191b11aa97e2782ebb1_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0679.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0529.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31fd22008191a91c2348d4df73f2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c41dae5e48191adc37588f487c71e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0866.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00773.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00417.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ccef99d04819182e89672496d4782_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01025.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01115.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00761.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0106.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3200effc8191957642ac76e79186_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00108.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01184.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00117.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00091.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00958.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0930.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00488.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d086688191824e9b625e1ae5c9_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00857.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dcfa308191adc9ac32722d8002_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00740.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00267.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3210f9d88191b41be7a4947d15f2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0053.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00694.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01087.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00937.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00020.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00879.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00563.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0579.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0610.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0014.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [180/240] Loss: 0.0090\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffce80819181ece6f20f03ae8a_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e453648191ba8a2ffccb9b3edd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00764.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00297.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0954.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00344.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00324.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d024848191ae2d9f5244d5b39c_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1038.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00956.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb3119db08191b3ccf2fa99331b54_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00530.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_6907ad3ead188191a7f954de5b268712_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00332.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00963.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00157.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c4b263860819184babbd74a80e291_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cd7aaa5d48191912c8a97dd22c3cb_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00732.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32052b7881919914cc5f7ccd6fa7_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00208.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01104.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00238.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3204275081918f1fe050a0568f0d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00986.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00185.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00599.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00288.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0039.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00586.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00409.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00575.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1002.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_1010.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00024.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00697.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c5b1e59888191af5ce0654aceb2c8_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00180.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0538.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c3b0ff2a0819182970b5232541a88_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [190/240] Loss: 0.0083\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00513.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01051.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00733.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cb2f955208191adf8b68551b51f3d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ffa500819186e6096a15adbfa2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0685.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00304.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00122.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00846.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01014.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0827.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d02de8819190f09fc3b4832044_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d3213a6f88191970d989b0642efdc_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0076.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc298eb80819181cb0dea1570b9f3_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00656.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31dbd2e08191b9c053193a91da62_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0374.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0547.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00407.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00925.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0472.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01009.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690bedb659ec8191a663bf4267144f56_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32023a008191b3925cbcef8d5c44_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc169ddf88191832982f0c750577b_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d32009eb8819190ba2909ae5d1002_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00101.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00312.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0266.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01118.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00036.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00388.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01079.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0840.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00019.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01177.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00405.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00454.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00413.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [200/240] Loss: 0.0078\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cecf278088191be1fdf878922aa34_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00082.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00647.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31f5a2048191b02f1b41ebf1d949_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01010.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c9135e0548191b7af5e6bf480ec22_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00216.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01140.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00055.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31e828888191955e2d6059c357d6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c05fd87ac8191a7d0c6b2d0eb69c0_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00023.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00189.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c042dfd688191b5e7d3bff9a6b9c2_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01015.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00571.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0933.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00896.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01130.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00249.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00201.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00472.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00822.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c25fe15608191a9c5e07053424c2d_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0598.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00066.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00664.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c097502908191b43c94ac837152dd_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00525.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00029.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00529.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0506.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00234.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01096.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01081.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0237.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0750.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31d0fbf081919b73aa633c34ca64_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690d31ee79488191a7a34fa4ae466e70_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0768.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[MID] Epoch [1/1] Step [210/240] Loss: 0.0073\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00833.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_01019.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690cc031c2908191bde0923022b59ba6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c2374c4c88191aa34b9e3bfd4ad7e_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00432.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00578.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00211.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/AudioModel/AI/ai_0814.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690ce8542e18819190d5f3624c8faff6_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/genvid_ucf_1200/real_1200/ucf_00493.avi\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690c026f1c648191a0743501f0015d38_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n",
            "[WARN] Skipping unreadable video (first try): /content/drive/MyDrive/soravideo/sora2aivideos/s_690be69b5f148191974e9f2adb0943bf_1.mp4\n",
            "[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# FULL COLAB SCRIPT (balanced subset + RAW/MID/HIGH/GRAD/MULTI)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision pandas opencv-python\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.io import read_video\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Paths (your dataset)\n",
        "# ------------------------------------------------------------\n",
        "REAL_DIR  = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "AI_DIR1   = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "AI_DIR2   = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "\n",
        "MANIFEST_CSV             = \"/content/video_manifest.csv\"\n",
        "CLEAN_MANIFEST_CSV       = \"/content/video_manifest_clean.csv\"\n",
        "BALANCED_MANIFEST_CSV    = \"/content/video_manifest_balanced_1200.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# 1. Manifest builder\n",
        "# ============================================================\n",
        "\n",
        "def list_videos(root, exts=(\".mp4\", \".avi\", \".mkv\", \".mov\")):\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths.extend(glob.glob(os.path.join(root, f\"*{ext}\")))\n",
        "    return sorted(paths)\n",
        "\n",
        "def build_full_manifest(real_dir, ai_dirs, out_csv):\n",
        "    rows = []\n",
        "    # Real videos -> label 0\n",
        "    real_paths = list_videos(real_dir)\n",
        "    print(f\"{real_dir} -> {len(real_paths)} videos (label 0)\")\n",
        "    for p in real_paths:\n",
        "        rows.append({\"path\": p, \"label\": 0})\n",
        "\n",
        "    # AI videos -> label 1\n",
        "    total_ai = 0\n",
        "    for d in ai_dirs:\n",
        "        ai_paths = list_videos(d)\n",
        "        print(f\"{d} -> {len(ai_paths)} videos (label 1)\")\n",
        "        for p in ai_paths:\n",
        "            rows.append({\"path\": p, \"label\": 1})\n",
        "        total_ai += len(ai_paths)\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Total samples (before shuffle): {len(rows)}\")\n",
        "    print(f\"Manifest written to: {out_csv}\")\n",
        "\n",
        "build_full_manifest(REAL_DIR, [AI_DIR1, AI_DIR2], MANIFEST_CSV)\n",
        "\n",
        "# ============================================================\n",
        "# 2. QUICK CLEAN: remove unreadable videos from manifest\n",
        "# ============================================================\n",
        "\n",
        "def quick_check_video(path):\n",
        "    \"\"\"Use OpenCV to just open and grab 1 frame.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return False\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        if not cap.isOpened():\n",
        "            cap.release()\n",
        "            return False\n",
        "        ok, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ok or frame is None:\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def clean_manifest(in_csv, out_csv):\n",
        "    df = pd.read_csv(in_csv)\n",
        "    good_rows = []\n",
        "    bad_paths = []\n",
        "\n",
        "    print(\"\\n=== Cleaning manifest: checking video readability (OpenCV) ===\")\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        path = row[\"path\"]\n",
        "        if quick_check_video(path):\n",
        "            good_rows.append(row)\n",
        "        else:\n",
        "            bad_paths.append(path)\n",
        "\n",
        "    clean_df = pd.DataFrame(good_rows)\n",
        "    clean_df.to_csv(out_csv, index=False)\n",
        "    print(f\"Kept {len(clean_df)} / {len(df)} videos.\")\n",
        "    print(f\"Dropped {len(bad_paths)} unreadable videos.\")\n",
        "    if bad_paths:\n",
        "        print(\"Examples of bad videos:\", bad_paths[:5])\n",
        "\n",
        "clean_manifest(MANIFEST_CSV, CLEAN_MANIFEST_CSV)\n",
        "\n",
        "# ============================================================\n",
        "# 2b. Balanced sub-sampling so training is faster\n",
        "# ============================================================\n",
        "\n",
        "def make_balanced_subset(in_csv, out_csv, per_class=600, seed=42):\n",
        "    \"\"\"\n",
        "    Take at most `per_class` real (label 0) and `per_class` AI (label 1)\n",
        "    from the cleaned manifest and write a smaller CSV for faster training.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(in_csv)\n",
        "    if \"label\" not in df.columns or \"path\" not in df.columns:\n",
        "        raise RuntimeError(f\"{in_csv} must contain 'path' and 'label' columns.\")\n",
        "\n",
        "    real_df = df[df[\"label\"] == 0]\n",
        "    ai_df   = df[df[\"label\"] == 1]\n",
        "\n",
        "    print(f\"Clean manifest has {len(real_df)} REAL and {len(ai_df)} AI videos.\")\n",
        "\n",
        "    real_sub = real_df.sample(\n",
        "        n=min(per_class, len(real_df)),\n",
        "        random_state=seed\n",
        "    )\n",
        "    ai_sub = ai_df.sample(\n",
        "        n=min(per_class, len(ai_df)),\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    balanced = pd.concat([real_sub, ai_sub], axis=0).sample(\n",
        "        frac=1.0, random_state=seed\n",
        "    ).reset_index(drop=True)\n",
        "    balanced.to_csv(out_csv, index=False)\n",
        "\n",
        "    print(f\"Balanced subset written to: {out_csv}\")\n",
        "    print(f\" -> {len(real_sub)} REAL + {len(ai_sub)} AI = {len(balanced)} total\")\n",
        "\n",
        "# build a 600/600 (or fewer if not enough) subset\n",
        "make_balanced_subset(CLEAN_MANIFEST_CSV, BALANCED_MANIFEST_CSV, per_class=600)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Video loading + transforms (raw, mid, high, grad)\n",
        "# ============================================================\n",
        "\n",
        "def load_video_clip(path, num_frames=16, resize=(112, 112)):\n",
        "    \"\"\"\n",
        "    Load a video, sample num_frames uniformly,\n",
        "    resize, normalize to [0,1], return [C, T, H, W].\n",
        "    Raises RuntimeError if video can't be opened.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise RuntimeError(f\"Video path does not exist: {path}\")\n",
        "\n",
        "    try:\n",
        "        # video: [T, H, W, C], uint8 0-255\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Could not open video: {path}\") from e\n",
        "\n",
        "    if video.numel() == 0:\n",
        "        raise RuntimeError(f\"Empty or invalid video: {path}\")\n",
        "\n",
        "    # [T, C, H, W]\n",
        "    video = video.permute(0, 3, 1, 2)\n",
        "    T, C, H, W = video.shape\n",
        "\n",
        "    # Sample indices\n",
        "    if T >= num_frames:\n",
        "        indices = np.linspace(0, T-1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = math.ceil(num_frames / T)\n",
        "        all_idx = np.tile(np.arange(T), reps)\n",
        "        indices = all_idx[:num_frames]\n",
        "    video = video[indices]  # [num_frames, C, H, W]\n",
        "\n",
        "    # Resize each frame to resize, normalize to [0,1]\n",
        "    resized_frames = []\n",
        "    for i in range(video.shape[0]):\n",
        "        frame = video[i] / 255.0  # [C,H,W] float\n",
        "        frame = F.resize(frame, resize)\n",
        "        resized_frames.append(frame)\n",
        "    video = torch.stack(resized_frames, dim=0)  # [T, C, H, W]\n",
        "\n",
        "    # Reorder to [C, T, H, W]\n",
        "    video = video.permute(1, 0, 2, 3)\n",
        "    return video\n",
        "\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W], values in [0,1]\n",
        "    Keep only midtones based on luminance.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    r, g, b = frames[0], frames[1], frames[2]\n",
        "    lum = 0.299 * r + 0.587 * g + 0.114 * b  # [T,H,W]\n",
        "    lum = lum.unsqueeze(0)  # [1,T,H,W]\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask  # zero out non-midtones\n",
        "\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Subtract Gaussian blur to keep high-frequency component.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        frame_np = frame.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
        "        blur = cv2.GaussianBlur(frame_np, (ksize, ksize), 0)\n",
        "        residual = frame_np - blur\n",
        "        # Rough normalize:\n",
        "        residual = np.clip(residual * 2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:, t] = torch.from_numpy(residual).permute(2, 0, 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Sobel gradient magnitude on grayscale, repeated to 3 channels.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        gx = cv2.Sobel(gray_np, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray_np, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)  # [3,H,W]\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "# ============================================================\n",
        "# 4. Robust VideoDataset (multi-stream outputs + retry)\n",
        "# ============================================================\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        manifest_csv,\n",
        "        split=\"train\",\n",
        "        val_ratio=0.2,\n",
        "        num_frames=16,\n",
        "        resize=(112, 112),\n",
        "        use_raw=True,\n",
        "        use_mid=False,\n",
        "        use_high=False,\n",
        "        use_grad=False,\n",
        "        seed=42,\n",
        "        max_retry=10,\n",
        "    ):\n",
        "        self.manifest = pd.read_csv(manifest_csv)\n",
        "        self.num_frames = num_frames\n",
        "        self.resize = resize\n",
        "        self.use_raw = use_raw\n",
        "        self.use_mid = use_mid\n",
        "        self.use_high = use_high\n",
        "        self.use_grad = use_grad\n",
        "        self.max_retry = max_retry\n",
        "\n",
        "        idxs = list(range(len(self.manifest)))\n",
        "        random.Random(seed).shuffle(idxs)\n",
        "        split_idx = int((1 - val_ratio) * len(idxs))\n",
        "\n",
        "        if split == \"train\":\n",
        "            self.idxs = idxs[:split_idx]\n",
        "        else:\n",
        "            self.idxs = idxs[split_idx:]\n",
        "\n",
        "        # Kinetics-400 normalization\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(3, 1, 1, 1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(3, 1, 1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def _load_one(self, idx):\n",
        "        row = self.manifest.iloc[idx]\n",
        "        path = row[\"path\"]\n",
        "        label = int(row[\"label\"])\n",
        "\n",
        "        frames = load_video_clip(path, num_frames=self.num_frames, resize=self.resize)  # [C,T,H,W]\n",
        "\n",
        "        streams = {}\n",
        "\n",
        "        if self.use_raw:\n",
        "            x = (frames - self.mean) / self.std\n",
        "            streams[\"raw\"] = x\n",
        "\n",
        "        if self.use_mid:\n",
        "            m = midtone_mask(frames.clone())\n",
        "            m = (m - self.mean) / self.std\n",
        "            streams[\"mid\"] = m\n",
        "\n",
        "        if self.use_high:\n",
        "            h = high_frequency_residual(frames.clone())\n",
        "            h = (h - self.mean) / self.std\n",
        "            streams[\"high\"] = h\n",
        "\n",
        "        if self.use_grad:\n",
        "            g = gradient_magnitude(frames.clone())\n",
        "            g = (g - self.mean) / self.std\n",
        "            streams[\"grad\"] = g\n",
        "\n",
        "        return streams, label, path\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Robust: retry a few times on different indices;\n",
        "        if it still fails, return a dummy zero clip so training\n",
        "        doesn't crash.\n",
        "        \"\"\"\n",
        "        for attempt in range(self.max_retry):\n",
        "            idx = self.idxs[i] if attempt == 0 else random.choice(self.idxs)\n",
        "            try:\n",
        "                streams, label, path = self._load_one(idx)\n",
        "                if attempt > 0:\n",
        "                    print(f\"[RECOVERED] Found good sample after failures; last good path: {path}\")\n",
        "                return streams, label\n",
        "            except RuntimeError as e:\n",
        "                if attempt == 0:\n",
        "                    print(f\"[WARN] Skipping unreadable video (first try): {self.manifest.iloc[idx]['path']}\")\n",
        "                continue\n",
        "\n",
        "        # Fallback dummy sample\n",
        "        C = 3\n",
        "        T = self.num_frames\n",
        "        H, W = self.resize\n",
        "        streams = {}\n",
        "        if self.use_raw:\n",
        "            streams[\"raw\"] = torch.zeros(C, T, H, W)\n",
        "        if self.use_mid:\n",
        "            streams[\"mid\"] = torch.zeros(C, T, H, W)\n",
        "        if self.use_high:\n",
        "            streams[\"high\"] = torch.zeros(C, T, H, W)\n",
        "        if self.use_grad:\n",
        "            streams[\"grad\"] = torch.zeros(C, T, H, W)\n",
        "        label = 0\n",
        "        print(\"[FATAL WARN] Too many unreadable videos; returning dummy zero clip.\")\n",
        "        return streams, label\n",
        "\n",
        "# ============================================================\n",
        "# 5. R(2+1)D: backbone + multi-stream classifier\n",
        "# ============================================================\n",
        "\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "def make_backbone():\n",
        "    weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "    model = r2plus1d_18(weights=weights)\n",
        "    feat_dim = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, feat_dim\n",
        "\n",
        "class MultiStreamClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Can act as:\n",
        "      - RAW-only (use_raw=True)\n",
        "      - MID-only (use_mid=True)\n",
        "      - HIGH-only (use_high=True)\n",
        "      - GRAD-only (use_grad=True)\n",
        "      - or any combination (multi-stream fusion).\n",
        "    \"\"\"\n",
        "    def __init__(self, use_raw=True, use_mid=True, use_high=True, use_grad=True):\n",
        "        super().__init__()\n",
        "        self.use_raw = use_raw\n",
        "        self.use_mid = use_mid\n",
        "        self.use_high = use_high\n",
        "        self.use_grad = use_grad\n",
        "\n",
        "        self.streams = nn.ModuleDict()\n",
        "        feat_dims = []\n",
        "\n",
        "        if use_raw:\n",
        "            b, d = make_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"raw\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_mid:\n",
        "            b, d = make_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"mid\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_high:\n",
        "            b, d = make_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"high\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_grad:\n",
        "            b, d = make_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"grad\"] = b\n",
        "            feat_dims.append(d)\n",
        "\n",
        "        total_dim = sum(feat_dims)\n",
        "        self.head = nn.Linear(total_dim, 2)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        feats_list = []\n",
        "        if self.use_raw:\n",
        "            feats_list.append(self.streams[\"raw\"](streams[\"raw\"]))\n",
        "        if self.use_mid:\n",
        "            feats_list.append(self.streams[\"mid\"](streams[\"mid\"]))\n",
        "        if self.use_high:\n",
        "            feats_list.append(self.streams[\"high\"](streams[\"high\"]))\n",
        "        if self.use_grad:\n",
        "            feats_list.append(self.streams[\"grad\"](streams[\"grad\"]))\n",
        "\n",
        "        feats = torch.cat(feats_list, dim=1)\n",
        "        out = self.head(feats)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# 6. Train / eval helpers\n",
        "# ============================================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, epoch, total_epochs, tag=\"\"):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for step, (streams, labels) in enumerate(loader, start=1):\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Move all streams to device\n",
        "        for k in streams.keys():\n",
        "            streams[k] = streams[k].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(streams)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"[{tag}] Epoch [{epoch+1}/{total_epochs}] Step [{step}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / max(total, 1)\n",
        "    epoch_acc = correct / max(total, 1)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, tag=\"VAL\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for streams, labels in loader:\n",
        "            labels = labels.to(device)\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "\n",
        "            outputs = model(streams)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / max(total, 1)\n",
        "    epoch_acc = correct / max(total, 1)\n",
        "    print(f\"[{tag}] Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# ============================================================\n",
        "# 7. Datasets & dataloaders for RAW / MID / HIGH / GRAD / MULTI\n",
        "# ============================================================\n",
        "\n",
        "NUM_FRAMES = 16\n",
        "RESIZE = (112, 112)\n",
        "BATCH_SIZE = 4\n",
        "VAL_RATIO = 0.2\n",
        "EPOCHS = 1        # keep small on CPU; increase if you get GPU\n",
        "LR = 1e-4\n",
        "\n",
        "# Use the BALANCED manifest instead of the full cleaned one\n",
        "manifest_for_training = BALANCED_MANIFEST_CSV\n",
        "\n",
        "# RAW only\n",
        "train_raw = VideoDataset(manifest_for_training, split=\"train\", val_ratio=VAL_RATIO,\n",
        "                         num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                         use_raw=True, use_mid=False, use_high=False, use_grad=False)\n",
        "val_raw   = VideoDataset(manifest_for_training, split=\"val\",   val_ratio=VAL_RATIO,\n",
        "                         num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                         use_raw=True, use_mid=False, use_high=False, use_grad=False)\n",
        "\n",
        "train_loader_raw = DataLoader(train_raw, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=0, pin_memory=False)\n",
        "val_loader_raw   = DataLoader(val_raw,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=0, pin_memory=False)\n",
        "\n",
        "# MID only\n",
        "train_mid = VideoDataset(manifest_for_training, split=\"train\", val_ratio=VAL_RATIO,\n",
        "                         num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                         use_raw=False, use_mid=True, use_high=False, use_grad=False)\n",
        "val_mid   = VideoDataset(manifest_for_training, split=\"val\",   val_ratio=VAL_RATIO,\n",
        "                         num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                         use_raw=False, use_mid=True, use_high=False, use_grad=False)\n",
        "\n",
        "train_loader_mid = DataLoader(train_mid, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=0, pin_memory=False)\n",
        "val_loader_mid   = DataLoader(val_mid,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=0, pin_memory=False)\n",
        "\n",
        "# HIGH only\n",
        "train_high = VideoDataset(manifest_for_training, split=\"train\", val_ratio=VAL_RATIO,\n",
        "                          num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                          use_raw=False, use_mid=False, use_high=True, use_grad=False)\n",
        "val_high   = VideoDataset(manifest_for_training, split=\"val\",   val_ratio=VAL_RATIO,\n",
        "                          num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                          use_raw=False, use_mid=False, use_high=True, use_grad=False)\n",
        "\n",
        "train_loader_high = DataLoader(train_high, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                               num_workers=0, pin_memory=False)\n",
        "val_loader_high   = DataLoader(val_high,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                               num_workers=0, pin_memory=False)\n",
        "\n",
        "# GRAD only\n",
        "train_grad = VideoDataset(manifest_for_training, split=\"train\", val_ratio=VAL_RATIO,\n",
        "                          num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                          use_raw=False, use_mid=False, use_high=False, use_grad=True)\n",
        "val_grad   = VideoDataset(manifest_for_training, split=\"val\",   val_ratio=VAL_RATIO,\n",
        "                          num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                          use_raw=False, use_mid=False, use_high=False, use_grad=True)\n",
        "\n",
        "train_loader_grad = DataLoader(train_grad, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                               num_workers=0, pin_memory=False)\n",
        "val_loader_grad   = DataLoader(val_grad,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                               num_workers=0, pin_memory=False)\n",
        "\n",
        "# MULTI (raw+mid+high+grad)\n",
        "train_multi = VideoDataset(manifest_for_training, split=\"train\", val_ratio=VAL_RATIO,\n",
        "                           num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                           use_raw=True, use_mid=True, use_high=True, use_grad=True)\n",
        "val_multi   = VideoDataset(manifest_for_training, split=\"val\",   val_ratio=VAL_RATIO,\n",
        "                           num_frames=NUM_FRAMES, resize=RESIZE,\n",
        "                           use_raw=True, use_mid=True, use_high=True, use_grad=True)\n",
        "\n",
        "train_loader_multi = DataLoader(train_multi, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                num_workers=0, pin_memory=False)\n",
        "val_loader_multi   = DataLoader(val_multi,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                num_workers=0, pin_memory=False)\n",
        "\n",
        "# ============================================================\n",
        "# 8. Train 5 models + results table (with flags)\n",
        "# ============================================================\n",
        "\n",
        "TRAIN_RAW   = True\n",
        "TRAIN_MID   = True\n",
        "TRAIN_HIGH  = True\n",
        "TRAIN_GRAD  = False   # set True if you want to run\n",
        "TRAIN_MULTI = False   # set True if you want to run\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "results = {}\n",
        "\n",
        "# ---- RAW ----\n",
        "if TRAIN_RAW:\n",
        "    print(\"\\n######## Training RAW model ########\")\n",
        "    model_raw = MultiStreamClassifier(use_raw=True, use_mid=False, use_high=False, use_grad=False).to(device)\n",
        "    opt_raw = optim.Adam(model_raw.head.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model_raw, train_loader_raw, opt_raw, criterion, epoch, EPOCHS, tag=\"RAW\")\n",
        "        va_loss, va_acc = evaluate(model_raw, val_loader_raw, criterion, tag=\"RAW-VAL\")\n",
        "    results[\"RAW\"] = va_acc\n",
        "\n",
        "# ---- MID ----\n",
        "if TRAIN_MID:\n",
        "    print(\"\\n######## Training MID model ########\")\n",
        "    model_mid = MultiStreamClassifier(use_raw=False, use_mid=True, use_high=False, use_grad=False).to(device)\n",
        "    opt_mid = optim.Adam(model_mid.head.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model_mid, train_loader_mid, opt_mid, criterion, epoch, EPOCHS, tag=\"MID\")\n",
        "        va_loss, va_acc = evaluate(model_mid, val_loader_mid, criterion, tag=\"MID-VAL\")\n",
        "    results[\"MID\"] = va_acc\n",
        "\n",
        "# ---- HIGH ----\n",
        "if TRAIN_HIGH:\n",
        "    print(\"\\n######## Training HIGH model ########\")\n",
        "    model_high = MultiStreamClassifier(use_raw=False, use_mid=False, use_high=True, use_grad=False).to(device)\n",
        "    opt_high = optim.Adam(model_high.head.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model_high, train_loader_high, opt_high, criterion, epoch, EPOCHS, tag=\"HIGH\")\n",
        "        va_loss, va_acc = evaluate(model_high, val_loader_high, criterion, tag=\"HIGH-VAL\")\n",
        "    results[\"HIGH\"] = va_acc\n",
        "\n",
        "# ---- GRAD ----\n",
        "if TRAIN_GRAD:\n",
        "    print(\"\\n######## Training GRAD model ########\")\n",
        "    model_grad = MultiStreamClassifier(use_raw=False, use_mid=False, use_high=False, use_grad=True).to(device)\n",
        "    opt_grad = optim.Adam(model_grad.head.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model_grad, train_loader_grad, opt_grad, criterion, epoch, EPOCHS, tag=\"GRAD\")\n",
        "        va_loss, va_acc = evaluate(model_grad, val_loader_grad, criterion, tag=\"GRAD-VAL\")\n",
        "    results[\"GRAD\"] = va_acc\n",
        "\n",
        "# ---- MULTI (raw+mid+high+grad) ----\n",
        "if TRAIN_MULTI:\n",
        "    print(\"\\n######## Training MULTI model ########\")\n",
        "    model_multi = MultiStreamClassifier(use_raw=True, use_mid=True, use_high=True, use_grad=True).to(device)\n",
        "    opt_multi = optim.Adam(model_multi.head.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model_multi, train_loader_multi, opt_multi, criterion, epoch, EPOCHS, tag=\"MULTI\")\n",
        "        va_loss, va_acc = evaluate(model_multi, val_loader_multi, criterion, tag=\"MULTI-VAL\")\n",
        "    results[\"MULTI\"] = va_acc\n",
        "\n",
        "# ---- Results table ----\n",
        "print(\"\\n================ RESULTS TABLE ================\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Model\", \"Val Acc\"))\n",
        "print(\"----------------------------------------------\")\n",
        "for k, v in results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"==============================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fdTUDQlx0I4"
      },
      "outputs": [],
      "source": [
        "class VideoViTClassifier(nn.Module):\n",
        "    ...\n",
        "    def forward(self, streams):\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]\n",
        "\n",
        "        # x: [B,C,T,H,W]\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        # ðŸ”§ Make sure spatial size matches ViT expectation (224x224)\n",
        "        if H != 224 or W != 224:\n",
        "            x = x.view(B * T, C, H, W)\n",
        "            x = F.interpolate(x, size=(224, 224),\n",
        "                              mode=\"bilinear\",\n",
        "                              align_corners=False)\n",
        "            x = x.view(B, T, C, 224, 224).permute(0, 2, 1, 3, 4)  # back to [B,C,T,H,W]\n",
        "        B, C, T, H, W = x.shape  # now 224x224\n",
        "\n",
        "        # [B,C,T,H,W] -> [B*T, C, H, W]\n",
        "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = self.vit(x)  # [B*T, D]\n",
        "\n",
        "        feats = feats.view(B, T, -1).mean(dim=1)  # temporal average\n",
        "        out = self.fc(feats)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSGfGpZwLYFD",
        "outputId": "a3625e8c-567f-45de-f813-a2838abfd021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Found in folders (total):\n",
            "  AI_CORE total: 1200\n",
            "  SORA total   : 1250\n",
            "  REAL total   : 1200\n",
            "\n",
            "Using for this experiment:\n",
            "  AI_CORE used : 10\n",
            "  SORA used    : 10\n",
            "  REAL used    : 10\n",
            "\n",
            "Total samples: 30\n",
            "Train videos: 21\n",
            "Test  videos: 9\n",
            "\n",
            "Sanity check batch:\n",
            "raw shape: torch.Size([1, 3, 8, 96, 96]) labels: tensor([1])\n",
            "\n",
            "######## R(2+1)D EXPERIMENTS ########\n",
            "\n",
            "--- R2 RAW ---\n",
            "[R2-RAW] Epoch 1/3 - loss: 0.6587 - acc: 0.667\n",
            "[R2-RAW] Epoch 2/3 - loss: 0.6534 - acc: 0.667\n",
            "[R2-RAW] Epoch 3/3 - loss: 0.6505 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.5714    0.6667    0.6154         6\n",
            "\n",
            "    accuracy                         0.4444         9\n",
            "   macro avg     0.2857    0.3333    0.3077         9\n",
            "weighted avg     0.3810    0.4444    0.4103         9\n",
            "\n",
            "Accuracy: 0.4444444444444444\n",
            "\n",
            "--- R2 MID ---\n",
            "[R2-MID] Epoch 1/3 - loss: 0.6749 - acc: 0.619\n",
            "[R2-MID] Epoch 2/3 - loss: 0.6311 - acc: 0.667\n",
            "[R2-MID] Epoch 3/3 - loss: 0.6303 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6250    0.8333    0.7143         6\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.3125    0.4167    0.3571         9\n",
            "weighted avg     0.4167    0.5556    0.4762         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- R2 HIGH ---\n",
            "[R2-HIGH] Epoch 1/3 - loss: 0.7571 - acc: 0.333\n",
            "[R2-HIGH] Epoch 2/3 - loss: 0.6724 - acc: 0.667\n",
            "[R2-HIGH] Epoch 3/3 - loss: 0.6478 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6250    0.8333    0.7143         6\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.3125    0.4167    0.3571         9\n",
            "weighted avg     0.4167    0.5556    0.4762         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- R2 GRAD ---\n",
            "[R2-GRAD] Epoch 1/3 - loss: 0.6793 - acc: 0.667\n",
            "[R2-GRAD] Epoch 2/3 - loss: 0.6342 - acc: 0.667\n",
            "[R2-GRAD] Epoch 3/3 - loss: 0.6359 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-FREQ] Epoch 1/3 - loss: 0.6478 - acc: 0.667\n",
            "[R2-FREQ] Epoch 2/3 - loss: 0.6388 - acc: 0.667\n",
            "[R2-FREQ] Epoch 3/3 - loss: 0.6607 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 MULTI (raw+mid+high+grad+freq) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-MULTI] Epoch 1/3 - loss: 0.7106 - acc: 0.619\n",
            "[R2-MULTI] Epoch 2/3 - loss: 0.6519 - acc: 0.667\n",
            "[R2-MULTI] Epoch 3/3 - loss: 0.6873 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "=========== R(2+1)D RESULTS TABLE ===========\n",
            "Model      |  TestAcc\n",
            "---------------------------------------------\n",
            "RAW        |   0.4444\n",
            "MID        |   0.5556\n",
            "HIGH       |   0.5556\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   0.6667\n",
            "=============================================\n",
            "\n",
            "\n",
            "######## ViT TRANSFORMER EXPERIMENTS ########\n",
            "\n",
            "--- ViT RAW ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT-RAW] Epoch 1/3 - loss: 0.7933 - acc: 0.429\n",
            "[ViT-RAW] Epoch 2/3 - loss: 0.7275 - acc: 0.524\n",
            "[ViT-RAW] Epoch 3/3 - loss: 0.6753 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (ViT-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.5000    0.5000    0.5000         6\n",
            "\n",
            "    accuracy                         0.3333         9\n",
            "   macro avg     0.2500    0.2500    0.2500         9\n",
            "weighted avg     0.3333    0.3333    0.3333         9\n",
            "\n",
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "--- ViT MID ---\n",
            "[ViT-MID] Epoch 1/3 - loss: 0.8017 - acc: 0.286\n",
            "[ViT-MID] Epoch 2/3 - loss: 0.7262 - acc: 0.476\n",
            "[ViT-MID] Epoch 3/3 - loss: 0.6771 - acc: 0.571\n",
            "\n",
            "=== TEST RESULTS (ViT-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.4286    1.0000    0.6000         3\n",
            " AI/Sora (1)     1.0000    0.3333    0.5000         6\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.7143    0.6667    0.5500         9\n",
            "weighted avg     0.8095    0.5556    0.5333         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- ViT HIGH ---\n",
            "[ViT-HIGH] Epoch 1/3 - loss: 0.7294 - acc: 0.333\n",
            "[ViT-HIGH] Epoch 2/3 - loss: 0.6581 - acc: 0.619\n",
            "[ViT-HIGH] Epoch 3/3 - loss: 0.6057 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViT-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.6000    1.0000    0.7500         3\n",
            " AI/Sora (1)     1.0000    0.6667    0.8000         6\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.8000    0.8333    0.7750         9\n",
            "weighted avg     0.8667    0.7778    0.7833         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- ViT GRAD ---\n",
            "[ViT-GRAD] Epoch 1/3 - loss: 0.7249 - acc: 0.429\n",
            "[ViT-GRAD] Epoch 2/3 - loss: 0.6403 - acc: 0.667\n",
            "[ViT-GRAD] Epoch 3/3 - loss: 0.5959 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViT-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT-FREQ] Epoch 1/3 - loss: 0.6928 - acc: 0.571\n",
            "[ViT-FREQ] Epoch 2/3 - loss: 0.6642 - acc: 0.667\n",
            "[ViT-FREQ] Epoch 3/3 - loss: 0.6523 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViT-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT MULTI ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT-MULTI] Epoch 1/3 - loss: 0.7171 - acc: 0.476\n",
            "[ViT-MULTI] Epoch 2/3 - loss: 0.6541 - acc: 0.667\n",
            "[ViT-MULTI] Epoch 3/3 - loss: 0.6079 - acc: 0.810\n",
            "\n",
            "=== TEST RESULTS (ViT-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.5000    0.5000    0.5000         6\n",
            "\n",
            "    accuracy                         0.3333         9\n",
            "   macro avg     0.2500    0.2500    0.2500         9\n",
            "weighted avg     0.3333    0.3333    0.3333         9\n",
            "\n",
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "=========== ViT RESULTS TABLE ===========\n",
            "Model      |  TestAcc\n",
            "----------------------------------------\n",
            "RAW        |   0.3333\n",
            "MID        |   0.5556\n",
            "HIGH       |   0.7778\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   0.3333\n",
            "=========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Tiny 10/10/10 experiment with ALL feature ideas:\n",
        "#   RAW, MID, HIGH, GRAD, FREQ, MULTI\n",
        "#\n",
        "#   - R(2+1)D multi-stream CNN (temporal convs)\n",
        "#   - ViT-based transformer video classifier\n",
        "#\n",
        "# Folders:\n",
        "#   AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"              (AI)\n",
        "#   SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    (AI)\n",
        "#   REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  (Real)\n",
        "#\n",
        "# Each model is trained on 21 clips, tested on 9 clips.\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Paths (your folders)\n",
        "# -----------------------------\n",
        "AI_CORE_DIR  = \"/content/drive/MyDrive/AudioModel/AI\"              # AI class\n",
        "SORA_DIR     = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    # AI class\n",
        "REAL_DIR     = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  # Real class\n",
        "\n",
        "# -----------------------------\n",
        "# List videos\n",
        "# -----------------------------\n",
        "def list_videos(folder):\n",
        "    vids = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        vids.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return sorted(vids)\n",
        "\n",
        "ai_core_all  = list_videos(AI_CORE_DIR)\n",
        "sora_all     = list_videos(SORA_DIR)\n",
        "real_all     = list_videos(REAL_DIR)\n",
        "\n",
        "# Take first 10 from each to match your experiment\n",
        "ai_core_files = ai_core_all[:10]\n",
        "sora_files    = sora_all[:10]\n",
        "real_files    = real_all[:10]\n",
        "\n",
        "print(\"Found in folders (total):\")\n",
        "print(\"  AI_CORE total:\", len(ai_core_all))\n",
        "print(\"  SORA total   :\", len(sora_all))\n",
        "print(\"  REAL total   :\", len(real_all))\n",
        "\n",
        "print(\"\\nUsing for this experiment:\")\n",
        "print(\"  AI_CORE used :\", len(ai_core_files))\n",
        "print(\"  SORA used    :\", len(sora_files))\n",
        "print(\"  REAL used    :\", len(real_files))\n",
        "\n",
        "# -----------------------------\n",
        "# Build samples (Real=0, AI/Sora=1)\n",
        "# -----------------------------\n",
        "samples = []\n",
        "for p in ai_core_files:\n",
        "    samples.append((p, 1))  # AI\n",
        "for p in sora_files:\n",
        "    samples.append((p, 1))  # Sora as AI\n",
        "for p in real_files:\n",
        "    samples.append((p, 0))  # Real\n",
        "\n",
        "random.shuffle(samples)\n",
        "\n",
        "print(\"\\nTotal samples:\", len(samples))  # ~30\n",
        "\n",
        "# Train / Test split\n",
        "train_ratio = 0.7\n",
        "train_size = max(1, int(len(samples) * train_ratio))\n",
        "train_samples = samples[:train_size]\n",
        "test_samples  = samples[train_size:]\n",
        "\n",
        "print(\"Train videos:\", len(train_samples))\n",
        "print(\"Test  videos:\", len(test_samples))\n",
        "\n",
        "# ============================================================\n",
        "# Low-RAM OpenCV loader (8 frames, 96x96)\n",
        "# ============================================================\n",
        "\n",
        "def load_video_clip_opencv(path, num_frames=8, size=(96, 96)):\n",
        "    \"\"\"\n",
        "    RAM-friendly:\n",
        "      - OpenCV only\n",
        "      - sample num_frames across total frames\n",
        "      - load & resize those frames\n",
        "      - return [C, T, H, W] in [0,1]\n",
        "    \"\"\"\n",
        "    H, W = size\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] Missing file:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[WARN] Cannot open video:\", path)\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        print(\"[WARN] Empty video:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    if frame_count >= num_frames:\n",
        "        idxs = np.linspace(0, frame_count - 1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = int(np.ceil(num_frames / frame_count))\n",
        "        all_idx = np.tile(np.arange(frame_count), reps)\n",
        "        idxs = all_idx[:num_frames]\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    cap.release()\n",
        "    clip = torch.stack(frames, dim=1)  # [C, T, H, W]\n",
        "    return clip\n",
        "\n",
        "# ============================================================\n",
        "# Feature transforms: MID, HIGH, GRAD, FREQ\n",
        "# ============================================================\n",
        "\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645]).view(3, 1, 1, 1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989]).view(3, 1, 1, 1)\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Keep only midtones based on luminance.\n",
        "    \"\"\"\n",
        "    r, g, b = frames[0], frames[1], frames[2]\n",
        "    lum = 0.299 * r + 0.587 * g + 0.114 * b  # [T,H,W]\n",
        "    lum = lum.unsqueeze(0)  # [1,T,H,W]\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Subtract Gaussian blur, keep HF component.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        frame_np = frame.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
        "        blur = cv2.GaussianBlur(frame_np, (ksize, ksize), 0)\n",
        "        residual = frame_np - blur\n",
        "        residual = np.clip(residual * 2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:, t] = torch.from_numpy(residual).permute(2, 0, 1)\n",
        "    return out\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Sobel gradient magnitude on grayscale, repeated to 3 channels.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        gx = cv2.Sobel(gray_np, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray_np, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)  # [3,H,W]\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def frequency_spectrum(frames):\n",
        "    \"\"\"\n",
        "    frames: [C, T, H, W] in [0,1]\n",
        "    Approximate \"frequency signature\" via log-magnitude FFT of grayscale,\n",
        "    normalized per frame, repeated to 3 channels.\n",
        "    \"\"\"\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # 2D FFT\n",
        "        fft = np.fft.fft2(gray_np)\n",
        "        fft_shift = np.fft.fftshift(fft)\n",
        "        mag = np.abs(fft_shift)\n",
        "        mag = np.log1p(mag)  # log magnitude\n",
        "\n",
        "        # Normalize per frame\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    \"\"\"Normalize [C,T,H,W] with Kinetics mean/std.\"\"\"\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "# ============================================================\n",
        "# Dataset that returns ALL streams at once\n",
        "# ============================================================\n",
        "\n",
        "class MultiStreamTinyDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=(96,96)):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_clip_opencv(path, self.num_frames, self.size)  # [C,T,H,W], [0,1]\n",
        "\n",
        "        # Build all streams BEFORE normalization, then normalize each\n",
        "        raw  = normalize_kinetics(frames.clone())\n",
        "        mid  = normalize_kinetics(midtone_mask(frames.clone()))\n",
        "        high = normalize_kinetics(high_frequency_residual(frames.clone()))\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames.clone()))\n",
        "        freq = normalize_kinetics(frequency_spectrum(frames.clone()))\n",
        "\n",
        "        streams = {\n",
        "            \"raw\":  raw,\n",
        "            \"mid\":  mid,\n",
        "            \"high\": high,\n",
        "            \"grad\": grad,\n",
        "            \"freq\": freq,\n",
        "        }\n",
        "        return streams, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = MultiStreamTinyDataset(train_samples, num_frames=8, size=(96,96))\n",
        "test_ds  = MultiStreamTinyDataset(test_samples,  num_frames=8, size=(96,96))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,  num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"\\nSanity check batch:\")\n",
        "for streams, labels in train_loader:\n",
        "    print(\"raw shape:\", streams[\"raw\"].shape, \"labels:\", labels)\n",
        "    break\n",
        "\n",
        "# ============================================================\n",
        "# 1) R(2+1)D multi-stream models\n",
        "# ============================================================\n",
        "\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "def make_r2plus1d_backbone():\n",
        "    weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "    model = r2plus1d_18(weights=weights)\n",
        "    feat_dim = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, feat_dim\n",
        "\n",
        "class MultiStreamR2Plus1D(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-stream R(2+1)D backbone.\n",
        "    Streams: 'raw', 'mid', 'high', 'grad', 'freq'\n",
        "    \"\"\"\n",
        "    def __init__(self, use_raw=True, use_mid=False, use_high=False,\n",
        "                 use_grad=False, use_freq=False, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.use_raw = use_raw\n",
        "        self.use_mid = use_mid\n",
        "        self.use_high = use_high\n",
        "        self.use_grad = use_grad\n",
        "        self.use_freq = use_freq\n",
        "\n",
        "        self.streams = nn.ModuleDict()\n",
        "        feat_dims = []\n",
        "\n",
        "        if use_raw:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"raw\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_mid:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"mid\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_high:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"high\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_grad:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"grad\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_freq:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"freq\"] = b\n",
        "            feat_dims.append(d)\n",
        "\n",
        "        total_dim = sum(feat_dims)\n",
        "        self.head = nn.Linear(total_dim, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        \"\"\"\n",
        "        streams: dict of tensors [B,C,T,H,W]\n",
        "        \"\"\"\n",
        "        feats = []\n",
        "        if self.use_raw:\n",
        "            feats.append(self.streams[\"raw\"](streams[\"raw\"]))\n",
        "        if self.use_mid:\n",
        "            feats.append(self.streams[\"mid\"](streams[\"mid\"]))\n",
        "        if self.use_high:\n",
        "            feats.append(self.streams[\"high\"](streams[\"high\"]))\n",
        "        if self.use_grad:\n",
        "            feats.append(self.streams[\"grad\"](streams[\"grad\"]))\n",
        "        if self.use_freq:\n",
        "            feats.append(self.streams[\"freq\"](streams[\"freq\"]))\n",
        "\n",
        "        x = torch.cat(feats, dim=1)\n",
        "        out = self.head(x)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# 2) ViT-based transformer video classifier\n",
        "# ============================================================\n",
        "\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "\n",
        "def make_vit_backbone():\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    vit = vit_b_16(weights=weights)\n",
        "    feat_dim = vit.heads.head.in_features\n",
        "    vit.heads.head = nn.Identity()\n",
        "    return vit, feat_dim\n",
        "\n",
        "class VideoViTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-stream ViT classifier.\n",
        "    stream_key in {'raw', 'mid', 'high', 'grad', 'freq', 'multi'}.\n",
        "\n",
        "    For 'multi', we fuse (raw+mid+high+grad+freq)/5 and feed that\n",
        "    as a single 3-channel video into ViT (frame-wise).\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "        self.vit, feat_dim = make_vit_backbone()\n",
        "        for p in self.vit.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]\n",
        "\n",
        "        # x: [B,C,T,H,W]\n",
        "        B, C, T, H, W = x.shape\n",
        "        # [B,C,T,H,W] -> [B*T, C, H, W]\n",
        "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "\n",
        "        # ViT expects 224x224\n",
        "        if H != 224 or W != 224:\n",
        "            x = F.interpolate(x, size=(224, 224),\n",
        "                              mode=\"bilinear\",\n",
        "                              align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = self.vit(x)  # [B*T, D]\n",
        "\n",
        "        feats = feats.view(B, T, -1).mean(dim=1)  # temporal average\n",
        "        out = self.fc(feats)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Generic train / eval helpers\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, epochs=3, tag=\"\"):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # pick correct parameter group\n",
        "    if hasattr(model, \"fc\"):\n",
        "        params = model.fc.parameters()\n",
        "    else:\n",
        "        params = model.head.parameters()\n",
        "\n",
        "    optimizer = optim.Adam(params, lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for streams, labels in train_loader:\n",
        "            # Move to device\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(streams)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / max(total, 1)\n",
        "        epoch_acc  = correct / max(total, 1)\n",
        "        print(f\"[{tag}] Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.3f}\")\n",
        "\n",
        "def eval_model(model, test_loader, tag=\"\", target_names=None):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for streams, labels in test_loader:\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(streams)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    if len(all_labels) == 0:\n",
        "        print(f\"[{tag}] Not enough test samples.\")\n",
        "        return 0.0\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({tag}) ===\")\n",
        "    print(classification_report(all_labels, all_preds,\n",
        "                                target_names=target_names,\n",
        "                                digits=4))\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    return acc\n",
        "\n",
        "TARGET_NAMES = [\"Real (0)\", \"AI/Sora (1)\"]\n",
        "\n",
        "# ============================================================\n",
        "# Run R(2+1)D variants\n",
        "# ============================================================\n",
        "\n",
        "r2_results = {}\n",
        "\n",
        "print(\"\\n######## R(2+1)D EXPERIMENTS ########\")\n",
        "\n",
        "# RAW\n",
        "print(\"\\n--- R2 RAW ---\")\n",
        "model_r2_raw = MultiStreamR2Plus1D(use_raw=True, use_mid=False,\n",
        "                                   use_high=False, use_grad=False,\n",
        "                                   use_freq=False, num_classes=2)\n",
        "train_model(model_r2_raw, train_loader, epochs=3, tag=\"R2-RAW\")\n",
        "acc_raw = eval_model(model_r2_raw, test_loader, tag=\"R2-RAW\", target_names=TARGET_NAMES)\n",
        "r2_results[\"RAW\"] = acc_raw\n",
        "\n",
        "# MID\n",
        "print(\"\\n--- R2 MID ---\")\n",
        "model_r2_mid = MultiStreamR2Plus1D(use_raw=False, use_mid=True,\n",
        "                                   use_high=False, use_grad=False,\n",
        "                                   use_freq=False, num_classes=2)\n",
        "train_model(model_r2_mid, train_loader, epochs=3, tag=\"R2-MID\")\n",
        "acc_mid = eval_model(model_r2_mid, test_loader, tag=\"R2-MID\", target_names=TARGET_NAMES)\n",
        "r2_results[\"MID\"] = acc_mid\n",
        "\n",
        "# HIGH\n",
        "print(\"\\n--- R2 HIGH ---\")\n",
        "model_r2_high = MultiStreamR2Plus1D(use_raw=False, use_mid=False,\n",
        "                                    use_high=True, use_grad=False,\n",
        "                                    use_freq=False, num_classes=2)\n",
        "train_model(model_r2_high, train_loader, epochs=3, tag=\"R2-HIGH\")\n",
        "acc_high = eval_model(model_r2_high, test_loader, tag=\"R2-HIGH\", target_names=TARGET_NAMES)\n",
        "r2_results[\"HIGH\"] = acc_high\n",
        "\n",
        "# GRAD\n",
        "print(\"\\n--- R2 GRAD ---\")\n",
        "model_r2_grad = MultiStreamR2Plus1D(use_raw=False, use_mid=False,\n",
        "                                    use_high=False, use_grad=True,\n",
        "                                    use_freq=False, num_classes=2)\n",
        "train_model(model_r2_grad, train_loader, epochs=3, tag=\"R2-GRAD\")\n",
        "acc_grad = eval_model(model_r2_grad, test_loader, tag=\"R2-GRAD\", target_names=TARGET_NAMES)\n",
        "r2_results[\"GRAD\"] = acc_grad\n",
        "\n",
        "# FREQ\n",
        "print(\"\\n--- R2 FREQ ---\")\n",
        "model_r2_freq = MultiStreamR2Plus1D(use_raw=False, use_mid=False,\n",
        "                                    use_high=False, use_grad=False,\n",
        "                                    use_freq=True, num_classes=2)\n",
        "train_model(model_r2_freq, train_loader, epochs=3, tag=\"R2-FREQ\")\n",
        "acc_freq = eval_model(model_r2_freq, test_loader, tag=\"R2-FREQ\", target_names=TARGET_NAMES)\n",
        "r2_results[\"FREQ\"] = acc_freq\n",
        "\n",
        "# MULTI (raw + mid + high + grad + freq)\n",
        "print(\"\\n--- R2 MULTI (raw+mid+high+grad+freq) ---\")\n",
        "model_r2_multi = MultiStreamR2Plus1D(use_raw=True, use_mid=True,\n",
        "                                     use_high=True, use_grad=True,\n",
        "                                     use_freq=True, num_classes=2)\n",
        "train_model(model_r2_multi, train_loader, epochs=3, tag=\"R2-MULTI\")\n",
        "acc_multi = eval_model(model_r2_multi, test_loader, tag=\"R2-MULTI\", target_names=TARGET_NAMES)\n",
        "r2_results[\"MULTI\"] = acc_multi\n",
        "\n",
        "print(\"\\n=========== R(2+1)D RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Model\", \"TestAcc\"))\n",
        "print(\"---------------------------------------------\")\n",
        "for k, v in r2_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=============================================\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Run ViT transformer variants\n",
        "# ============================================================\n",
        "\n",
        "vit_results = {}\n",
        "\n",
        "print(\"\\n######## ViT TRANSFORMER EXPERIMENTS ########\")\n",
        "\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- ViT {key} ---\")\n",
        "    if key == \"RAW\":\n",
        "        sk = \"raw\"\n",
        "    elif key == \"MID\":\n",
        "        sk = \"mid\"\n",
        "    elif key == \"HIGH\":\n",
        "        sk = \"high\"\n",
        "    elif key == \"GRAD\":\n",
        "        sk = \"grad\"\n",
        "    elif key == \"FREQ\":\n",
        "        sk = \"freq\"\n",
        "    elif key == \"MULTI\":\n",
        "        sk = \"multi\"\n",
        "\n",
        "    model_vit = VideoViTClassifier(stream_key=sk, num_classes=2)\n",
        "    train_model(model_vit, train_loader, epochs=3, tag=f\"ViT-{key}\")\n",
        "    acc = eval_model(model_vit, test_loader, tag=f\"ViT-{key}\", target_names=TARGET_NAMES)\n",
        "    vit_results[key] = acc\n",
        "\n",
        "print(\"\\n=========== ViT RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Model\", \"TestAcc\"))\n",
        "print(\"----------------------------------------\")\n",
        "for k, v in vit_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=========================================\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ghL8kzlS4X",
        "outputId": "9c4b57f5-34f4-4d03-c11a-9d2bfd1d3299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Found in folders (total):\n",
            "  AI_CORE total: 1200\n",
            "  SORA total   : 1250\n",
            "  REAL total   : 1200\n",
            "\n",
            "Using for this experiment:\n",
            "  AI_CORE used : 10\n",
            "  SORA used    : 10\n",
            "  REAL used    : 10\n",
            "\n",
            "Total samples: 30\n",
            "Train videos: 21\n",
            "Test  videos: 9\n",
            "\n",
            "Sanity check batch:\n",
            "raw shape: torch.Size([1, 3, 8, 96, 96]) labels: tensor([0])\n",
            "\n",
            "######## PLAIN R(2+1)D EXPERIMENTS ########\n",
            "\n",
            "--- R2 RAW ---\n",
            "[R2-RAW] Epoch 1/3 - loss: 0.6861 - acc: 0.524\n",
            "[R2-RAW] Epoch 2/3 - loss: 0.6433 - acc: 0.667\n",
            "[R2-RAW] Epoch 3/3 - loss: 0.6518 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6250    0.8333    0.7143         6\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.3125    0.4167    0.3571         9\n",
            "weighted avg     0.4167    0.5556    0.4762         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- R2 MID ---\n",
            "[R2-MID] Epoch 1/3 - loss: 0.6705 - acc: 0.667\n",
            "[R2-MID] Epoch 2/3 - loss: 0.6532 - acc: 0.667\n",
            "[R2-MID] Epoch 3/3 - loss: 0.6707 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 HIGH ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-HIGH] Epoch 1/3 - loss: 0.7344 - acc: 0.476\n",
            "[R2-HIGH] Epoch 2/3 - loss: 0.6641 - acc: 0.667\n",
            "[R2-HIGH] Epoch 3/3 - loss: 0.6391 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 GRAD ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-GRAD] Epoch 1/3 - loss: 0.6563 - acc: 0.667\n",
            "[R2-GRAD] Epoch 2/3 - loss: 0.6540 - acc: 0.667\n",
            "[R2-GRAD] Epoch 3/3 - loss: 0.6409 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-FREQ] Epoch 1/3 - loss: 0.8026 - acc: 0.333\n",
            "[R2-FREQ] Epoch 2/3 - loss: 0.6529 - acc: 0.667\n",
            "[R2-FREQ] Epoch 3/3 - loss: 0.6566 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2 MULTI ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2-MULTI] Epoch 1/3 - loss: 0.6901 - acc: 0.667\n",
            "[R2-MULTI] Epoch 2/3 - loss: 0.6902 - acc: 0.667\n",
            "[R2-MULTI] Epoch 3/3 - loss: 0.6596 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.6667    0.5714         3\n",
            " AI/Sora (1)     0.8000    0.6667    0.7273         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6500    0.6667    0.6494         9\n",
            "weighted avg     0.7000    0.6667    0.6753         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "=========== R(2+1)D RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "---------------------------------------------\n",
            "RAW        |   0.5556\n",
            "MID        |   0.6667\n",
            "HIGH       |   0.6667\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   0.6667\n",
            "=============================================\n",
            "\n",
            "\n",
            "######## PLAIN ViT EXPERIMENTS ########\n",
            "\n",
            "--- ViT RAW ---\n",
            "[ViT-RAW] Epoch 1/3 - loss: 0.7444 - acc: 0.286\n",
            "[ViT-RAW] Epoch 2/3 - loss: 0.6805 - acc: 0.667\n",
            "[ViT-RAW] Epoch 3/3 - loss: 0.6336 - acc: 0.810\n",
            "\n",
            "=== TEST RESULTS (ViT-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.6667    0.5714         3\n",
            " AI/Sora (1)     0.8000    0.6667    0.7273         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6500    0.6667    0.6494         9\n",
            "weighted avg     0.7000    0.6667    0.6753         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT MID ---\n",
            "[ViT-MID] Epoch 1/3 - loss: 0.7155 - acc: 0.429\n",
            "[ViT-MID] Epoch 2/3 - loss: 0.6634 - acc: 0.571\n",
            "[ViT-MID] Epoch 3/3 - loss: 0.6199 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (ViT-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.3333    0.3333    0.3333         3\n",
            " AI/Sora (1)     0.6667    0.6667    0.6667         6\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.5000    0.5000    0.5000         9\n",
            "weighted avg     0.5556    0.5556    0.5556         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- ViT HIGH ---\n",
            "[ViT-HIGH] Epoch 1/3 - loss: 0.6522 - acc: 0.619\n",
            "[ViT-HIGH] Epoch 2/3 - loss: 0.5889 - acc: 0.667\n",
            "[ViT-HIGH] Epoch 3/3 - loss: 0.5419 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViT-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.3333    0.4000         3\n",
            " AI/Sora (1)     0.7143    0.8333    0.7692         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6071    0.5833    0.5846         9\n",
            "weighted avg     0.6429    0.6667    0.6462         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT GRAD ---\n",
            "[ViT-GRAD] Epoch 1/3 - loss: 0.7509 - acc: 0.381\n",
            "[ViT-GRAD] Epoch 2/3 - loss: 0.6413 - acc: 0.619\n",
            "[ViT-GRAD] Epoch 3/3 - loss: 0.5859 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViT-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.3333    0.4000         3\n",
            " AI/Sora (1)     0.7143    0.8333    0.7692         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6071    0.5833    0.5846         9\n",
            "weighted avg     0.6429    0.6667    0.6462         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT FREQ ---\n",
            "[ViT-FREQ] Epoch 1/3 - loss: 0.6859 - acc: 0.667\n",
            "[ViT-FREQ] Epoch 2/3 - loss: 0.6565 - acc: 0.667\n",
            "[ViT-FREQ] Epoch 3/3 - loss: 0.6670 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViT-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT MULTI ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT-MULTI] Epoch 1/3 - loss: 0.7496 - acc: 0.381\n",
            "[ViT-MULTI] Epoch 2/3 - loss: 0.6768 - acc: 0.524\n",
            "[ViT-MULTI] Epoch 3/3 - loss: 0.6153 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViT-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.3333    0.4000         3\n",
            " AI/Sora (1)     0.7143    0.8333    0.7692         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6071    0.5833    0.5846         9\n",
            "weighted avg     0.6429    0.6667    0.6462         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "=========== ViT RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "----------------------------------------\n",
            "RAW        |   0.6667\n",
            "MID        |   0.5556\n",
            "HIGH       |   0.6667\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   0.6667\n",
            "=========================================\n",
            "\n",
            "\n",
            "######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- R2+ResNet RAW ---\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 131MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-RAW] Epoch 1/3 - loss: 0.6942 - acc: 0.571\n",
            "[R2Res-RAW] Epoch 2/3 - loss: 0.6746 - acc: 0.667\n",
            "[R2Res-RAW] Epoch 3/3 - loss: 0.6502 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2+ResNet MID ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-MID] Epoch 1/3 - loss: 0.7375 - acc: 0.667\n",
            "[R2Res-MID] Epoch 2/3 - loss: 0.6527 - acc: 0.667\n",
            "[R2Res-MID] Epoch 3/3 - loss: 0.6473 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2+ResNet HIGH ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-HIGH] Epoch 1/3 - loss: 0.6677 - acc: 0.571\n",
            "[R2Res-HIGH] Epoch 2/3 - loss: 0.6719 - acc: 0.667\n",
            "[R2Res-HIGH] Epoch 3/3 - loss: 0.6548 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2+ResNet GRAD ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-GRAD] Epoch 1/3 - loss: 0.6648 - acc: 0.667\n",
            "[R2Res-GRAD] Epoch 2/3 - loss: 0.6501 - acc: 0.667\n",
            "[R2Res-GRAD] Epoch 3/3 - loss: 0.6640 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2+ResNet FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-FREQ] Epoch 1/3 - loss: 0.7064 - acc: 0.524\n",
            "[R2Res-FREQ] Epoch 2/3 - loss: 0.6819 - acc: 0.667\n",
            "[R2Res-FREQ] Epoch 3/3 - loss: 0.6617 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.3333    0.4000         3\n",
            " AI/Sora (1)     0.7143    0.8333    0.7692         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6071    0.5833    0.5846         9\n",
            "weighted avg     0.6429    0.6667    0.6462         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- R2+ResNet MULTI ---\n",
            "[R2Res-MULTI] Epoch 1/3 - loss: 0.6723 - acc: 0.667\n",
            "[R2Res-MULTI] Epoch 2/3 - loss: 0.6471 - acc: 0.667\n",
            "[R2Res-MULTI] Epoch 3/3 - loss: 0.6730 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (R2Res-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "=========== R2 + ResNet RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "-----------------------------------------------\n",
            "RAW        |   0.6667\n",
            "MID        |   0.6667\n",
            "HIGH       |   0.6667\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   0.6667\n",
            "=================================================\n",
            "\n",
            "\n",
            "######## ViT + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- ViT+ResNet RAW ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViTRes-RAW] Epoch 1/3 - loss: 0.7589 - acc: 0.571\n",
            "[ViTRes-RAW] Epoch 2/3 - loss: 0.6823 - acc: 0.571\n",
            "[ViTRes-RAW] Epoch 3/3 - loss: 0.6392 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (ViTRes-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT+ResNet MID ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViTRes-MID] Epoch 1/3 - loss: 0.6644 - acc: 0.667\n",
            "[ViTRes-MID] Epoch 2/3 - loss: 0.5777 - acc: 0.667\n",
            "[ViTRes-MID] Epoch 3/3 - loss: 0.5551 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViTRes-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.3333    0.4000         3\n",
            " AI/Sora (1)     0.7143    0.8333    0.7692         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.6071    0.5833    0.5846         9\n",
            "weighted avg     0.6429    0.6667    0.6462         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT+ResNet HIGH ---\n",
            "[ViTRes-HIGH] Epoch 1/3 - loss: 0.7511 - acc: 0.524\n",
            "[ViTRes-HIGH] Epoch 2/3 - loss: 0.6153 - acc: 0.714\n",
            "[ViTRes-HIGH] Epoch 3/3 - loss: 0.5595 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViTRes-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     1.0000    0.3333    0.5000         3\n",
            " AI/Sora (1)     0.7500    1.0000    0.8571         6\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.8750    0.6667    0.6786         9\n",
            "weighted avg     0.8333    0.7778    0.7381         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- ViT+ResNet GRAD ---\n",
            "[ViTRes-GRAD] Epoch 1/3 - loss: 0.6704 - acc: 0.571\n",
            "[ViTRes-GRAD] Epoch 2/3 - loss: 0.5500 - acc: 0.762\n",
            "[ViTRes-GRAD] Epoch 3/3 - loss: 0.5169 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViTRes-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         3\n",
            " AI/Sora (1)     0.6667    1.0000    0.8000         6\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3333    0.5000    0.4000         9\n",
            "weighted avg     0.4444    0.6667    0.5333         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT+ResNet FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViTRes-FREQ] Epoch 1/3 - loss: 0.7009 - acc: 0.619\n",
            "[ViTRes-FREQ] Epoch 2/3 - loss: 0.6863 - acc: 0.667\n",
            "[ViTRes-FREQ] Epoch 3/3 - loss: 0.6555 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViTRes-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     1.0000    0.3333    0.5000         3\n",
            " AI/Sora (1)     0.7500    1.0000    0.8571         6\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.8750    0.6667    0.6786         9\n",
            "weighted avg     0.8333    0.7778    0.7381         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- ViT+ResNet MULTI ---\n",
            "[ViTRes-MULTI] Epoch 1/3 - loss: 0.8672 - acc: 0.429\n",
            "[ViTRes-MULTI] Epoch 2/3 - loss: 0.6147 - acc: 0.714\n",
            "[ViTRes-MULTI] Epoch 3/3 - loss: 0.5521 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViTRes-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.6667    0.6667    0.6667         3\n",
            " AI/Sora (1)     0.8333    0.8333    0.8333         6\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.7500    0.7500    0.7500         9\n",
            "weighted avg     0.7778    0.7778    0.7778         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "=========== ViT + ResNet RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "-----------------------------------------------\n",
            "RAW        |   0.6667\n",
            "MID        |   0.6667\n",
            "HIGH       |   0.7778\n",
            "GRAD       |   0.6667\n",
            "FREQ       |   0.7778\n",
            "MULTI      |   0.7778\n",
            "=================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Tiny 10/10/10 experiment with ALL feature ideas:\n",
        "#   RAW, MID, HIGH, GRAD, FREQ, MULTI\n",
        "#\n",
        "#   - R(2+1)D multi-stream CNN (temporal convs)\n",
        "#   - ViT-based transformer video classifier\n",
        "#   - R(2+1)D + ResNet fusion\n",
        "#   - ViT + ResNet fusion\n",
        "#\n",
        "# Folders:\n",
        "#   AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"              (AI)\n",
        "#   SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    (AI)\n",
        "#   REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  (Real)\n",
        "#\n",
        "# Each model is trained on 21 clips, tested on 9 clips.\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Paths (your folders)\n",
        "# -----------------------------\n",
        "AI_CORE_DIR  = \"/content/drive/MyDrive/AudioModel/AI\"              # AI class\n",
        "SORA_DIR     = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    # AI class\n",
        "REAL_DIR     = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  # Real class\n",
        "\n",
        "# -----------------------------\n",
        "# List videos\n",
        "# -----------------------------\n",
        "def list_videos(folder):\n",
        "    vids = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        vids.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return sorted(vids)\n",
        "\n",
        "ai_core_all  = list_videos(AI_CORE_DIR)\n",
        "sora_all     = list_videos(SORA_DIR)\n",
        "real_all     = list_videos(REAL_DIR)\n",
        "\n",
        "# Take first 10 from each to match your experiment\n",
        "ai_core_files = ai_core_all[:10]\n",
        "sora_files    = sora_all[:10]\n",
        "real_files    = real_all[:10]\n",
        "\n",
        "print(\"Found in folders (total):\")\n",
        "print(\"  AI_CORE total:\", len(ai_core_all))\n",
        "print(\"  SORA total   :\", len(sora_all))\n",
        "print(\"  REAL total   :\", len(real_all))\n",
        "\n",
        "print(\"\\nUsing for this experiment:\")\n",
        "print(\"  AI_CORE used :\", len(ai_core_files))\n",
        "print(\"  SORA used    :\", len(sora_files))\n",
        "print(\"  REAL used    :\", len(real_files))\n",
        "\n",
        "# -----------------------------\n",
        "# Build samples (Real=0, AI/Sora=1)\n",
        "# -----------------------------\n",
        "samples = []\n",
        "for p in ai_core_files:\n",
        "    samples.append((p, 1))  # AI\n",
        "for p in sora_files:\n",
        "    samples.append((p, 1))  # Sora as AI\n",
        "for p in real_files:\n",
        "    samples.append((p, 0))  # Real\n",
        "\n",
        "random.shuffle(samples)\n",
        "\n",
        "print(\"\\nTotal samples:\", len(samples))  # ~30\n",
        "\n",
        "# Train / Test split\n",
        "train_ratio = 0.7\n",
        "train_size = max(1, int(len(samples) * train_ratio))\n",
        "train_samples = samples[:train_size]\n",
        "test_samples  = samples[train_size:]\n",
        "\n",
        "print(\"Train videos:\", len(train_samples))\n",
        "print(\"Test  videos:\", len(test_samples))\n",
        "\n",
        "# ============================================================\n",
        "# Low-RAM OpenCV loader (8 frames, 96x96)\n",
        "# ============================================================\n",
        "\n",
        "def load_video_clip_opencv(path, num_frames=8, size=(96, 96)):\n",
        "    \"\"\"\n",
        "    RAM-friendly:\n",
        "      - OpenCV only\n",
        "      - sample num_frames across total frames\n",
        "      - load & resize those frames\n",
        "      - return [C, T, H, W] in [0,1]\n",
        "    \"\"\"\n",
        "    H, W = size\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] Missing file:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[WARN] Cannot open video:\", path)\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        print(\"[WARN] Empty video:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    if frame_count >= num_frames:\n",
        "        idxs = np.linspace(0, frame_count - 1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = int(np.ceil(num_frames / frame_count))\n",
        "        all_idx = np.tile(np.arange(frame_count), reps)\n",
        "        idxs = all_idx[:num_frames]\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    cap.release()\n",
        "    clip = torch.stack(frames, dim=1)  # [C, T, H, W]\n",
        "    return clip\n",
        "\n",
        "# ============================================================\n",
        "# Feature transforms: MID, HIGH, GRAD, FREQ\n",
        "# ============================================================\n",
        "\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645]).view(3, 1, 1, 1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989]).view(3, 1, 1, 1)\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    r, g, b = frames[0], frames[1], frames[2]\n",
        "    lum = 0.299 * r + 0.587 * g + 0.114 * b  # [T,H,W]\n",
        "    lum = lum.unsqueeze(0)  # [1,T,H,W]\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        frame_np = frame.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
        "        blur = cv2.GaussianBlur(frame_np, (ksize, ksize), 0)\n",
        "        residual = frame_np - blur\n",
        "        residual = np.clip(residual * 2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:, t] = torch.from_numpy(residual).permute(2, 0, 1)\n",
        "    return out\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        gx = cv2.Sobel(gray_np, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray_np, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)  # [3,H,W]\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def frequency_spectrum(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        fft = np.fft.fft2(gray_np)\n",
        "        fft_shift = np.fft.fftshift(fft)\n",
        "        mag = np.abs(fft_shift)\n",
        "        mag = np.log1p(mag)\n",
        "\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "# ============================================================\n",
        "# Dataset returning all streams\n",
        "# ============================================================\n",
        "\n",
        "class MultiStreamTinyDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=(96,96)):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_clip_opencv(path, self.num_frames, self.size)  # [C,T,H,W], [0,1]\n",
        "\n",
        "        raw  = normalize_kinetics(frames.clone())\n",
        "        mid  = normalize_kinetics(midtone_mask(frames.clone()))\n",
        "        high = normalize_kinetics(high_frequency_residual(frames.clone()))\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames.clone()))\n",
        "        freq = normalize_kinetics(frequency_spectrum(frames.clone()))\n",
        "\n",
        "        streams = {\n",
        "            \"raw\":  raw,\n",
        "            \"mid\":  mid,\n",
        "            \"high\": high,\n",
        "            \"grad\": grad,\n",
        "            \"freq\": freq,\n",
        "        }\n",
        "        return streams, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = MultiStreamTinyDataset(train_samples, num_frames=8, size=(96,96))\n",
        "test_ds  = MultiStreamTinyDataset(test_samples,  num_frames=8, size=(96,96))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,  num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"\\nSanity check batch:\")\n",
        "for streams, labels in train_loader:\n",
        "    print(\"raw shape:\", streams[\"raw\"].shape, \"labels:\", labels)\n",
        "    break\n",
        "\n",
        "# ============================================================\n",
        "# Backbones: R(2+1)D, ViT, ResNet\n",
        "# ============================================================\n",
        "\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def make_r2plus1d_backbone():\n",
        "    weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "    model = r2plus1d_18(weights=weights)\n",
        "    feat_dim = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, feat_dim\n",
        "\n",
        "def make_vit_backbone():\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    vit = vit_b_16(weights=weights)\n",
        "    feat_dim = vit.heads.head.in_features\n",
        "    vit.heads.head = nn.Identity()\n",
        "    return vit, feat_dim\n",
        "\n",
        "def make_resnet_backbone():\n",
        "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
        "    net = resnet18(weights=weights)\n",
        "    feat_dim = net.fc.in_features\n",
        "    net.fc = nn.Identity()\n",
        "    return net, feat_dim\n",
        "\n",
        "# ============================================================\n",
        "# Plain R(2+1)D multi-stream (for reference)\n",
        "# ============================================================\n",
        "\n",
        "class MultiStreamR2Plus1D(nn.Module):\n",
        "    def __init__(self, use_raw=True, use_mid=False, use_high=False,\n",
        "                 use_grad=False, use_freq=False, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.use_raw = use_raw\n",
        "        self.use_mid = use_mid\n",
        "        self.use_high = use_high\n",
        "        self.use_grad = use_grad\n",
        "        self.use_freq = use_freq\n",
        "\n",
        "        self.streams = nn.ModuleDict()\n",
        "        feat_dims = []\n",
        "\n",
        "        if use_raw:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"raw\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_mid:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"mid\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_high:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"high\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_grad:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"grad\"] = b\n",
        "            feat_dims.append(d)\n",
        "        if use_freq:\n",
        "            b, d = make_r2plus1d_backbone()\n",
        "            for p in b.parameters(): p.requires_grad = False\n",
        "            self.streams[\"freq\"] = b\n",
        "            feat_dims.append(d)\n",
        "\n",
        "        total_dim = sum(feat_dims)\n",
        "        self.head = nn.Linear(total_dim, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        feats = []\n",
        "        if self.use_raw:\n",
        "            feats.append(self.streams[\"raw\"](streams[\"raw\"]))\n",
        "        if self.use_mid:\n",
        "            feats.append(self.streams[\"mid\"](streams[\"mid\"]))\n",
        "        if self.use_high:\n",
        "            feats.append(self.streams[\"high\"](streams[\"high\"]))\n",
        "        if self.use_grad:\n",
        "            feats.append(self.streams[\"grad\"](streams[\"grad\"]))\n",
        "        if self.use_freq:\n",
        "            feats.append(self.streams[\"freq\"](streams[\"freq\"]))\n",
        "\n",
        "        x = torch.cat(feats, dim=1)\n",
        "        out = self.head(x)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Plain ViT video classifier (for reference)\n",
        "# ============================================================\n",
        "\n",
        "class VideoViTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    stream_key in {'raw','mid','high','grad','freq','multi'}\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "        self.vit, feat_dim = make_vit_backbone()\n",
        "        for p in self.vit.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "\n",
        "        if H != 224 or W != 224:\n",
        "            x = F.interpolate(x, size=(224, 224),\n",
        "                              mode=\"bilinear\",\n",
        "                              align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = self.vit(x)  # [B*T, D]\n",
        "\n",
        "        feats = feats.view(B, T, -1).mean(dim=1)\n",
        "        out = self.fc(feats)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Fusion: R(2+1)D + ResNet\n",
        "# ============================================================\n",
        "\n",
        "class R2PlusResNetFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    R(2+1)D video features + ResNet per-frame features (same stream).\n",
        "    stream_key in {'raw','mid','high','grad','freq','multi'}.\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.r2, d_r2 = make_r2plus1d_backbone()\n",
        "        for p in self.r2.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "        for p in self.resnet.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_r2 + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        # pick stream\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]    # [B,C,T,H,W]\n",
        "\n",
        "        # R(2+1)D branch\n",
        "        r2_feats = self.r2(x)              # [B, d_r2]\n",
        "\n",
        "        # ResNet branch: per-frame features + temporal average\n",
        "        B, C, T, H, W = x.shape\n",
        "        frames = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "        if H != 224 or W != 224:\n",
        "            frames = F.interpolate(frames, size=(224, 224),\n",
        "                                   mode=\"bilinear\",\n",
        "                                   align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            res_feats = self.resnet(frames)   # [B*T, d_res]\n",
        "\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)  # [B, d_res]\n",
        "\n",
        "        fused = torch.cat([r2_feats, res_feats], dim=1)\n",
        "        out = self.fc(fused)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Fusion: ViT + ResNet\n",
        "# ============================================================\n",
        "\n",
        "class ViTPlusResNetFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT video features + ResNet per-frame features (same stream).\n",
        "    stream_key in {'raw','mid','high','grad','freq','multi'}.\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.vit, d_vit = make_vit_backbone()\n",
        "        for p in self.vit.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "        for p in self.resnet.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_vit + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        # pick stream\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]    # [B,C,T,H,W]\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        # ViT branch\n",
        "        vit_in = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "        if H != 224 or W != 224:\n",
        "            vit_in = F.interpolate(vit_in, size=(224, 224),\n",
        "                                   mode=\"bilinear\",\n",
        "                                   align_corners=False)\n",
        "        with torch.no_grad():\n",
        "            vit_feats = self.vit(vit_in)      # [B*T, d_vit]\n",
        "        vit_feats = vit_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        # ResNet branch (same frames)\n",
        "        res_in = vit_in\n",
        "        with torch.no_grad():\n",
        "            res_feats = self.resnet(res_in)   # [B*T, d_res]\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        fused = torch.cat([vit_feats, res_feats], dim=1)\n",
        "        out = self.fc(fused)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Generic train / eval helpers\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, epochs=3, tag=\"\"):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if hasattr(model, \"fc\"):\n",
        "        params = model.fc.parameters()\n",
        "    else:\n",
        "        params = model.head.parameters()\n",
        "\n",
        "    optimizer = optim.Adam(params, lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for streams, labels in train_loader:\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(streams)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / max(total, 1)\n",
        "        epoch_acc  = correct / max(total, 1)\n",
        "        print(f\"[{tag}] Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.3f}\")\n",
        "\n",
        "def eval_model(model, test_loader, tag=\"\", target_names=None):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for streams, labels in test_loader:\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(streams)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    if len(all_labels) == 0:\n",
        "        print(f\"[{tag}] Not enough test samples.\")\n",
        "        return 0.0\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({tag}) ===\")\n",
        "    print(classification_report(all_labels, all_preds,\n",
        "                                target_names=target_names,\n",
        "                                digits=4))\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    return acc\n",
        "\n",
        "TARGET_NAMES = [\"Real (0)\", \"AI/Sora (1)\"]\n",
        "\n",
        "# ============================================================\n",
        "# 1) Optional: plain R(2+1)D per stream (reference)\n",
        "# ============================================================\n",
        "\n",
        "r2_results = {}\n",
        "print(\"\\n######## PLAIN R(2+1)D EXPERIMENTS ########\")\n",
        "for cfg_name, kwargs in [\n",
        "    (\"RAW\",   dict(use_raw=True,  use_mid=False, use_high=False, use_grad=False, use_freq=False)),\n",
        "    (\"MID\",   dict(use_raw=False, use_mid=True,  use_high=False, use_grad=False, use_freq=False)),\n",
        "    (\"HIGH\",  dict(use_raw=False, use_mid=False, use_high=True,  use_grad=False, use_freq=False)),\n",
        "    (\"GRAD\",  dict(use_raw=False, use_mid=False, use_high=False, use_grad=True,  use_freq=False)),\n",
        "    (\"FREQ\",  dict(use_raw=False, use_mid=False, use_high=False, use_grad=False, use_freq=True)),\n",
        "    (\"MULTI\", dict(use_raw=True,  use_mid=True,  use_high=True,  use_grad=True,  use_freq=True)),\n",
        "]:\n",
        "    print(f\"\\n--- R2 {cfg_name} ---\")\n",
        "    model = MultiStreamR2Plus1D(num_classes=2, **kwargs)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"R2-{cfg_name}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"R2-{cfg_name}\", target_names=TARGET_NAMES)\n",
        "    r2_results[cfg_name] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== R(2+1)D RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"---------------------------------------------\")\n",
        "for k, v in r2_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=============================================\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Optional: plain ViT per stream (reference)\n",
        "# ============================================================\n",
        "\n",
        "vit_results = {}\n",
        "print(\"\\n######## PLAIN ViT EXPERIMENTS ########\")\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- ViT {key} ---\")\n",
        "    sk = key.lower() if key != \"MULTI\" else \"multi\"\n",
        "    model = VideoViTClassifier(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"ViT-{key}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"ViT-{key}\", target_names=TARGET_NAMES)\n",
        "    vit_results[key] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== ViT RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"----------------------------------------\")\n",
        "for k, v in vit_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=========================================\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) R(2+1)D + ResNet fusion per stream\n",
        "# ============================================================\n",
        "\n",
        "r2res_results = {}\n",
        "print(\"\\n######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\")\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- R2+ResNet {key} ---\")\n",
        "    sk = key.lower() if key != \"MULTI\" else \"multi\"\n",
        "    model = R2PlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"R2Res-{key}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"R2Res-{key}\", target_names=TARGET_NAMES)\n",
        "    r2res_results[key] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== R2 + ResNet RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"-----------------------------------------------\")\n",
        "for k, v in r2res_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=================================================\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) ViT + ResNet fusion per stream\n",
        "# ============================================================\n",
        "\n",
        "vitres_results = {}\n",
        "print(\"\\n######## ViT + ResNet FUSION EXPERIMENTS ########\")\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- ViT+ResNet {key} ---\")\n",
        "    sk = key.lower() if key != \"MULTI\" else \"multi\"\n",
        "    model = ViTPlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"ViTRes-{key}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"ViTRes-{key}\", target_names=TARGET_NAMES)\n",
        "    vitres_results[key] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== ViT + ResNet RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"-----------------------------------------------\")\n",
        "for k, v in vitres_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=================================================\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k9gkET2nreB",
        "outputId": "170a7b98-e6e7-49ad-b2e8-07e432fa791d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Found in folders (total):\n",
            "  AI_CORE total: 1200\n",
            "  SORA total   : 1250\n",
            "  REAL total   : 1200\n",
            "\n",
            "Using for this experiment:\n",
            "  AI_CORE used : 10\n",
            "  SORA used    : 10\n",
            "  REAL used    : 10\n",
            "\n",
            "Total samples: 30\n",
            "Train videos: 21\n",
            "Test  videos: 9\n",
            "\n",
            "Sanity check batch:\n",
            "raw shape: torch.Size([1, 3, 8, 96, 96]) labels: tensor([1])\n",
            "\n",
            "######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- R2+ResNet RAW ---\n",
            "[R2Res-RAW] Epoch 1/3 - loss: 0.7040 - acc: 0.571\n",
            "[R2Res-RAW] Epoch 2/3 - loss: 0.6769 - acc: 0.619\n",
            "[R2Res-RAW] Epoch 3/3 - loss: 0.6812 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7778    1.0000    0.8750         7\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.3889    0.5000    0.4375         9\n",
            "weighted avg     0.6049    0.7778    0.6806         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- R2+ResNet MID ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-MID] Epoch 1/3 - loss: 0.7415 - acc: 0.429\n",
            "[R2Res-MID] Epoch 2/3 - loss: 0.6767 - acc: 0.619\n",
            "[R2Res-MID] Epoch 3/3 - loss: 0.6726 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.3333    1.0000    0.5000         2\n",
            " AI/Sora (1)     1.0000    0.4286    0.6000         7\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.6667    0.7143    0.5500         9\n",
            "weighted avg     0.8519    0.5556    0.5778         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- R2+ResNet HIGH ---\n",
            "[R2Res-HIGH] Epoch 1/3 - loss: 0.7067 - acc: 0.571\n",
            "[R2Res-HIGH] Epoch 2/3 - loss: 0.6759 - acc: 0.619\n",
            "[R2Res-HIGH] Epoch 3/3 - loss: 0.6884 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7778    1.0000    0.8750         7\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.3889    0.5000    0.4375         9\n",
            "weighted avg     0.6049    0.7778    0.6806         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- R2+ResNet GRAD ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[R2Res-GRAD] Epoch 1/3 - loss: 0.7415 - acc: 0.524\n",
            "[R2Res-GRAD] Epoch 2/3 - loss: 0.6713 - acc: 0.619\n",
            "[R2Res-GRAD] Epoch 3/3 - loss: 0.6757 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     1.0000    0.5000    0.6667         2\n",
            " AI/Sora (1)     0.8750    1.0000    0.9333         7\n",
            "\n",
            "    accuracy                         0.8889         9\n",
            "   macro avg     0.9375    0.7500    0.8000         9\n",
            "weighted avg     0.9028    0.8889    0.8741         9\n",
            "\n",
            "Accuracy: 0.8888888888888888\n",
            "\n",
            "--- R2+ResNet FREQ ---\n",
            "[R2Res-FREQ] Epoch 1/3 - loss: 0.7264 - acc: 0.619\n",
            "[R2Res-FREQ] Epoch 2/3 - loss: 0.6727 - acc: 0.619\n",
            "[R2Res-FREQ] Epoch 3/3 - loss: 0.6873 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.5000    0.2857    0.3636         7\n",
            "\n",
            "    accuracy                         0.2222         9\n",
            "   macro avg     0.2500    0.1429    0.1818         9\n",
            "weighted avg     0.3889    0.2222    0.2828         9\n",
            "\n",
            "Accuracy: 0.2222222222222222\n",
            "\n",
            "--- R2+ResNet MULTI ---\n",
            "[R2Res-MULTI] Epoch 1/3 - loss: 0.8063 - acc: 0.524\n",
            "[R2Res-MULTI] Epoch 2/3 - loss: 0.6922 - acc: 0.619\n",
            "[R2Res-MULTI] Epoch 3/3 - loss: 0.6790 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (R2Res-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7143    0.7143    0.7143         7\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.3571    0.3571    0.3571         9\n",
            "weighted avg     0.5556    0.5556    0.5556         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "=========== R2 + ResNet RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "-----------------------------------------------\n",
            "RAW        |   0.7778\n",
            "MID        |   0.5556\n",
            "HIGH       |   0.7778\n",
            "GRAD       |   0.8889\n",
            "FREQ       |   0.2222\n",
            "MULTI      |   0.5556\n",
            "=================================================\n",
            "\n",
            "\n",
            "######## ViT + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- ViT+ResNet RAW ---\n",
            "[ViTRes-RAW] Epoch 1/3 - loss: 0.7050 - acc: 0.524\n",
            "[ViTRes-RAW] Epoch 2/3 - loss: 0.5761 - acc: 0.810\n",
            "[ViTRes-RAW] Epoch 3/3 - loss: 0.5379 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViTRes-RAW) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7143    0.7143    0.7143         7\n",
            "\n",
            "    accuracy                         0.5556         9\n",
            "   macro avg     0.3571    0.3571    0.3571         9\n",
            "weighted avg     0.5556    0.5556    0.5556         9\n",
            "\n",
            "Accuracy: 0.5555555555555556\n",
            "\n",
            "--- ViT+ResNet MID ---\n",
            "[ViTRes-MID] Epoch 1/3 - loss: 0.7453 - acc: 0.476\n",
            "[ViTRes-MID] Epoch 2/3 - loss: 0.6259 - acc: 0.714\n",
            "[ViTRes-MID] Epoch 3/3 - loss: 0.5899 - acc: 0.762\n",
            "\n",
            "=== TEST RESULTS (ViTRes-MID) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.5000    0.5000    0.5000         2\n",
            " AI/Sora (1)     0.8571    0.8571    0.8571         7\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.6786    0.6786    0.6786         9\n",
            "weighted avg     0.7778    0.7778    0.7778         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- ViT+ResNet HIGH ---\n",
            "[ViTRes-HIGH] Epoch 1/3 - loss: 0.7350 - acc: 0.524\n",
            "[ViTRes-HIGH] Epoch 2/3 - loss: 0.6665 - acc: 0.619\n",
            "[ViTRes-HIGH] Epoch 3/3 - loss: 0.6243 - acc: 0.667\n",
            "\n",
            "=== TEST RESULTS (ViTRes-HIGH) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     1.0000    0.5000    0.6667         2\n",
            " AI/Sora (1)     0.8750    1.0000    0.9333         7\n",
            "\n",
            "    accuracy                         0.8889         9\n",
            "   macro avg     0.9375    0.7500    0.8000         9\n",
            "weighted avg     0.9028    0.8889    0.8741         9\n",
            "\n",
            "Accuracy: 0.8888888888888888\n",
            "\n",
            "--- ViT+ResNet GRAD ---\n",
            "[ViTRes-GRAD] Epoch 1/3 - loss: 0.6579 - acc: 0.619\n",
            "[ViTRes-GRAD] Epoch 2/3 - loss: 0.5798 - acc: 0.714\n",
            "[ViTRes-GRAD] Epoch 3/3 - loss: 0.5458 - acc: 0.714\n",
            "\n",
            "=== TEST RESULTS (ViTRes-GRAD) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7778    1.0000    0.8750         7\n",
            "\n",
            "    accuracy                         0.7778         9\n",
            "   macro avg     0.3889    0.5000    0.4375         9\n",
            "weighted avg     0.6049    0.7778    0.6806         9\n",
            "\n",
            "Accuracy: 0.7777777777777778\n",
            "\n",
            "--- ViT+ResNet FREQ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViTRes-FREQ] Epoch 1/3 - loss: 0.7703 - acc: 0.333\n",
            "[ViTRes-FREQ] Epoch 2/3 - loss: 0.6851 - acc: 0.619\n",
            "[ViTRes-FREQ] Epoch 3/3 - loss: 0.6788 - acc: 0.619\n",
            "\n",
            "=== TEST RESULTS (ViTRes-FREQ) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     0.0000    0.0000    0.0000         2\n",
            " AI/Sora (1)     0.7500    0.8571    0.8000         7\n",
            "\n",
            "    accuracy                         0.6667         9\n",
            "   macro avg     0.3750    0.4286    0.4000         9\n",
            "weighted avg     0.5833    0.6667    0.6222         9\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "\n",
            "--- ViT+ResNet MULTI ---\n",
            "[ViTRes-MULTI] Epoch 1/3 - loss: 0.6381 - acc: 0.619\n",
            "[ViTRes-MULTI] Epoch 2/3 - loss: 0.5450 - acc: 0.714\n",
            "[ViTRes-MULTI] Epoch 3/3 - loss: 0.4937 - acc: 0.762\n",
            "\n",
            "=== TEST RESULTS (ViTRes-MULTI) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Real (0)     1.0000    1.0000    1.0000         2\n",
            " AI/Sora (1)     1.0000    1.0000    1.0000         7\n",
            "\n",
            "    accuracy                         1.0000         9\n",
            "   macro avg     1.0000    1.0000    1.0000         9\n",
            "weighted avg     1.0000    1.0000    1.0000         9\n",
            "\n",
            "Accuracy: 1.0\n",
            "\n",
            "=========== ViT + ResNet RESULTS TABLE ===========\n",
            "Stream     |  TestAcc\n",
            "-----------------------------------------------\n",
            "RAW        |   0.5556\n",
            "MID        |   0.7778\n",
            "HIGH       |   0.8889\n",
            "GRAD       |   0.7778\n",
            "FREQ       |   0.6667\n",
            "MULTI      |   1.0000\n",
            "=================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Tiny 10/10/10 experiment with ALL feature ideas:\n",
        "#   RAW, MID, HIGH, GRAD, FREQ, MULTI\n",
        "#\n",
        "#   - R(2+1)D + ResNet18 fusion (video convs + ImageNet features)\n",
        "#   - ViT-B/16 + ResNet18 fusion\n",
        "#\n",
        "# Folders:\n",
        "#   AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"              (AI)\n",
        "#   SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    (AI)\n",
        "#   REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  (Real)\n",
        "#\n",
        "# Each model is trained on 21 clips, tested on 9 clips.\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Paths (your folders)\n",
        "# -----------------------------\n",
        "AI_CORE_DIR  = \"/content/drive/MyDrive/AudioModel/AI\"              # AI class\n",
        "SORA_DIR     = \"/content/drive/MyDrive/soravideo/sora2aivideos\"    # AI class\n",
        "REAL_DIR     = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"  # Real class\n",
        "\n",
        "# -----------------------------\n",
        "# List videos\n",
        "# -----------------------------\n",
        "def list_videos(folder):\n",
        "    vids = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        vids.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return sorted(vids)\n",
        "\n",
        "ai_core_all  = list_videos(AI_CORE_DIR)\n",
        "sora_all     = list_videos(SORA_DIR)\n",
        "real_all     = list_videos(REAL_DIR)\n",
        "\n",
        "# Take first 10 from each to match the tiny experiment\n",
        "ai_core_files = ai_core_all[:10]\n",
        "sora_files    = sora_all[:10]\n",
        "real_files    = real_all[:10]\n",
        "\n",
        "print(\"Found in folders (total):\")\n",
        "print(\"  AI_CORE total:\", len(ai_core_all))\n",
        "print(\"  SORA total   :\", len(sora_all))\n",
        "print(\"  REAL total   :\", len(real_all))\n",
        "\n",
        "print(\"\\nUsing for this experiment:\")\n",
        "print(\"  AI_CORE used :\", len(ai_core_files))\n",
        "print(\"  SORA used    :\", len(sora_files))\n",
        "print(\"  REAL used    :\", len(real_files))\n",
        "\n",
        "# -----------------------------\n",
        "# Build samples (Real=0, AI/Sora=1)\n",
        "# -----------------------------\n",
        "samples = []\n",
        "for p in ai_core_files:\n",
        "    samples.append((p, 1))  # AI\n",
        "for p in sora_files:\n",
        "    samples.append((p, 1))  # Sora as AI\n",
        "for p in real_files:\n",
        "    samples.append((p, 0))  # Real\n",
        "\n",
        "random.shuffle(samples)\n",
        "\n",
        "print(\"\\nTotal samples:\", len(samples))  # ~30\n",
        "\n",
        "# Train / Test split\n",
        "train_ratio = 0.7\n",
        "train_size = max(1, int(len(samples) * train_ratio))\n",
        "train_samples = samples[:train_size]\n",
        "test_samples  = samples[train_size:]\n",
        "\n",
        "print(\"Train videos:\", len(train_samples))\n",
        "print(\"Test  videos:\", len(test_samples))\n",
        "\n",
        "# ============================================================\n",
        "# Low-RAM OpenCV loader (8 frames, 96x96)\n",
        "# ============================================================\n",
        "\n",
        "def load_video_clip_opencv(path, num_frames=8, size=(96, 96)):\n",
        "    \"\"\"\n",
        "    RAM-friendly:\n",
        "      - OpenCV only\n",
        "      - sample num_frames across total frames\n",
        "      - load & resize those frames\n",
        "      - return [C, T, H, W] in [0,1]\n",
        "    \"\"\"\n",
        "    H, W = size\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] Missing file:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[WARN] Cannot open video:\", path)\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        print(\"[WARN] Empty video:\", path)\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    if frame_count >= num_frames:\n",
        "        idxs = np.linspace(0, frame_count - 1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = int(np.ceil(num_frames / frame_count))\n",
        "        all_idx = np.tile(np.arange(frame_count), reps)\n",
        "        idxs = all_idx[:num_frames]\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    cap.release()\n",
        "    clip = torch.stack(frames, dim=1)  # [C, T, H, W]\n",
        "    return clip\n",
        "\n",
        "# ============================================================\n",
        "# Feature transforms: MID, HIGH, GRAD, FREQ\n",
        "# ============================================================\n",
        "\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645]).view(3, 1, 1, 1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989]).view(3, 1, 1, 1)\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    r, g, b = frames[0], frames[1], frames[2]\n",
        "    lum = 0.299 * r + 0.587 * g + 0.114 * b  # [T,H,W]\n",
        "    lum = lum.unsqueeze(0)  # [1,T,H,W]\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        frame_np = frame.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
        "        blur = cv2.GaussianBlur(frame_np, (ksize, ksize), 0)\n",
        "        residual = frame_np - blur\n",
        "        residual = np.clip(residual * 2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:, t] = torch.from_numpy(residual).permute(2, 0, 1)\n",
        "    return out\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]  # [C,H,W]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        gx = cv2.Sobel(gray_np, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray_np, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)  # [3,H,W]\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def frequency_spectrum(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "        gray_np = gray.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        fft = np.fft.fft2(gray_np)\n",
        "        fft_shift = np.fft.fftshift(fft)\n",
        "        mag = np.abs(fft_shift)\n",
        "        mag = np.log1p(mag)\n",
        "\n",
        "        if mag.max() > 0:\n",
        "            mag /= (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "# ============================================================\n",
        "# Dataset returning all streams\n",
        "# ============================================================\n",
        "\n",
        "class MultiStreamTinyDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=(96,96)):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_clip_opencv(path, self.num_frames, self.size)  # [C,T,H,W], [0,1]\n",
        "\n",
        "        raw  = normalize_kinetics(frames.clone())\n",
        "        mid  = normalize_kinetics(midtone_mask(frames.clone()))\n",
        "        high = normalize_kinetics(high_frequency_residual(frames.clone()))\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames.clone()))\n",
        "        freq = normalize_kinetics(frequency_spectrum(frames.clone()))\n",
        "\n",
        "        streams = {\n",
        "            \"raw\":  raw,\n",
        "            \"mid\":  mid,\n",
        "            \"high\": high,\n",
        "            \"grad\": grad,\n",
        "            \"freq\": freq,\n",
        "        }\n",
        "        return streams, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = MultiStreamTinyDataset(train_samples, num_frames=8, size=(96,96))\n",
        "test_ds  = MultiStreamTinyDataset(test_samples,  num_frames=8, size=(96,96))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,  num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"\\nSanity check batch:\")\n",
        "for streams, labels in train_loader:\n",
        "    print(\"raw shape:\", streams[\"raw\"].shape, \"labels:\", labels)\n",
        "    break\n",
        "\n",
        "# ============================================================\n",
        "# Backbones: R(2+1)D, ViT, ResNet\n",
        "# ============================================================\n",
        "\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def make_r2plus1d_backbone():\n",
        "    weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "    model = r2plus1d_18(weights=weights)\n",
        "    feat_dim = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, feat_dim\n",
        "\n",
        "def make_vit_backbone():\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    vit = vit_b_16(weights=weights)\n",
        "    feat_dim = vit.heads.head.in_features\n",
        "    vit.heads.head = nn.Identity()\n",
        "    return vit, feat_dim\n",
        "\n",
        "def make_resnet_backbone():\n",
        "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
        "    net = resnet18(weights=weights)\n",
        "    feat_dim = net.fc.in_features\n",
        "    net.fc = nn.Identity()\n",
        "    return net, feat_dim\n",
        "\n",
        "# ============================================================\n",
        "# Fusion: R(2+1)D + ResNet\n",
        "# ============================================================\n",
        "\n",
        "class R2PlusResNetFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    R(2+1)D video features + ResNet per-frame features (same stream).\n",
        "    stream_key in {'raw','mid','high','grad','freq','multi'}.\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.r2, d_r2 = make_r2plus1d_backbone()\n",
        "        for p in self.r2.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_r2 + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        # pick stream\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]    # [B,C,T,H,W]\n",
        "\n",
        "        # R(2+1)D branch\n",
        "        r2_feats = self.r2(x)              # [B, d_r2]\n",
        "\n",
        "        # ResNet branch: per-frame features + temporal average\n",
        "        B, C, T, H, W = x.shape\n",
        "        frames = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "        if H != 224 or W != 224:\n",
        "            frames = F.interpolate(frames, size=(224, 224),\n",
        "                                   mode=\"bilinear\",\n",
        "                                   align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            res_feats = self.resnet(frames)   # [B*T, d_res]\n",
        "\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)  # [B, d_res]\n",
        "\n",
        "        fused = torch.cat([r2_feats, res_feats], dim=1)\n",
        "        out = self.fc(fused)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Fusion: ViT + ResNet\n",
        "# ============================================================\n",
        "\n",
        "class ViTPlusResNetFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT video features + ResNet per-frame features (same stream).\n",
        "    stream_key in {'raw','mid','high','grad','freq','multi'}.\n",
        "    \"\"\"\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.vit, d_vit = make_vit_backbone()\n",
        "        for p in self.vit.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_vit + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        # pick stream\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] +\n",
        "                 streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]    # [B,C,T,H,W]\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        # ViT branch\n",
        "        vit_in = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
        "        if H != 224 or W != 224:\n",
        "            vit_in = F.interpolate(vit_in, size=(224, 224),\n",
        "                                   mode=\"bilinear\",\n",
        "                                   align_corners=False)\n",
        "        with torch.no_grad():\n",
        "            vit_feats = self.vit(vit_in)      # [B*T, d_vit]\n",
        "        vit_feats = vit_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        # ResNet branch (same frames)\n",
        "        res_in = vit_in\n",
        "        with torch.no_grad():\n",
        "            res_feats = self.resnet(res_in)   # [B*T, d_res]\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        fused = torch.cat([vit_feats, res_feats], dim=1)\n",
        "        out = self.fc(fused)\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# Generic train / eval helpers\n",
        "# ============================================================\n",
        "\n",
        "def train_model(model, train_loader, epochs=3, tag=\"\"):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    params = model.fc.parameters()\n",
        "    optimizer = optim.Adam(params, lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for streams, labels in train_loader:\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(streams)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / max(total, 1)\n",
        "        epoch_acc  = correct / max(total, 1)\n",
        "        print(f\"[{tag}] Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.3f}\")\n",
        "\n",
        "def eval_model(model, test_loader, tag=\"\", target_names=None):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for streams, labels in test_loader:\n",
        "            for k in streams.keys():\n",
        "                streams[k] = streams[k].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(streams)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    if len(all_labels) == 0:\n",
        "        print(f\"[{tag}] Not enough test samples.\")\n",
        "        return 0.0\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({tag}) ===\")\n",
        "    print(classification_report(all_labels, all_preds,\n",
        "                                target_names=target_names,\n",
        "                                digits=4))\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    return acc\n",
        "\n",
        "TARGET_NAMES = [\"Real (0)\", \"AI/Sora (1)\"]\n",
        "\n",
        "# ============================================================\n",
        "# 1) R(2+1)D + ResNet fusion per stream\n",
        "# ============================================================\n",
        "\n",
        "r2res_results = {}\n",
        "print(\"\\n######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\")\n",
        "\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- R2+ResNet {key} ---\")\n",
        "    sk = key.lower() if key != \"MULTI\" else \"multi\"\n",
        "    model = R2PlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"R2Res-{key}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"R2Res-{key}\", target_names=TARGET_NAMES)\n",
        "    r2res_results[key] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== R2 + ResNet RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"-----------------------------------------------\")\n",
        "for k, v in r2res_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=================================================\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) ViT + ResNet fusion per stream\n",
        "# ============================================================\n",
        "\n",
        "vitres_results = {}\n",
        "print(\"\\n######## ViT + ResNet FUSION EXPERIMENTS ########\")\n",
        "\n",
        "for key in [\"RAW\", \"MID\", \"HIGH\", \"GRAD\", \"FREQ\", \"MULTI\"]:\n",
        "    print(f\"\\n--- ViT+ResNet {key} ---\")\n",
        "    sk = key.lower() if key != \"MULTI\" else \"multi\"\n",
        "    model = ViTPlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, tag=f\"ViTRes-{key}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"ViTRes-{key}\", target_names=TARGET_NAMES)\n",
        "    vitres_results[key] = acc\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n=========== ViT + ResNet RESULTS TABLE ===========\")\n",
        "print(\"{:<10} | {:>8}\".format(\"Stream\", \"TestAcc\"))\n",
        "print(\"-----------------------------------------------\")\n",
        "for k, v in vitres_results.items():\n",
        "    print(\"{:<10} | {:>8.4f}\".format(k, v))\n",
        "print(\"=================================================\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tNAkgFVLSIG",
        "outputId": "17df6d14-4cd0-405a-811f-76c794f6f3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "AI_CORE: 1200\n",
            "SORA   : 1250\n",
            "REAL   : 1200\n",
            "AI total: 2450\n",
            "\n",
            "Checking AI candidates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|â–ˆâ–‰        | 29/150 [00:06<00:27,  4.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking REAL candidates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|â–ˆâ–‰        | 29/150 [00:00<00:00, 250.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final sample size: 60 (AI=30, REAL=30)\n",
            "\n",
            "Split sizes (no overlap):\n",
            "Train: 42\n",
            "Val  : 9\n",
            "Test : 9\n",
            "\n",
            "Sanity batch: torch.Size([4, 3, 8, 96, 96]) tensor([0, 1, 0, 1])\n",
            "Train counts: Counter({1: 21, 0: 21}) class_weights: [0.5, 0.5]\n",
            "\n",
            "Saving run to: /content/drive/MyDrive/ai_detection_eval/ABLATION_60_NOLEAK_R2RES_20251213_025808\n",
            "\n",
            "==============================\n",
            "Training method: RAW\n",
            "==============================\n",
            "  RAW | Epoch 01/8 | loss=0.7888 acc=0.333 | val_acc=0.556 val_f1=0.500\n",
            "  RAW | Epoch 02/8 | loss=0.7434 acc=0.500 | val_acc=0.667 val_f1=0.400\n",
            "  RAW | Epoch 03/8 | loss=0.7139 acc=0.524 | val_acc=0.667 val_f1=0.400\n",
            "  RAW | Epoch 04/8 | loss=0.6954 acc=0.524 | val_acc=0.556 val_f1=0.333\n",
            "  RAW | Epoch 05/8 | loss=0.6724 acc=0.524 | val_acc=0.556 val_f1=0.333\n",
            "  RAW | Epoch 06/8 | loss=0.6553 acc=0.595 | val_acc=0.556 val_f1=0.333\n",
            "  RAW | Epoch 07/8 | loss=0.6561 acc=0.548 | val_acc=0.778 val_f1=0.500\n",
            "  RAW | Epoch 08/8 | loss=0.6268 acc=0.643 | val_acc=0.667 val_f1=0.400\n",
            "\n",
            "==============================\n",
            "Training method: MID\n",
            "==============================\n",
            "  MID | Epoch 01/8 | loss=0.7743 acc=0.381 | val_acc=0.444 val_f1=0.444\n",
            "  MID | Epoch 02/8 | loss=0.7675 acc=0.357 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 03/8 | loss=0.7122 acc=0.476 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 04/8 | loss=0.6921 acc=0.595 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 05/8 | loss=0.7068 acc=0.429 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 06/8 | loss=0.6782 acc=0.595 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 07/8 | loss=0.6615 acc=0.690 | val_acc=0.667 val_f1=0.400\n",
            "  MID | Epoch 08/8 | loss=0.6867 acc=0.571 | val_acc=0.667 val_f1=0.400\n",
            "\n",
            "==============================\n",
            "Training method: HIGH\n",
            "==============================\n",
            " HIGH | Epoch 01/8 | loss=0.6956 acc=0.548 | val_acc=0.667 val_f1=0.400\n",
            " HIGH | Epoch 02/8 | loss=0.6613 acc=0.667 | val_acc=0.556 val_f1=0.500\n",
            " HIGH | Epoch 03/8 | loss=0.6840 acc=0.595 | val_acc=0.667 val_f1=0.667\n",
            " HIGH | Epoch 04/8 | loss=0.6456 acc=0.643 | val_acc=0.778 val_f1=0.750\n",
            " HIGH | Epoch 05/8 | loss=0.6625 acc=0.595 | val_acc=0.778 val_f1=0.750\n",
            " HIGH | Epoch 06/8 | loss=0.5826 acc=0.690 | val_acc=0.778 val_f1=0.750\n",
            " HIGH | Epoch 07/8 | loss=0.5931 acc=0.738 | val_acc=0.778 val_f1=0.750\n",
            " HIGH | Epoch 08/8 | loss=0.5736 acc=0.786 | val_acc=0.778 val_f1=0.750\n",
            "\n",
            "==============================\n",
            "Training method: GRAD\n",
            "==============================\n",
            " GRAD | Epoch 01/8 | loss=0.7160 acc=0.429 | val_acc=0.556 val_f1=0.000\n",
            " GRAD | Epoch 02/8 | loss=0.6677 acc=0.643 | val_acc=0.667 val_f1=0.000\n",
            " GRAD | Epoch 03/8 | loss=0.6452 acc=0.619 | val_acc=0.667 val_f1=0.000\n",
            " GRAD | Epoch 04/8 | loss=0.6427 acc=0.619 | val_acc=0.667 val_f1=0.000\n",
            " GRAD | Epoch 05/8 | loss=0.6346 acc=0.690 | val_acc=0.667 val_f1=0.000\n",
            " GRAD | Epoch 06/8 | loss=0.5802 acc=0.786 | val_acc=0.667 val_f1=0.000\n",
            " GRAD | Epoch 07/8 | loss=0.6442 acc=0.643 | val_acc=0.778 val_f1=0.500\n",
            " GRAD | Epoch 08/8 | loss=0.6077 acc=0.786 | val_acc=0.778 val_f1=0.500\n",
            "\n",
            "==============================\n",
            "Training method: FREQ\n",
            "==============================\n",
            " FREQ | Epoch 01/8 | loss=0.7593 acc=0.405 | val_acc=0.667 val_f1=0.000\n",
            " FREQ | Epoch 02/8 | loss=0.7175 acc=0.476 | val_acc=0.556 val_f1=0.000\n",
            " FREQ | Epoch 03/8 | loss=0.7191 acc=0.476 | val_acc=0.667 val_f1=0.000\n",
            " FREQ | Epoch 04/8 | loss=0.7204 acc=0.429 | val_acc=0.667 val_f1=0.000\n",
            " FREQ | Epoch 05/8 | loss=0.7034 acc=0.476 | val_acc=0.667 val_f1=0.000\n",
            " FREQ | Epoch 06/8 | loss=0.6720 acc=0.595 | val_acc=0.556 val_f1=0.000\n",
            " FREQ | Epoch 07/8 | loss=0.6608 acc=0.738 | val_acc=0.444 val_f1=0.000\n",
            " FREQ | Epoch 08/8 | loss=0.6344 acc=0.738 | val_acc=0.444 val_f1=0.000\n",
            "\n",
            "==============================\n",
            "Training method: MULTI\n",
            "==============================\n",
            "MULTI | Epoch 01/8 | loss=0.7055 acc=0.476 | val_acc=0.444 val_f1=0.545\n",
            "MULTI | Epoch 02/8 | loss=0.6720 acc=0.548 | val_acc=0.556 val_f1=0.500\n",
            "MULTI | Epoch 03/8 | loss=0.6386 acc=0.667 | val_acc=0.667 val_f1=0.400\n",
            "MULTI | Epoch 04/8 | loss=0.6304 acc=0.619 | val_acc=0.667 val_f1=0.571\n",
            "MULTI | Epoch 05/8 | loss=0.6114 acc=0.714 | val_acc=0.667 val_f1=0.571\n",
            "MULTI | Epoch 06/8 | loss=0.5751 acc=0.762 | val_acc=0.667 val_f1=0.571\n",
            "MULTI | Epoch 07/8 | loss=0.5605 acc=0.762 | val_acc=0.667 val_f1=0.571\n",
            "MULTI | Epoch 08/8 | loss=0.5755 acc=0.810 | val_acc=0.667 val_f1=0.400\n",
            "\n",
            "=========== SUMMARY (TEST) ===========\n",
            "  RAW | acc=0.667 f1=0.800 auc=0.6111111111111112\n",
            "  MID | acc=0.778 f1=0.857 auc=0.8333333333333334\n",
            " HIGH | acc=0.556 f1=0.667 auc=0.3888888888888889\n",
            " GRAD | acc=0.889 f1=0.909 auc=0.8333333333333334\n",
            " FREQ | acc=0.333 f1=0.000 auc=0.16666666666666666\n",
            "MULTI | acc=0.556 f1=0.600 auc=0.5555555555555557\n",
            "\n",
            "All saved to: /content/drive/MyDrive/ai_detection_eval/ABLATION_60_NOLEAK_R2RES_20251213_025808\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# QUICK ABLATION ON 60 VIDEOS (NO LEAKAGE)\n",
        "# Methods: RAW, MID, HIGH, GRAD, FREQ, MULTI\n",
        "# Model: R(2+1)D + ResNet18 fusion (frozen backbones, train head only)\n",
        "# Data: AI = AI_CORE + SORA, REAL = REAL folder\n",
        "# Split: train/val/test by file-path (no overlap)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "import os, glob, random, json, time\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "    # folders\n",
        "    AI_CORE_DIR: str = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "    SORA_DIR:    str = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "    REAL_DIR:    str = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "    # 60-video experiment (balanced)\n",
        "    total_videos: int = 60\n",
        "    per_class: int = 30  # 30 AI + 30 REAL\n",
        "\n",
        "    # split (no leakage)\n",
        "    train_ratio: float = 0.70\n",
        "    val_ratio:   float = 0.15\n",
        "    # remainder is test\n",
        "\n",
        "    # video decoding\n",
        "    num_frames: int = 8\n",
        "    resize_hw: tuple = (96, 96)  # H,W for fast runs\n",
        "\n",
        "    # training\n",
        "    epochs: int = 8\n",
        "    batch_size: int = 4\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-3  # regularization on head\n",
        "\n",
        "    # save\n",
        "    out_root: str = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "    run_name: str = \"ABLATION_60_NOLEAK_R2RES\"\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_all(cfg.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers: list + validate videos\n",
        "# -----------------------------\n",
        "def list_videos(folder):\n",
        "    vids = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        vids.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return sorted(vids)\n",
        "\n",
        "def is_valid_video(path, min_frames=2):\n",
        "    if not os.path.exists(path):\n",
        "        return False\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return False\n",
        "    fc = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "    return fc >= min_frames\n",
        "\n",
        "ai_core_all = list_videos(cfg.AI_CORE_DIR)\n",
        "sora_all    = list_videos(cfg.SORA_DIR)\n",
        "real_all    = list_videos(cfg.REAL_DIR)\n",
        "\n",
        "ai_all = ai_core_all + sora_all\n",
        "\n",
        "print(\"AI_CORE:\", len(ai_core_all))\n",
        "print(\"SORA   :\", len(sora_all))\n",
        "print(\"REAL   :\", len(real_all))\n",
        "print(\"AI total:\", len(ai_all))\n",
        "\n",
        "# -----------------------------\n",
        "# Sample balanced 30/30 AFTER cleaning\n",
        "# -----------------------------\n",
        "random.shuffle(ai_all)\n",
        "random.shuffle(real_all)\n",
        "\n",
        "# Over-sample candidates so we still get enough after filtering\n",
        "ai_candidates   = ai_all[: max(cfg.per_class * 5, cfg.per_class)]\n",
        "real_candidates = real_all[: max(cfg.per_class * 5, cfg.per_class)]\n",
        "\n",
        "clean_ai, clean_real = [], []\n",
        "\n",
        "print(\"\\nChecking AI candidates...\")\n",
        "for p in tqdm(ai_candidates, total=len(ai_candidates)):\n",
        "    if is_valid_video(p):\n",
        "        clean_ai.append(p)\n",
        "        if len(clean_ai) >= cfg.per_class:\n",
        "            break\n",
        "\n",
        "print(\"Checking REAL candidates...\")\n",
        "for p in tqdm(real_candidates, total=len(real_candidates)):\n",
        "    if is_valid_video(p):\n",
        "        clean_real.append(p)\n",
        "        if len(clean_real) >= cfg.per_class:\n",
        "            break\n",
        "\n",
        "assert len(clean_ai) == cfg.per_class, f\"Not enough valid AI videos. Got {len(clean_ai)}\"\n",
        "assert len(clean_real) == cfg.per_class, f\"Not enough valid REAL videos. Got {len(clean_real)}\"\n",
        "\n",
        "# Build samples: (path,label) label: Real=0, AI=1\n",
        "samples = [(p, 1) for p in clean_ai] + [(p, 0) for p in clean_real]\n",
        "random.shuffle(samples)\n",
        "print(f\"\\nFinal sample size: {len(samples)} (AI={cfg.per_class}, REAL={cfg.per_class})\")\n",
        "\n",
        "# -----------------------------\n",
        "# No-leakage split by unique path\n",
        "# -----------------------------\n",
        "n = len(samples)\n",
        "n_train = int(n * cfg.train_ratio)\n",
        "n_val   = int(n * cfg.val_ratio)\n",
        "n_test  = n - n_train - n_val\n",
        "\n",
        "train_samples = samples[:n_train]\n",
        "val_samples   = samples[n_train:n_train+n_val]\n",
        "test_samples  = samples[n_train+n_val:]\n",
        "\n",
        "# leakage check\n",
        "train_paths = set([p for p,_ in train_samples])\n",
        "val_paths   = set([p for p,_ in val_samples])\n",
        "test_paths  = set([p for p,_ in test_samples])\n",
        "\n",
        "assert train_paths.isdisjoint(val_paths)\n",
        "assert train_paths.isdisjoint(test_paths)\n",
        "assert val_paths.isdisjoint(test_paths)\n",
        "\n",
        "print(\"\\nSplit sizes (no overlap):\")\n",
        "print(\"Train:\", len(train_samples))\n",
        "print(\"Val  :\", len(val_samples))\n",
        "print(\"Test :\", len(test_samples))\n",
        "\n",
        "# -----------------------------\n",
        "# Video loader (OpenCV) -> [C,T,H,W] in [0,1]\n",
        "# -----------------------------\n",
        "def load_video_clip_opencv(path, num_frames=8, size=(96,96)):\n",
        "    H, W = size\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    if frame_count >= num_frames:\n",
        "        idxs = np.linspace(0, frame_count - 1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = int(np.ceil(num_frames / frame_count))\n",
        "        idxs = np.tile(np.arange(frame_count), reps)[:num_frames]\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        frames.append(torch.from_numpy(frame_rgb).permute(2,0,1).float()/255.0)\n",
        "\n",
        "    cap.release()\n",
        "    return torch.stack(frames, dim=1)  # [C,T,H,W]\n",
        "\n",
        "# -----------------------------\n",
        "# Streams (RAW/MID/HIGH/GRAD/FREQ/MULTI)\n",
        "# -----------------------------\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    r,g,b = frames[0],frames[1],frames[2]\n",
        "    lum = (0.299*r + 0.587*g + 0.114*b).unsqueeze(0)  # [1,T,H,W]\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    C,T,H,W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        f = frames[:,t].permute(1,2,0).cpu().numpy()\n",
        "        blur = cv2.GaussianBlur(f, (ksize,ksize), 0)\n",
        "        res = f - blur\n",
        "        res = np.clip(res*2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:,t] = torch.from_numpy(res).permute(2,0,1)\n",
        "    return out\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    C,T,H,W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        f = frames[:,t]\n",
        "        gray = (0.299*f[0] + 0.587*f[1] + 0.114*f[2]).cpu().numpy().astype(np.float32)\n",
        "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx*gx + gy*gy)\n",
        "        if mag.max() > 0:\n",
        "            mag = mag / (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag,mag,mag], axis=0)\n",
        "        out[:,t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def frequency_spectrum(frames):\n",
        "    C,T,H,W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        f = frames[:,t]\n",
        "        gray = (0.299*f[0] + 0.587*f[1] + 0.114*f[2]).cpu().numpy().astype(np.float32)\n",
        "        fft = np.fft.fft2(gray)\n",
        "        fft = np.fft.fftshift(fft)\n",
        "        mag = np.log1p(np.abs(fft))\n",
        "        if mag.max() > 0:\n",
        "            mag = mag / (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag,mag,mag], axis=0)\n",
        "        out[:,t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset that returns all streams\n",
        "# -----------------------------\n",
        "class MultiStreamDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=(96,96)):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_clip_opencv(path, self.num_frames, self.size)  # [C,T,H,W] [0,1]\n",
        "\n",
        "        raw  = normalize_kinetics(frames.clone())\n",
        "        mid  = normalize_kinetics(midtone_mask(frames.clone()))\n",
        "        high = normalize_kinetics(high_frequency_residual(frames.clone()))\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames.clone()))\n",
        "        freq = normalize_kinetics(frequency_spectrum(frames.clone()))\n",
        "\n",
        "        streams = {\"raw\":raw, \"mid\":mid, \"high\":high, \"grad\":grad, \"freq\":freq}\n",
        "        return streams, torch.tensor(label, dtype=torch.long), path\n",
        "\n",
        "def make_loaders(train_s, val_s, test_s):\n",
        "    train_ds = MultiStreamDataset(train_s, cfg.num_frames, cfg.resize_hw)\n",
        "    val_ds   = MultiStreamDataset(val_s,   cfg.num_frames, cfg.resize_hw)\n",
        "    test_ds  = MultiStreamDataset(test_s,  cfg.num_frames, cfg.resize_hw)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = make_loaders(train_samples, val_samples, test_samples)\n",
        "\n",
        "# sanity\n",
        "streams_b, y_b, p_b = next(iter(train_loader))\n",
        "print(\"\\nSanity batch:\", streams_b[\"raw\"].shape, y_b)\n",
        "\n",
        "# -----------------------------\n",
        "# Class weights (from TRAIN only)\n",
        "# -----------------------------\n",
        "train_counts = Counter([y for _,y in train_samples])\n",
        "real_count = train_counts.get(0,0)\n",
        "ai_count   = train_counts.get(1,0)\n",
        "w_real = 1.0 / max(real_count,1)\n",
        "w_ai   = 1.0 / max(ai_count,1)\n",
        "s = w_real + w_ai\n",
        "class_weights = torch.tensor([w_real/s, w_ai/s], dtype=torch.float32, device=device)\n",
        "print(\"Train counts:\", train_counts, \"class_weights:\", class_weights.tolist())\n",
        "\n",
        "# -----------------------------\n",
        "# Model: R2+ResNet fusion (frozen backbones)\n",
        "# -----------------------------\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def make_r2_backbone():\n",
        "    m = r2plus1d_18(weights=R2Plus1D_18_Weights.KINETICS400_V1)\n",
        "    d = m.fc.in_features\n",
        "    m.fc = nn.Identity()\n",
        "    return m, d\n",
        "\n",
        "def make_resnet_backbone():\n",
        "    m = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    d = m.fc.in_features\n",
        "    m.fc = nn.Identity()\n",
        "    return m, d\n",
        "\n",
        "class R2ResFusion(nn.Module):\n",
        "    def __init__(self, stream_key=\"raw\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "        self.r2, d_r2 = make_r2_backbone()\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "        for p in self.r2.parameters(): p.requires_grad = False\n",
        "        for p in self.resnet.parameters(): p.requires_grad = False\n",
        "        self.fc = nn.Linear(d_r2 + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        if self.stream_key == \"multi\":\n",
        "            x = (streams[\"raw\"] + streams[\"mid\"] + streams[\"high\"] + streams[\"grad\"] + streams[\"freq\"]) / 5.0\n",
        "        else:\n",
        "            x = streams[self.stream_key]  # [B,C,T,H,W]\n",
        "\n",
        "        r2_feats = self.r2(x)  # [B,d_r2]\n",
        "\n",
        "        B,C,T,H,W = x.shape\n",
        "        frames = x.permute(0,2,1,3,4).contiguous().view(B*T, C, H, W)\n",
        "        frames = F.interpolate(frames, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
        "        with torch.no_grad():\n",
        "            res = self.resnet(frames)  # [B*T,d_res]\n",
        "        res = res.view(B, T, -1).mean(dim=1)  # [B,d_res]\n",
        "\n",
        "        fused = torch.cat([r2_feats, res], dim=1)\n",
        "        return self.fc(fused)\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, y_pred, y_prob):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0)\n",
        "    out = {\"acc\": float(acc), \"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1)}\n",
        "    # AUC/AP require both classes present\n",
        "    try:\n",
        "        out[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob))\n",
        "    except:\n",
        "        out[\"roc_auc\"] = None\n",
        "    try:\n",
        "        out[\"ap\"] = float(average_precision_score(y_true, y_prob))\n",
        "    except:\n",
        "        out[\"ap\"] = None\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop(model, loader, stream_key):\n",
        "    model.eval()\n",
        "    ys, preds, probs, paths = [], [], [], []\n",
        "    for streams, y, p in loader:\n",
        "        for k in streams: streams[k] = streams[k].to(device)\n",
        "        y = y.to(device).view(-1)\n",
        "        logits = model(streams)\n",
        "        prob = torch.softmax(logits, dim=1)[:,1]\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        ys += y.cpu().tolist()\n",
        "        preds += pred.cpu().tolist()\n",
        "        probs += prob.cpu().tolist()\n",
        "        paths += list(p)\n",
        "\n",
        "    return compute_metrics(ys, preds, probs), ys, preds, probs, paths\n",
        "\n",
        "# -----------------------------\n",
        "# Train loop (head only) + best val-f1\n",
        "# -----------------------------\n",
        "def train_one_method(stream_key):\n",
        "    model = R2ResFusion(stream_key=stream_key).to(device)\n",
        "    crit = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    opt = optim.Adam(model.fc.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    best = {\"val_f1\": -1, \"state\": None, \"epoch\": -1, \"val_metrics\": None}\n",
        "\n",
        "    for ep in range(1, cfg.epochs+1):\n",
        "        model.train()\n",
        "        total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "        for streams, y, _ in train_loader:\n",
        "            for k in streams: streams[k] = streams[k].to(device)\n",
        "            y = y.to(device).view(-1)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            logits = model(streams)\n",
        "            loss = crit(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        train_loss = loss_sum / max(total,1)\n",
        "        train_acc  = correct / max(total,1)\n",
        "\n",
        "        val_metrics, *_ = eval_loop(model, val_loader, stream_key)\n",
        "        print(f\"{stream_key.upper():>5} | Epoch {ep:02d}/{cfg.epochs} | loss={train_loss:.4f} acc={train_acc:.3f} | val_acc={val_metrics['acc']:.3f} val_f1={val_metrics['f1']:.3f}\")\n",
        "\n",
        "        if val_metrics[\"f1\"] > best[\"val_f1\"]:\n",
        "            best[\"val_f1\"] = val_metrics[\"f1\"]\n",
        "            best[\"epoch\"] = ep\n",
        "            best[\"val_metrics\"] = val_metrics\n",
        "            best[\"state\"] = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "\n",
        "    # load best\n",
        "    model.load_state_dict(best[\"state\"])\n",
        "    test_metrics, y_true, y_pred, y_prob, paths = eval_loop(model, test_loader, stream_key)\n",
        "    return model, best, test_metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Run folder\n",
        "# -----------------------------\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_dir = os.path.join(cfg.out_root, f\"{cfg.run_name}_{ts}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "print(\"\\nSaving run to:\", run_dir)\n",
        "\n",
        "# Save split paths (proves no leakage)\n",
        "split_dump = {\n",
        "    \"train\": [p for p,_ in train_samples],\n",
        "    \"val\":   [p for p,_ in val_samples],\n",
        "    \"test\":  [p for p,_ in test_samples],\n",
        "}\n",
        "with open(os.path.join(run_dir, \"splits_paths.json\"), \"w\") as f:\n",
        "    json.dump(split_dump, f, indent=2)\n",
        "\n",
        "# -----------------------------\n",
        "# Run all methods\n",
        "# -----------------------------\n",
        "methods = [\"raw\", \"mid\", \"high\", \"grad\", \"freq\", \"multi\"]\n",
        "results = {}\n",
        "\n",
        "for m in methods:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Training method:\", m.upper())\n",
        "    print(\"==============================\")\n",
        "    model, best, test_metrics = train_one_method(m)\n",
        "\n",
        "    # Save model checkpoint (best val-f1)\n",
        "    ckpt_path = os.path.join(run_dir, f\"R2Res_{m.upper()}_best.pt\")\n",
        "    torch.save({\n",
        "        \"method\": m,\n",
        "        \"cfg\": cfg.__dict__,\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"best_val_metrics\": best[\"val_metrics\"],\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"class_weights\": class_weights.detach().cpu().tolist(),\n",
        "        \"train_counts\": dict(Counter([y for _,y in train_samples])),\n",
        "        \"splits_paths_file\": \"splits_paths.json\",\n",
        "    }, ckpt_path)\n",
        "\n",
        "    results[m] = {\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"best_val_f1\": best[\"val_f1\"],\n",
        "        \"best_val_metrics\": best[\"val_metrics\"],\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"ckpt\": ckpt_path,\n",
        "    }\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save summary table\n",
        "with open(os.path.join(run_dir, \"ablation_results.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\n=========== SUMMARY (TEST) ===========\")\n",
        "for m in methods:\n",
        "    tm = results[m][\"test_metrics\"]\n",
        "    print(f\"{m.upper():>5} | acc={tm['acc']:.3f} f1={tm['f1']:.3f} auc={tm['roc_auc'] if tm['roc_auc'] is not None else 'NA'}\")\n",
        "\n",
        "print(\"\\nAll saved to:\", run_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqc0gI6xx_-d",
        "outputId": "79354053-5d4e-4002-b31c-746d7698048e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run dir: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251229_070439\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "Balanced pool: 2400 total | AI=1200 REAL=1200\n",
            "\n",
            "Computing hashes (this may take a few minutes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [19:05<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact dupes removed: 2\n",
            "\n",
            "Clustering near-dupes within buckets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Buckets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1936/1936 [00:00<00:00, 29049.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Near-dupes removed: 27\n",
            "\n",
            "After dedup: 2370 videos\n",
            "\n",
            "Split sizes (group-safe):\n",
            "Train: 1897  | Test: 473\n",
            "Train AI/REAL: 953 / 944\n",
            "Test  AI/REAL: 238 / 235\n",
            "\n",
            "Normalizing TRAIN videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1897/1897 [16:54<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train normalized: 1897 / 1897\n",
            "\n",
            "Normalizing TEST videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [04:18<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test normalized: 473 / 473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHA1 train_norm: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1897/1897 [00:13<00:00, 140.28it/s]\n",
            "SHA1 test_norm: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:03<00:00, 142.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[NORM CHECK 1] Exact dupes across splits: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dHash train_norm: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1897/1897 [00:13<00:00, 145.28it/s]\n",
            "dHash test_norm: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:03<00:00, 139.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NORM CHECK 2] Near-dupe pairs found (first 50 search) H<=3: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1897/1897 [00:08<00:00, 221.75it/s]\n",
            "Metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:02<00:00, 216.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[NORM CHECK 3] Metadata-only baseline (should be MUCH LOWER than before)\n",
            "ACC: 0.7294 F1: 0.6735 AUC: 0.7457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PCA train feats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1897/1897 [01:17<00:00, 24.33it/s]\n",
            "PCA test feats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:19<00:00, 24.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PCA feat extraction done.\n",
            "Train feats: (1897, 1024) bad: 0\n",
            "Test  feats: (473, 1024) bad: 0\n",
            "\n",
            "=== PCA DETECTOR (Temporal Gradient-PCA) ===\n",
            "ACC: 0.8668\n",
            "F1 : 0.8627\n",
            "AUC: 0.9212\n",
            "AP : 0.9439\n",
            "\n",
            "Saved plots to: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251229_070439\n",
            "\n",
            "DONE âœ…\n",
            "Key outputs:\n",
            " - train_manifest_normalized.csv / test_manifest_normalized.csv\n",
            " - leakage_checks_norm.json\n",
            " - pca_metrics.json\n",
            " - auc_comparison.png, pca_score_hist.png\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# RESTART-SAFE PIPELINE\n",
        "# - Loads videos from your folders\n",
        "# - Removes dupes/near-dupes BEFORE split (group-safe)\n",
        "# - Normalizes videos to metadata-neutral format (ffmpeg cache)\n",
        "# - Runs leakage checks again after normalization\n",
        "# - Runs PCA detector on the SAME split\n",
        "# - Saves CSVs + plots into run folder\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm matplotlib\n",
        "\n",
        "import os, cv2, hashlib, random, json, subprocess, shutil, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG (your paths)\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "OUT_ROOT    = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "\n",
        "RUN_NAME = \"DATAFIX_NEUTRAL_PCA_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Run dir:\", RUN_DIR)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Helpers\n",
        "# -----------------------------\n",
        "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\")\n",
        "\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(VIDEO_EXTS):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return sorted(vids)\n",
        "\n",
        "def safe_mkdir(p):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if (not ok) or frame is None:\n",
        "        return None\n",
        "    frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "    return frame\n",
        "\n",
        "def sha1_first_frame(path, size=(128,128)):\n",
        "    frame = get_first_frame(path, size=size)\n",
        "    if frame is None:\n",
        "        return None\n",
        "    return hashlib.sha1(frame.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    frame = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if frame is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:, 1:] > gray[:, :-1]\n",
        "    bits = diff.flatten()\n",
        "    h = 0\n",
        "    for b in bits:\n",
        "        h = (h << 1) | int(b)\n",
        "    return h  # 64-bit when hash_size=8\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "def ffmpeg_exists():\n",
        "    try:\n",
        "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "assert ffmpeg_exists(), \"ffmpeg not found in this environment.\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load videos\n",
        "# -----------------------------\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "assert len(real) > 0 and (len(ai_core)+len(sora)) > 0, \"Dataset paths incorrect.\"\n",
        "\n",
        "# Label convention: AI=1, REAL=0\n",
        "AI_ALL = [(p, 1, \"AI_CORE\") for p in ai_core] + [(p, 1, \"SORA\") for p in sora]\n",
        "REAL_ALL = [(p, 0, \"REAL\") for p in real]\n",
        "\n",
        "# Balance by class count (you can change N if you want smaller)\n",
        "N = min(len(AI_ALL), len(REAL_ALL))\n",
        "AI_ALL   = random.sample(AI_ALL, N)\n",
        "REAL_ALL = random.sample(REAL_ALL, N)\n",
        "\n",
        "ALL = AI_ALL + REAL_ALL\n",
        "random.shuffle(ALL)\n",
        "\n",
        "print(f\"\\nBalanced pool: {len(ALL)} total | AI={N} REAL={N}\")\n",
        "\n",
        "# Save raw manifest\n",
        "pd.DataFrame(ALL, columns=[\"path\",\"y\",\"source\"]).to_csv(os.path.join(RUN_DIR, \"manifest_raw_balanced.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Dedup / Near-dedup CLUSTERING before split\n",
        "#    Goal: ensure no near-dup crosses splits.\n",
        "#    Strategy:\n",
        "#      - compute SHA1(first frame) for exact dupes\n",
        "#      - compute dHash for near dupes\n",
        "#      - bucket by top bits of dHash to avoid O(n^2)\n",
        "#      - build clusters where hamming <= 3 inside buckets\n",
        "# -----------------------------\n",
        "MAX_HAMMING = 3\n",
        "BUCKET_BITS = 16  # bucket by top 16 bits of 64-bit dhash\n",
        "BUCKET_SHIFT = 64 - BUCKET_BITS\n",
        "\n",
        "print(\"\\nComputing hashes (this may take a few minutes)...\")\n",
        "rows = []\n",
        "for (p, y, src) in tqdm(ALL, desc=\"Hashing\"):\n",
        "    sha = sha1_first_frame(p, size=(128,128))\n",
        "    dh  = dhash_first_frame(p, hash_size=8)\n",
        "    rows.append((p, y, src, sha, dh))\n",
        "\n",
        "dfh = pd.DataFrame(rows, columns=[\"path\",\"y\",\"source\",\"sha1\",\"dhash\"])\n",
        "dfh.to_csv(os.path.join(RUN_DIR, \"hashes_raw.csv\"), index=False)\n",
        "\n",
        "# 3a) Exact duplicates by sha1: keep one per sha1\n",
        "dfh = dfh.dropna(subset=[\"sha1\",\"dhash\"]).copy()\n",
        "dfh[\"sha_group\"] = dfh[\"sha1\"]\n",
        "dfh[\"keep_exact\"] = ~dfh.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
        "df_exact_removed = dfh[~dfh[\"keep_exact\"]].copy()\n",
        "dfh = dfh[dfh[\"keep_exact\"]].copy()\n",
        "\n",
        "print(\"Exact dupes removed:\", len(df_exact_removed))\n",
        "df_exact_removed.to_csv(os.path.join(RUN_DIR, \"removed_exact_dupes.csv\"), index=False)\n",
        "\n",
        "# 3b) Near-duplicates via dHash within buckets\n",
        "dfh[\"bucket\"] = (dfh[\"dhash\"].astype(np.uint64) >> np.uint64(BUCKET_SHIFT)).astype(np.uint64)\n",
        "\n",
        "# Build adjacency (union-find) only within bucket\n",
        "parent = list(range(len(dfh)))\n",
        "\n",
        "def find(i):\n",
        "    while parent[i] != i:\n",
        "        parent[i] = parent[parent[i]]\n",
        "        i = parent[i]\n",
        "    return i\n",
        "\n",
        "def union(a, b):\n",
        "    ra, rb = find(a), find(b)\n",
        "    if ra != rb:\n",
        "        parent[rb] = ra\n",
        "\n",
        "# index mapping\n",
        "idxs_by_bucket = defaultdict(list)\n",
        "for i, b in enumerate(dfh[\"bucket\"].tolist()):\n",
        "    idxs_by_bucket[int(b)].append(i)\n",
        "\n",
        "print(\"\\nClustering near-dupes within buckets...\")\n",
        "for b, idxs in tqdm(idxs_by_bucket.items(), desc=\"Buckets\"):\n",
        "    if len(idxs) <= 1:\n",
        "        continue\n",
        "    # compare all pairs inside bucket (buckets are usually small)\n",
        "    dhashes = dfh.iloc[idxs][\"dhash\"].astype(np.uint64).tolist()\n",
        "    for ii in range(len(idxs)):\n",
        "        for jj in range(ii+1, len(idxs)):\n",
        "            a = dhashes[ii]\n",
        "            c = dhashes[jj]\n",
        "            if hamming(int(a), int(c)) <= MAX_HAMMING:\n",
        "                union(idxs[ii], idxs[jj])\n",
        "\n",
        "# Assign cluster IDs\n",
        "roots = [find(i) for i in range(len(dfh))]\n",
        "# compress to 0..K-1\n",
        "root_to_cluster = {}\n",
        "cluster_ids = []\n",
        "next_id = 0\n",
        "for r in roots:\n",
        "    if r not in root_to_cluster:\n",
        "        root_to_cluster[r] = next_id\n",
        "        next_id += 1\n",
        "    cluster_ids.append(root_to_cluster[r])\n",
        "\n",
        "dfh[\"cluster\"] = cluster_ids\n",
        "\n",
        "# For each cluster, keep one representative (first row)\n",
        "dfh[\"keep_near\"] = ~dfh.duplicated(subset=[\"cluster\"], keep=\"first\")\n",
        "df_near_removed = dfh[~dfh[\"keep_near\"]].copy()\n",
        "dfh = dfh[dfh[\"keep_near\"]].copy()\n",
        "\n",
        "print(\"Near-dupes removed:\", len(df_near_removed))\n",
        "df_near_removed.to_csv(os.path.join(RUN_DIR, \"removed_near_dupes.csv\"), index=False)\n",
        "\n",
        "print(\"\\nAfter dedup:\", len(dfh), \"videos\")\n",
        "\n",
        "# Save dedup manifest\n",
        "dfh[[\"path\",\"y\",\"source\",\"sha1\",\"dhash\",\"cluster\"]].to_csv(os.path.join(RUN_DIR, \"manifest_dedup.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Group-safe Train/Test split (no cluster overlap)\n",
        "#    We do class-stratified splitting by clusters:\n",
        "#    - split AI clusters and REAL clusters separately to preserve balance\n",
        "# -----------------------------\n",
        "TEST_FRAC = 0.2\n",
        "\n",
        "def group_split(df_label, test_frac=0.2):\n",
        "    # df_label has columns: path,y,cluster\n",
        "    clusters = df_label[\"cluster\"].unique().tolist()\n",
        "    random.shuffle(clusters)\n",
        "    test_clusters = set(clusters[:max(1, int(len(clusters)*test_frac))])\n",
        "    test_df  = df_label[df_label[\"cluster\"].isin(test_clusters)].copy()\n",
        "    train_df = df_label[~df_label[\"cluster\"].isin(test_clusters)].copy()\n",
        "    return train_df, test_df\n",
        "\n",
        "df_ai   = dfh[dfh[\"y\"] == 1].copy()\n",
        "df_real = dfh[dfh[\"y\"] == 0].copy()\n",
        "\n",
        "train_ai, test_ai     = group_split(df_ai, test_frac=TEST_FRAC)\n",
        "train_real, test_real = group_split(df_real, test_frac=TEST_FRAC)\n",
        "\n",
        "train_df = pd.concat([train_ai, train_real], ignore_index=True)\n",
        "test_df  = pd.concat([test_ai, test_real], ignore_index=True)\n",
        "\n",
        "# Shuffle within splits\n",
        "train_df = train_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "test_df  = test_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSplit sizes (group-safe):\")\n",
        "print(\"Train:\", len(train_df), \" | Test:\", len(test_df))\n",
        "print(\"Train AI/REAL:\", int(train_df.y.sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\"Test  AI/REAL:\", int(test_df.y.sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "# Verify no overlap in clusters\n",
        "assert set(train_df.cluster).isdisjoint(set(test_df.cluster)), \"Cluster overlap detected (should never happen).\"\n",
        "\n",
        "train_df.to_csv(os.path.join(RUN_DIR, \"train_manifest.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(RUN_DIR, \"test_manifest.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) METADATA-NEUTRAL NORMALIZATION (Fix the data issue)\n",
        "#    - re-encode everything to SAME: fps, size, duration, codec\n",
        "#    - remove audio\n",
        "#    This kills trivial source cues (container/fps/resolution/bitrate etc.)\n",
        "# -----------------------------\n",
        "NORM_DIR = os.path.join(RUN_DIR, \"normalized_cache\")\n",
        "safe_mkdir(NORM_DIR)\n",
        "\n",
        "TARGET_FPS = 16\n",
        "TARGET_SIZE = 224  # works for ViT/TimeSformer if you later reuse\n",
        "TARGET_SEC = 4     # fixed duration\n",
        "CRF = 23           # constant quality (still content-dependent size, but much less cue)\n",
        "PRESET = \"veryfast\"\n",
        "\n",
        "def norm_out_path(src_path):\n",
        "    # stable name from sha1 of original path string (not frame)\n",
        "    hid = hashlib.sha1(src_path.encode(\"utf-8\")).hexdigest()[:16]\n",
        "    return os.path.join(NORM_DIR, hid + \".mp4\")\n",
        "\n",
        "def normalize_video(in_path, out_path):\n",
        "    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:\n",
        "        return True\n",
        "    # -t fixed seconds, -an remove audio, scale+pad to fixed square, fps fixed\n",
        "    vf = (\n",
        "        f\"fps={TARGET_FPS},\"\n",
        "        f\"scale={TARGET_SIZE}:{TARGET_SIZE}:force_original_aspect_ratio=decrease,\"\n",
        "        f\"pad={TARGET_SIZE}:{TARGET_SIZE}:(ow-iw)/2:(oh-ih)/2\"\n",
        "    )\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-t\", str(TARGET_SEC),\n",
        "        \"-vf\", vf,\n",
        "        \"-an\",\n",
        "        \"-c:v\", \"libx264\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        \"-preset\", PRESET,\n",
        "        \"-crf\", str(CRF),\n",
        "        out_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "        return os.path.exists(out_path) and os.path.getsize(out_path) > 1000\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "print(\"\\nNormalizing TRAIN videos...\")\n",
        "train_norm = []\n",
        "for p, y, src in tqdm(train_df[[\"path\",\"y\",\"source\"]].values.tolist(), desc=\"Normalize train\"):\n",
        "    outp = norm_out_path(p)\n",
        "    ok = normalize_video(p, outp)\n",
        "    if ok:\n",
        "        train_norm.append((outp, int(y), src))\n",
        "print(\"Train normalized:\", len(train_norm), \"/\", len(train_df))\n",
        "\n",
        "print(\"\\nNormalizing TEST videos...\")\n",
        "test_norm = []\n",
        "for p, y, src in tqdm(test_df[[\"path\",\"y\",\"source\"]].values.tolist(), desc=\"Normalize test\"):\n",
        "    outp = norm_out_path(p)\n",
        "    ok = normalize_video(p, outp)\n",
        "    if ok:\n",
        "        test_norm.append((outp, int(y), src))\n",
        "print(\"Test normalized:\", len(test_norm), \"/\", len(test_df))\n",
        "\n",
        "# Save normalized manifests\n",
        "pd.DataFrame(train_norm, columns=[\"path\",\"y\",\"source\"]).to_csv(os.path.join(RUN_DIR, \"train_manifest_normalized.csv\"), index=False)\n",
        "pd.DataFrame(test_norm,  columns=[\"path\",\"y\",\"source\"]).to_csv(os.path.join(RUN_DIR, \"test_manifest_normalized.csv\"), index=False)\n",
        "\n",
        "assert len(train_norm) > 0 and len(test_norm) > 0, \"Normalization failed (no videos produced).\"\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Leakage checks AGAIN on normalized split\n",
        "#    CHECK 1: exact dupes by SHA1(first frame)\n",
        "#    CHECK 2: near dupes by dHash(first frame)\n",
        "#    CHECK 3: metadata-only baseline (should drop a lot)\n",
        "# -----------------------------\n",
        "def build_sha_map(samples, split_name):\n",
        "    hmap = defaultdict(list)\n",
        "    for p, y, src in tqdm(samples, desc=f\"SHA1 {split_name}\"):\n",
        "        h = sha1_first_frame(p, size=(128,128))\n",
        "        if h:\n",
        "            hmap[h].append((split_name, y, src, p))\n",
        "    return hmap\n",
        "\n",
        "train_sha = build_sha_map(train_norm, \"train_norm\")\n",
        "test_sha  = build_sha_map(test_norm,  \"test_norm\")\n",
        "\n",
        "all_sha = defaultdict(list)\n",
        "for d in [train_sha, test_sha]:\n",
        "    for k,v in d.items():\n",
        "        all_sha[k].extend(v)\n",
        "\n",
        "exact_cross = {k:v for k,v in all_sha.items() if len(set(x[0] for x in v)) > 1}\n",
        "print(\"\\n[NORM CHECK 1] Exact dupes across splits:\", len(exact_cross))\n",
        "\n",
        "def build_dhash_list(samples, split_name):\n",
        "    out = []\n",
        "    for p, y, src in tqdm(samples, desc=f\"dHash {split_name}\"):\n",
        "        h = dhash_first_frame(p, hash_size=8)\n",
        "        if h is not None:\n",
        "            out.append((split_name, y, src, p, int(h)))\n",
        "    return out\n",
        "\n",
        "train_d = build_dhash_list(train_norm, \"train_norm\")\n",
        "test_d  = build_dhash_list(test_norm,  \"test_norm\")\n",
        "\n",
        "# Near-dup count (limited comparisons for speed)\n",
        "MAX_PAIRS_PRINT = 10\n",
        "near_hits = []\n",
        "for a in train_d:\n",
        "    for b in test_d:\n",
        "        dist = hamming(a[4], b[4])\n",
        "        if dist <= MAX_HAMMING:\n",
        "            near_hits.append((dist, a, b))\n",
        "            if len(near_hits) >= 50:\n",
        "                break\n",
        "    if len(near_hits) >= 50:\n",
        "        break\n",
        "\n",
        "print(f\"[NORM CHECK 2] Near-dupe pairs found (first 50 search) H<={MAX_HAMMING}:\", len(near_hits))\n",
        "pd.DataFrame(\n",
        "    [{\n",
        "        \"hamming\": d,\n",
        "        \"a_split\": a[0], \"a_y\": a[1], \"a_src\": a[2], \"a_path\": a[3],\n",
        "        \"b_split\": b[0], \"b_y\": b[1], \"b_src\": b[2], \"b_path\": b[3],\n",
        "    } for (d,a,b) in near_hits]\n",
        ").to_csv(os.path.join(RUN_DIR, \"near_dupes_norm_sample.csv\"), index=False)\n",
        "\n",
        "# Metadata-only baseline\n",
        "def video_metadata(path):\n",
        "    size = os.path.getsize(path) if os.path.exists(path) else np.nan\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return [size, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
        "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "    dur = frames / fps if (fps and fps > 0 and frames and frames > 0) else np.nan\n",
        "    return [size, w, h, fps, frames, dur]\n",
        "\n",
        "def build_meta_XY(samples):\n",
        "    X, Y = [], []\n",
        "    for p, y, src in tqdm(samples, desc=\"Metadata\"):\n",
        "        X.append(video_metadata(p))\n",
        "        Y.append(y)\n",
        "    X = np.array(X, dtype=np.float64)\n",
        "    Y = np.array(Y, dtype=np.int64)\n",
        "    # fill NaNs by train medians later\n",
        "    return X, Y\n",
        "\n",
        "Xtr, Ytr = build_meta_XY(train_norm)\n",
        "Xte, Yte = build_meta_XY(test_norm)\n",
        "\n",
        "# fill NaNs with train medians\n",
        "for j in range(Xtr.shape[1]):\n",
        "    med = np.nanmedian(Xtr[:, j])\n",
        "    Xtr[:, j] = np.nan_to_num(Xtr[:, j], nan=med)\n",
        "    Xte[:, j] = np.nan_to_num(Xte[:, j], nan=med)\n",
        "\n",
        "meta_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "meta_clf.fit(Xtr, Ytr)\n",
        "proba = meta_clf.predict_proba(Xte)[:,1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "meta_acc = accuracy_score(Yte, pred)\n",
        "meta_f1  = f1_score(Yte, pred)\n",
        "meta_auc = roc_auc_score(Yte, proba)\n",
        "\n",
        "print(\"\\n[NORM CHECK 3] Metadata-only baseline (should be MUCH LOWER than before)\")\n",
        "print(\"ACC:\", round(meta_acc, 4), \"F1:\", round(meta_f1, 4), \"AUC:\", round(meta_auc, 4))\n",
        "\n",
        "with open(os.path.join(RUN_DIR, \"leakage_checks_norm.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"norm_check1_exact_dupes\": int(len(exact_cross)),\n",
        "        \"norm_check2_near_dupes_found_in_sample_search\": int(len(near_hits)),\n",
        "        \"norm_check3_metadata_acc\": float(meta_acc),\n",
        "        \"norm_check3_metadata_f1\": float(meta_f1),\n",
        "        \"norm_check3_metadata_auc\": float(meta_auc),\n",
        "        \"params\": {\n",
        "            \"MAX_HAMMING\": MAX_HAMMING,\n",
        "            \"TARGET_FPS\": TARGET_FPS,\n",
        "            \"TARGET_SIZE\": TARGET_SIZE,\n",
        "            \"TARGET_SEC\": TARGET_SEC,\n",
        "            \"CRF\": CRF\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "\n",
        "# -----------------------------\n",
        "# 7) PCA DETECTOR (your extra method)\n",
        "#    \"Temporal Gradient-PCA\":\n",
        "#      - sample K frames uniformly from normalized clip\n",
        "#      - for each frame:\n",
        "#          grayscale -> Sobel gradient magnitude -> downsample to 32x32\n",
        "#      - stack/average across frames -> feature vector\n",
        "#      - PCA (fit on train) -> Logistic Regression\n",
        "# -----------------------------\n",
        "K_FRAMES = 8\n",
        "FEAT_SIZE = 32  # 32x32 per frame\n",
        "\n",
        "def sample_frames_uniform(path, k=8):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return []\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
        "    if n <= 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    idxs = np.linspace(0, max(0, n-1), num=k).astype(int).tolist()\n",
        "    frames = []\n",
        "    for t in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
        "        ok, frame = cap.read()\n",
        "        if ok and frame is not None:\n",
        "            frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def grad_mag_feature(frame_bgr, out_hw=32):\n",
        "    g = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    # Sobel gradients\n",
        "    gx = cv2.Sobel(g, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(g, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    mag = cv2.magnitude(gx, gy)\n",
        "    mag = cv2.resize(mag, (out_hw, out_hw), interpolation=cv2.INTER_AREA)\n",
        "    # normalize per-frame to reduce intensity scale effects\n",
        "    mag = (mag - mag.mean()) / (mag.std() + 1e-6)\n",
        "    return mag.astype(np.float32).reshape(-1)\n",
        "\n",
        "def pca_video_descriptor(path, k_frames=8, out_hw=32):\n",
        "    frames = sample_frames_uniform(path, k=k_frames)\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "    feats = [grad_mag_feature(fr, out_hw=out_hw) for fr in frames]\n",
        "    # average across time (simple but effective)\n",
        "    v = np.mean(np.stack(feats, axis=0), axis=0)\n",
        "    return v\n",
        "\n",
        "def build_pca_dataset(samples, desc=\"PCA feats\"):\n",
        "    X, Y = [], []\n",
        "    bad = 0\n",
        "    for p, y, src in tqdm(samples, desc=desc):\n",
        "        v = pca_video_descriptor(p, k_frames=K_FRAMES, out_hw=FEAT_SIZE)\n",
        "        if v is None:\n",
        "            bad += 1\n",
        "            continue\n",
        "        X.append(v)\n",
        "        Y.append(y)\n",
        "    X = np.stack(X, axis=0).astype(np.float32) if len(X) else np.zeros((0, FEAT_SIZE*FEAT_SIZE), dtype=np.float32)\n",
        "    Y = np.array(Y, dtype=np.int64)\n",
        "    return X, Y, bad\n",
        "\n",
        "Xtr_pca, Ytr_pca, bad_tr = build_pca_dataset(train_norm, desc=\"PCA train feats\")\n",
        "Xte_pca, Yte_pca, bad_te = build_pca_dataset(test_norm,  desc=\"PCA test feats\")\n",
        "\n",
        "print(\"\\nPCA feat extraction done.\")\n",
        "print(\"Train feats:\", Xtr_pca.shape, \"bad:\", bad_tr)\n",
        "print(\"Test  feats:\", Xte_pca.shape, \"bad:\", bad_te)\n",
        "assert Xtr_pca.shape[0] > 10 and Xte_pca.shape[0] > 10, \"Too many failed videos for PCA extraction.\"\n",
        "\n",
        "# PCA + LR\n",
        "PCA_COMPONENTS = 128\n",
        "\n",
        "pca_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"pca\", PCA(n_components=PCA_COMPONENTS, random_state=SEED)),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "pca_clf.fit(Xtr_pca, Ytr_pca)\n",
        "proba = pca_clf.predict_proba(Xte_pca)[:, 1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "pca_acc = accuracy_score(Yte_pca, pred)\n",
        "pca_f1  = f1_score(Yte_pca, pred)\n",
        "pca_auc = roc_auc_score(Yte_pca, proba)\n",
        "pca_ap  = average_precision_score(Yte_pca, proba)\n",
        "\n",
        "print(\"\\n=== PCA DETECTOR (Temporal Gradient-PCA) ===\")\n",
        "print(\"ACC:\", round(pca_acc, 4))\n",
        "print(\"F1 :\", round(pca_f1, 4))\n",
        "print(\"AUC:\", round(pca_auc, 4))\n",
        "print(\"AP :\", round(pca_ap, 4))\n",
        "\n",
        "# Save PCA metrics\n",
        "with open(os.path.join(RUN_DIR, \"pca_metrics.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"acc\": float(pca_acc),\n",
        "        \"f1\": float(pca_f1),\n",
        "        \"auc\": float(pca_auc),\n",
        "        \"ap\": float(pca_ap),\n",
        "        \"params\": {\n",
        "            \"K_FRAMES\": K_FRAMES,\n",
        "            \"FEAT_SIZE\": FEAT_SIZE,\n",
        "            \"PCA_COMPONENTS\": PCA_COMPONENTS\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Simple plots\n",
        "# -----------------------------\n",
        "# Plot 1: Metadata-only baseline vs PCA baseline (AUC)\n",
        "labels = [\"Metadata-only\\n(after neutralization)\", \"PCA detector\\n(grad-temporal)\"]\n",
        "aucs   = [meta_auc, pca_auc]\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(labels, aucs)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.title(\"AUC comparison (metadata-neutral split)\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RUN_DIR, \"auc_comparison.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# Plot 2: PCA score histogram\n",
        "plt.figure()\n",
        "plt.hist(proba[Yte_pca==0], bins=25, alpha=0.6, label=\"REAL (0)\")\n",
        "plt.hist(proba[Yte_pca==1], bins=25, alpha=0.6, label=\"AI (1)\")\n",
        "plt.title(\"PCA detector score distribution\")\n",
        "plt.xlabel(\"Predicted P(AI)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RUN_DIR, \"pca_score_hist.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved plots to:\", RUN_DIR)\n",
        "\n",
        "print(\"\\nDONE âœ…\")\n",
        "print(\"Key outputs:\")\n",
        "print(\" - train_manifest_normalized.csv / test_manifest_normalized.csv\")\n",
        "print(\" - leakage_checks_norm.json\")\n",
        "print(\" - pca_metrics.json\")\n",
        "print(\" - auc_comparison.png, pca_score_hist.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TINY FUSION EXPERIMENT (fixed & OOM-safe)\n",
        "# Uses normalized manifests from your DATAFIX_NEUTRAL_PCA run\n",
        "#\n",
        "# Streams:\n",
        "#   mid, high, grad, freq\n",
        "#   fusion_mhfg = (mid + high + grad + freq)/4   <-- what you asked for\n",
        "#\n",
        "# Models:\n",
        "#   1) R(2+1)D + ResNet18 fusion\n",
        "#   2) ViT-B/16 + ResNet18 fusion\n",
        "#\n",
        "# Each model trains on a tiny subset (e.g., 21) and tests on tiny (e.g., 9).\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install torch torchvision opencv-python-headless tqdm scikit-learn pandas\n",
        "\n",
        "import os, random, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# -----------------------------\n",
        "# Repro + device\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -----------------------------\n",
        "# Point this to YOUR successful run folder\n",
        "# -----------------------------\n",
        "RUN_DIR = \"/content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251229_070439\"\n",
        "\n",
        "TRAIN_NORM_CSV = os.path.join(RUN_DIR, \"train_manifest_normalized.csv\")\n",
        "TEST_NORM_CSV  = os.path.join(RUN_DIR, \"test_manifest_normalized.csv\")\n",
        "\n",
        "assert os.path.exists(TRAIN_NORM_CSV), f\"Missing: {TRAIN_NORM_CSV}\"\n",
        "assert os.path.exists(TEST_NORM_CSV),  f\"Missing: {TEST_NORM_CSV}\"\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_NORM_CSV)\n",
        "test_df  = pd.read_csv(TEST_NORM_CSV)\n",
        "\n",
        "# Normalize labels to int\n",
        "train_df[\"y\"] = train_df[\"y\"].astype(int)\n",
        "test_df[\"y\"]  = test_df[\"y\"].astype(int)\n",
        "\n",
        "print(\"Normalized train:\", len(train_df), \"test:\", len(test_df))\n",
        "print(\"Train AI/REAL:\", int(train_df.y.sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\"Test  AI/REAL:\", int(test_df.y.sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "# -----------------------------\n",
        "# Tiny selection: 10 AI + 10 REAL for train; 5 AI + 5 REAL for test (edit as you like)\n",
        "# -----------------------------\n",
        "TRAIN_PER_CLASS = 10\n",
        "TEST_PER_CLASS  = 5\n",
        "\n",
        "def pick_balanced(df, n_per_class):\n",
        "    ai = df[df[\"y\"]==1].sample(n=n_per_class, random_state=SEED)\n",
        "    re = df[df[\"y\"]==0].sample(n=n_per_class, random_state=SEED)\n",
        "    out = pd.concat([ai, re]).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "tiny_train = pick_balanced(train_df, TRAIN_PER_CLASS)\n",
        "tiny_test  = pick_balanced(test_df,  TEST_PER_CLASS)\n",
        "\n",
        "train_samples = list(zip(tiny_train[\"path\"].tolist(), tiny_train[\"y\"].tolist()))\n",
        "test_samples  = list(zip(tiny_test[\"path\"].tolist(),  tiny_test[\"y\"].tolist()))\n",
        "\n",
        "print(\"\\nTiny split:\")\n",
        "print(\" Train:\", len(train_samples), \" Test:\", len(test_samples))\n",
        "\n",
        "# ============================================================\n",
        "# OpenCV loader (fixed shape) -> returns [C,T,H,W] float32\n",
        "# ============================================================\n",
        "def load_video_clip_opencv(path, num_frames=8, size=224):\n",
        "    \"\"\"\n",
        "    Samples num_frames uniformly from the clip.\n",
        "    Returns clip in [C,T,H,W], float32 in [0,1], RGB.\n",
        "    \"\"\"\n",
        "    H = W = int(size)\n",
        "    if (not os.path.exists(path)) or os.path.getsize(path) < 1000:\n",
        "        return torch.zeros(3, num_frames, H, W, dtype=torch.float32)\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W, dtype=torch.float32)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W, dtype=torch.float32)\n",
        "\n",
        "    idxs = np.linspace(0, max(0, frame_count - 1), num_frames).astype(int).tolist()\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "        ft = torch.from_numpy(frame_rgb).permute(2,0,1).float() / 255.0\n",
        "        frames.append(ft)\n",
        "\n",
        "    cap.release()\n",
        "    clip = torch.stack(frames, dim=1)  # [C,T,H,W]\n",
        "    return clip\n",
        "\n",
        "# ============================================================\n",
        "# Feature transforms: MID / HIGH / GRAD / FREQ\n",
        "# (work on [C,T,H,W] in [0,1])\n",
        "# ============================================================\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645]).view(3, 1, 1, 1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989]).view(3, 1, 1, 1)\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "def midtone_mask(frames, low=0.3, high=0.7):\n",
        "    r, g, b = frames[0], frames[1], frames[2]\n",
        "    lum = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    lum = lum.unsqueeze(0)\n",
        "    mask = ((lum >= low) & (lum <= high)).float()\n",
        "    return frames * mask\n",
        "\n",
        "def high_frequency_residual(frames, ksize=7):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t].permute(1,2,0).cpu().numpy()\n",
        "        blur = cv2.GaussianBlur(frame, (ksize, ksize), 0)\n",
        "        residual = frame - blur\n",
        "        residual = np.clip(residual * 2.0 + 0.5, 0.0, 1.0)\n",
        "        out[:, t] = torch.from_numpy(residual).permute(2,0,1)\n",
        "    return out\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        gray = (0.299*frame[0] + 0.587*frame[1] + 0.114*frame[2]).cpu().numpy().astype(np.float32)\n",
        "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        mag = mag / (mag.max() + 1e-6) if mag.max() > 0 else mag\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "def frequency_spectrum(frames):\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        gray = (0.299*frame[0] + 0.587*frame[1] + 0.114*frame[2]).cpu().numpy().astype(np.float32)\n",
        "        fft = np.fft.fft2(gray)\n",
        "        fft_shift = np.fft.fftshift(fft)\n",
        "        mag = np.log1p(np.abs(fft_shift))\n",
        "        mag = mag / (mag.max() + 1e-6) if mag.max() > 0 else mag\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "# ============================================================\n",
        "# Dataset returning requested streams + fusion_mhfg\n",
        "# ============================================================\n",
        "class MultiStreamTinyDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=224):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames01 = load_video_clip_opencv(path, num_frames=self.num_frames, size=self.size)  # [C,T,H,W] in [0,1]\n",
        "\n",
        "        mid  = normalize_kinetics(midtone_mask(frames01.clone()))\n",
        "        high = normalize_kinetics(high_frequency_residual(frames01.clone()))\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames01.clone()))\n",
        "        freq = normalize_kinetics(frequency_spectrum(frames01.clone()))\n",
        "\n",
        "        fusion_mhfg = (mid + high + grad + freq) / 4.0\n",
        "\n",
        "        streams = {\n",
        "            \"mid\": mid,\n",
        "            \"high\": high,\n",
        "            \"grad\": grad,\n",
        "            \"freq\": freq,\n",
        "            \"fusion_mhfg\": fusion_mhfg,   # <-- your requested fusion\n",
        "        }\n",
        "        return streams, torch.tensor(int(label), dtype=torch.long)\n",
        "\n",
        "# Dataloaders (tiny)\n",
        "train_loader = DataLoader(MultiStreamTinyDataset(train_samples, num_frames=8, size=224),\n",
        "                          batch_size=1, shuffle=True, num_workers=0, pin_memory=(device.type==\"cuda\"))\n",
        "test_loader  = DataLoader(MultiStreamTinyDataset(test_samples, num_frames=8, size=224),\n",
        "                          batch_size=1, shuffle=False, num_workers=0, pin_memory=(device.type==\"cuda\"))\n",
        "\n",
        "# ============================================================\n",
        "# Backbones: R(2+1)D, ViT, ResNet\n",
        "# (all frozen; only fc trained)\n",
        "# ============================================================\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def make_r2plus1d_backbone():\n",
        "    weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "    model = r2plus1d_18(weights=weights)\n",
        "    feat_dim = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, feat_dim\n",
        "\n",
        "def make_vit_backbone():\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    vit = vit_b_16(weights=weights)\n",
        "    feat_dim = vit.heads.head.in_features\n",
        "    vit.heads.head = nn.Identity()\n",
        "    return vit, feat_dim\n",
        "\n",
        "def make_resnet_backbone():\n",
        "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
        "    net = resnet18(weights=weights)\n",
        "    feat_dim = net.fc.in_features\n",
        "    net.fc = nn.Identity()\n",
        "    return net, feat_dim\n",
        "\n",
        "# ============================================================\n",
        "# Fusion models\n",
        "# ============================================================\n",
        "class R2PlusResNetFusion(nn.Module):\n",
        "    def __init__(self, stream_key=\"mid\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.r2, d_r2 = make_r2plus1d_backbone()\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "\n",
        "        for p in self.r2.parameters(): p.requires_grad = False\n",
        "        for p in self.resnet.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_r2 + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        x = streams[self.stream_key]  # [B,C,T,H,W]\n",
        "\n",
        "        r2_feats = self.r2(x)  # [B, d_r2]\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "        frames = x.permute(0,2,1,3,4).contiguous().view(B*T, C, H, W)\n",
        "\n",
        "        res_feats = self.resnet(frames)  # [B*T, d_res]\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        fused = torch.cat([r2_feats, res_feats], dim=1)\n",
        "        return self.fc(fused)\n",
        "\n",
        "class ViTPlusResNetFusion(nn.Module):\n",
        "    def __init__(self, stream_key=\"mid\", num_classes=2):\n",
        "        super().__init__()\n",
        "        self.stream_key = stream_key\n",
        "\n",
        "        self.vit, d_vit = make_vit_backbone()\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "\n",
        "        for p in self.vit.parameters(): p.requires_grad = False\n",
        "        for p in self.resnet.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(d_vit + d_res, num_classes)\n",
        "\n",
        "    def forward(self, streams):\n",
        "        x = streams[self.stream_key]  # [B,C,T,H,W]\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        frames = x.permute(0,2,1,3,4).contiguous().view(B*T, C, H, W)\n",
        "\n",
        "        vit_feats = self.vit(frames)          # [B*T, d_vit]\n",
        "        vit_feats = vit_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        res_feats = self.resnet(frames)       # [B*T, d_res]\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        fused = torch.cat([vit_feats, res_feats], dim=1)\n",
        "        return self.fc(fused)\n",
        "\n",
        "# ============================================================\n",
        "# Train / eval (fp16 autocast + inference_mode to prevent OOM)\n",
        "# ============================================================\n",
        "def train_model(model, train_loader, epochs=3, lr=1e-4, tag=\"\"):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = optim.Adam(model.fc.parameters(), lr=lr)\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "        for streams, labels in train_loader:\n",
        "            for k in streams:\n",
        "                streams[k] = streams[k].to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                    out = model(streams)\n",
        "                    loss = crit(out, labels)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            else:\n",
        "                out = model(streams)\n",
        "                loss = crit(out, labels)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            loss_sum += float(loss.item())\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += int((pred == labels).sum().item())\n",
        "            total += int(labels.size(0))\n",
        "\n",
        "        print(f\"[{tag}] Epoch {ep+1}/{epochs} | loss={loss_sum/max(total,1):.4f} acc={correct/max(total,1):.3f}\")\n",
        "\n",
        "def eval_model(model, test_loader, tag=\"\"):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for streams, labels in test_loader:\n",
        "            for k in streams:\n",
        "                streams[k] = streams[k].to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            if device.type == \"cuda\":\n",
        "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                    out = model(streams)\n",
        "            else:\n",
        "                out = model(streams)\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            all_y.extend(labels.cpu().numpy().tolist())\n",
        "            all_p.extend(pred.cpu().numpy().tolist())\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({tag}) ===\")\n",
        "    print(classification_report(all_y, all_p, digits=4, target_names=[\"REAL (0)\", \"AI (1)\"]))\n",
        "    acc = accuracy_score(all_y, all_p)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    return acc\n",
        "\n",
        "# ============================================================\n",
        "# Run experiments\n",
        "# ============================================================\n",
        "STREAMS = [\"mid\", \"high\", \"grad\", \"freq\", \"fusion_mhfg\"]\n",
        "\n",
        "def cleanup(model):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\")\n",
        "r2res_results = {}\n",
        "for sk in STREAMS:\n",
        "    print(f\"\\n--- R2+ResNet stream={sk} ---\")\n",
        "    model = R2PlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, lr=1e-4, tag=f\"R2Res-{sk}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"R2Res-{sk}\")\n",
        "    r2res_results[sk] = acc\n",
        "    cleanup(model)\n",
        "\n",
        "print(\"\\n=========== R2 + ResNet RESULTS ===========\")\n",
        "for k,v in r2res_results.items():\n",
        "    print(f\"{k:12s} | {v:.4f}\")\n",
        "\n",
        "print(\"\\n######## ViT + ResNet FUSION EXPERIMENTS ########\")\n",
        "vitres_results = {}\n",
        "for sk in STREAMS:\n",
        "    print(f\"\\n--- ViT+ResNet stream={sk} ---\")\n",
        "    model = ViTPlusResNetFusion(stream_key=sk, num_classes=2)\n",
        "    train_model(model, train_loader, epochs=3, lr=1e-4, tag=f\"ViTRes-{sk}\")\n",
        "    acc = eval_model(model, test_loader, tag=f\"ViTRes-{sk}\")\n",
        "    vitres_results[sk] = acc\n",
        "    cleanup(model)\n",
        "\n",
        "print(\"\\n=========== ViT + ResNet RESULTS ===========\")\n",
        "for k,v in vitres_results.items():\n",
        "    print(f\"{k:12s} | {v:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYAtgWSxDe5D",
        "outputId": "c6d821ae-6432-4503-9327-740740edff18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Normalized train: 1897 test: 473\n",
            "Train AI/REAL: 953 / 944\n",
            "Test  AI/REAL: 238 / 235\n",
            "\n",
            "Tiny split:\n",
            " Train: 20  Test: 10\n",
            "\n",
            "######## R(2+1)D + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- R2+ResNet stream=mid ---\n",
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:00<00:00, 208MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 177MB/s]\n",
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[R2Res-mid] Epoch 1/3 | loss=0.7552 acc=0.500\n",
            "[R2Res-mid] Epoch 2/3 | loss=0.6971 acc=0.450\n",
            "[R2Res-mid] Epoch 3/3 | loss=0.6969 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (R2Res-mid) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.6250    1.0000    0.7692         5\n",
            "      AI (1)     1.0000    0.4000    0.5714         5\n",
            "\n",
            "    accuracy                         0.7000        10\n",
            "   macro avg     0.8125    0.7000    0.6703        10\n",
            "weighted avg     0.8125    0.7000    0.6703        10\n",
            "\n",
            "Accuracy: 0.7\n",
            "\n",
            "--- R2+ResNet stream=high ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[R2Res-high] Epoch 1/3 | loss=0.7652 acc=0.350\n",
            "[R2Res-high] Epoch 2/3 | loss=0.7153 acc=0.250\n",
            "[R2Res-high] Epoch 3/3 | loss=0.7407 acc=0.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (R2Res-high) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.4000    0.4000    0.4000         5\n",
            "      AI (1)     0.4000    0.4000    0.4000         5\n",
            "\n",
            "    accuracy                         0.4000        10\n",
            "   macro avg     0.4000    0.4000    0.4000        10\n",
            "weighted avg     0.4000    0.4000    0.4000        10\n",
            "\n",
            "Accuracy: 0.4\n",
            "\n",
            "--- R2+ResNet stream=grad ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[R2Res-grad] Epoch 1/3 | loss=0.7475 acc=0.500\n",
            "[R2Res-grad] Epoch 2/3 | loss=0.7041 acc=0.400\n",
            "[R2Res-grad] Epoch 3/3 | loss=0.7198 acc=0.300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (R2Res-grad) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.5000    0.6000    0.5455         5\n",
            "      AI (1)     0.5000    0.4000    0.4444         5\n",
            "\n",
            "    accuracy                         0.5000        10\n",
            "   macro avg     0.5000    0.5000    0.4949        10\n",
            "weighted avg     0.5000    0.5000    0.4949        10\n",
            "\n",
            "Accuracy: 0.5\n",
            "\n",
            "--- R2+ResNet stream=freq ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[R2Res-freq] Epoch 1/3 | loss=0.7110 acc=0.450\n",
            "[R2Res-freq] Epoch 2/3 | loss=0.6970 acc=0.500\n",
            "[R2Res-freq] Epoch 3/3 | loss=0.6995 acc=0.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (R2Res-freq) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.0000    0.0000    0.0000         5\n",
            "      AI (1)     0.3750    0.6000    0.4615         5\n",
            "\n",
            "    accuracy                         0.3000        10\n",
            "   macro avg     0.1875    0.3000    0.2308        10\n",
            "weighted avg     0.1875    0.3000    0.2308        10\n",
            "\n",
            "Accuracy: 0.3\n",
            "\n",
            "--- R2+ResNet stream=fusion_mhfg ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[R2Res-fusion_mhfg] Epoch 1/3 | loss=0.7340 acc=0.350\n",
            "[R2Res-fusion_mhfg] Epoch 2/3 | loss=0.7028 acc=0.500\n",
            "[R2Res-fusion_mhfg] Epoch 3/3 | loss=0.7150 acc=0.350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (R2Res-fusion_mhfg) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.5000    0.4000    0.4444         5\n",
            "      AI (1)     0.5000    0.6000    0.5455         5\n",
            "\n",
            "    accuracy                         0.5000        10\n",
            "   macro avg     0.5000    0.5000    0.4949        10\n",
            "weighted avg     0.5000    0.5000    0.4949        10\n",
            "\n",
            "Accuracy: 0.5\n",
            "\n",
            "=========== R2 + ResNet RESULTS ===========\n",
            "mid          | 0.7000\n",
            "high         | 0.4000\n",
            "grad         | 0.5000\n",
            "freq         | 0.3000\n",
            "fusion_mhfg  | 0.5000\n",
            "\n",
            "######## ViT + ResNet FUSION EXPERIMENTS ########\n",
            "\n",
            "--- ViT+ResNet stream=mid ---\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330M/330M [00:01<00:00, 257MB/s]\n",
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViTRes-mid] Epoch 1/3 | loss=0.6761 acc=0.600\n",
            "[ViTRes-mid] Epoch 2/3 | loss=0.6008 acc=0.700\n",
            "[ViTRes-mid] Epoch 3/3 | loss=0.5617 acc=0.900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (ViTRes-mid) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.2000    0.2000    0.2000         5\n",
            "      AI (1)     0.2000    0.2000    0.2000         5\n",
            "\n",
            "    accuracy                         0.2000        10\n",
            "   macro avg     0.2000    0.2000    0.2000        10\n",
            "weighted avg     0.2000    0.2000    0.2000        10\n",
            "\n",
            "Accuracy: 0.2\n",
            "\n",
            "--- ViT+ResNet stream=high ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViTRes-high] Epoch 1/3 | loss=0.7498 acc=0.500\n",
            "[ViTRes-high] Epoch 2/3 | loss=0.6408 acc=0.650\n",
            "[ViTRes-high] Epoch 3/3 | loss=0.6263 acc=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (ViTRes-high) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.5000    0.4000    0.4444         5\n",
            "      AI (1)     0.5000    0.6000    0.5455         5\n",
            "\n",
            "    accuracy                         0.5000        10\n",
            "   macro avg     0.5000    0.5000    0.4949        10\n",
            "weighted avg     0.5000    0.5000    0.4949        10\n",
            "\n",
            "Accuracy: 0.5\n",
            "\n",
            "--- ViT+ResNet stream=grad ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViTRes-grad] Epoch 1/3 | loss=0.7629 acc=0.450\n",
            "[ViTRes-grad] Epoch 2/3 | loss=0.6926 acc=0.650\n",
            "[ViTRes-grad] Epoch 3/3 | loss=0.6469 acc=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (ViTRes-grad) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.6250    1.0000    0.7692         5\n",
            "      AI (1)     1.0000    0.4000    0.5714         5\n",
            "\n",
            "    accuracy                         0.7000        10\n",
            "   macro avg     0.8125    0.7000    0.6703        10\n",
            "weighted avg     0.8125    0.7000    0.6703        10\n",
            "\n",
            "Accuracy: 0.7\n",
            "\n",
            "--- ViT+ResNet stream=freq ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViTRes-freq] Epoch 1/3 | loss=0.8018 acc=0.500\n",
            "[ViTRes-freq] Epoch 2/3 | loss=0.6809 acc=0.600\n",
            "[ViTRes-freq] Epoch 3/3 | loss=0.6903 acc=0.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (ViTRes-freq) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.6250    1.0000    0.7692         5\n",
            "      AI (1)     1.0000    0.4000    0.5714         5\n",
            "\n",
            "    accuracy                         0.7000        10\n",
            "   macro avg     0.8125    0.7000    0.6703        10\n",
            "weighted avg     0.8125    0.7000    0.6703        10\n",
            "\n",
            "Accuracy: 0.7\n",
            "\n",
            "--- ViT+ResNet stream=fusion_mhfg ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:335: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViTRes-fusion_mhfg] Epoch 1/3 | loss=0.7940 acc=0.350\n",
            "[ViTRes-fusion_mhfg] Epoch 2/3 | loss=0.7179 acc=0.450\n",
            "[ViTRes-fusion_mhfg] Epoch 3/3 | loss=0.6667 acc=0.600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1149741380.py:364: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST RESULTS (ViTRes-fusion_mhfg) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    REAL (0)     0.3333    0.2000    0.2500         5\n",
            "      AI (1)     0.4286    0.6000    0.5000         5\n",
            "\n",
            "    accuracy                         0.4000        10\n",
            "   macro avg     0.3810    0.4000    0.3750        10\n",
            "weighted avg     0.3810    0.4000    0.3750        10\n",
            "\n",
            "Accuracy: 0.4\n",
            "\n",
            "=========== ViT + ResNet RESULTS ===========\n",
            "mid          | 0.2000\n",
            "high         | 0.5000\n",
            "grad         | 0.7000\n",
            "freq         | 0.7000\n",
            "fusion_mhfg  | 0.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_HAMMING = 3\n",
        "BUCKET_BITS = 16\n",
        "BUCKET_SHIFT = 64 - BUCKET_BITS\n",
        "\n",
        "def dhash64_first(path, hash_size=8):\n",
        "    frame = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if frame is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:, 1:] > gray[:, :-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return int(h)\n",
        "\n",
        "def bucket_id(h):\n",
        "    return int((np.uint64(h) >> np.uint64(BUCKET_SHIFT)))\n",
        "\n",
        "def build_dhash_bucketed(samples, split_name):\n",
        "    buckets = defaultdict(list)  # bucket -> list of dict\n",
        "    for p, y, src in tqdm(samples, desc=f\"build dhash {split_name}\"):\n",
        "        h = dhash64_first(p)\n",
        "        if h is None:\n",
        "            continue\n",
        "        b = bucket_id(h)\n",
        "        buckets[b].append({\"split\": split_name, \"y\": int(y), \"src\": src, \"path\": p, \"dhash\": h})\n",
        "    return buckets\n",
        "\n",
        "train_b = build_dhash_bucketed(train_norm, \"train_norm\")\n",
        "test_b  = build_dhash_bucketed(test_norm,  \"test_norm\")\n",
        "\n",
        "def hamming(a,b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "hits = []\n",
        "common_buckets = sorted(set(train_b.keys()).intersection(set(test_b.keys())))\n",
        "print(\"Common buckets:\", len(common_buckets))\n",
        "\n",
        "for b in tqdm(common_buckets, desc=\"scan buckets\"):\n",
        "    A = train_b[b]\n",
        "    B = test_b[b]\n",
        "    for a in A:\n",
        "        ha = a[\"dhash\"]\n",
        "        for c in B:\n",
        "            dist = hamming(ha, c[\"dhash\"])\n",
        "            if dist <= MAX_HAMMING:\n",
        "                hits.append({\n",
        "                    \"bucket\": b,\n",
        "                    \"hamming\": dist,\n",
        "                    \"train_y\": a[\"y\"], \"train_src\": a[\"src\"], \"train_path\": a[\"path\"],\n",
        "                    \"test_y\":  c[\"y\"], \"test_src\":  c[\"src\"], \"test_path\":  c[\"path\"],\n",
        "                })\n",
        "\n",
        "hits_df = pd.DataFrame(hits).sort_values([\"hamming\",\"bucket\"]).reset_index(drop=True)\n",
        "print(\"TOTAL near-dup hits train/test (exhaustive bucketed):\", len(hits_df))\n",
        "\n",
        "hits_csv = os.path.join(RUN_DIR, \"near_dupes_norm_EXHAUSTIVE_bucketed.csv\")\n",
        "hits_df.to_csv(hits_csv, index=False)\n",
        "print(\"Saved:\", hits_csv)\n",
        "\n",
        "# quick peek\n",
        "display(hits_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cSI-XZyAdutp",
        "outputId": "7ca131f8-d9f1-4066-9a91-bd3536df13d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_norm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1236540604.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dhash_bucketed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_norm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtest_b\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbuild_dhash_bucketed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_norm\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"test_norm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_norm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas scikit-learn opencv-python-headless tqdm\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "import cv2\n",
        "import hashlib\n",
        "\n",
        "# =========================\n",
        "# 0) CONFIG\n",
        "# =========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "RUN_DIR = \"/content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456\"\n",
        "near_csv = os.path.join(RUN_DIR, \"near_dupes_norm_multiframe.csv\")\n",
        "\n",
        "train_manifest = os.path.join(RUN_DIR, \"train_manifest_normalized.csv\")\n",
        "test_manifest  = os.path.join(RUN_DIR, \"test_manifest_normalized.csv\")\n",
        "\n",
        "assert os.path.exists(near_csv), f\"Missing: {near_csv}\"\n",
        "assert os.path.exists(train_manifest), f\"Missing: {train_manifest}\"\n",
        "assert os.path.exists(test_manifest),  f\"Missing: {test_manifest}\"\n",
        "\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "print(\"near_csv:\", near_csv)\n",
        "\n",
        "# =========================\n",
        "# 1) LOAD manifests\n",
        "# Expected columns: path,y,src (if your names differ, edit below)\n",
        "# =========================\n",
        "train_df = pd.read_csv(train_manifest)\n",
        "test_df  = pd.read_csv(test_manifest)\n",
        "\n",
        "def normalize_cols(df):\n",
        "    # try to be flexible with column names\n",
        "    colmap = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if lc in [\"path\",\"filepath\",\"file\",\"video_path\"]:\n",
        "            colmap[c] = \"path\"\n",
        "        elif lc in [\"y\",\"label\",\"target\"]:\n",
        "            colmap[c] = \"y\"\n",
        "        elif lc in [\"src\",\"source\",\"domain\"]:\n",
        "            colmap[c] = \"src\"\n",
        "    df = df.rename(columns=colmap)\n",
        "    assert \"path\" in df.columns and \"y\" in df.columns, f\"Need path,y columns. Found: {df.columns.tolist()}\"\n",
        "    if \"src\" not in df.columns:\n",
        "        df[\"src\"] = \"unknown\"\n",
        "    df[\"path\"] = df[\"path\"].astype(str)\n",
        "    df[\"y\"] = df[\"y\"].astype(int)\n",
        "    df[\"src\"] = df[\"src\"].astype(str)\n",
        "    return df\n",
        "\n",
        "train_df = normalize_cols(train_df)\n",
        "test_df  = normalize_cols(test_df)\n",
        "\n",
        "all_df = pd.concat([train_df.assign(split=\"train\"), test_df.assign(split=\"test\")], ignore_index=True)\n",
        "all_paths = set(all_df[\"path\"].tolist())\n",
        "\n",
        "print(\"Loaded normalized manifests:\")\n",
        "print(\" train:\", len(train_df), \" test:\", len(test_df), \" total:\", len(all_df))\n",
        "\n",
        "# =========================\n",
        "# 2) UNION-FIND clustering from near-dup edges\n",
        "# =========================\n",
        "near = pd.read_csv(near_csv)\n",
        "# expected columns: train_path,test_path (or similar)\n",
        "# make robust:\n",
        "def pick_col(df, candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "c_train = pick_col(near, [\"train_path\",\"a_path\",\"path_a\",\"p_train\",\"path1\"])\n",
        "c_test  = pick_col(near, [\"test_path\",\"b_path\",\"path_b\",\"p_test\",\"path2\"])\n",
        "c_dist  = pick_col(near, [\"min_hamming\",\"hamming\",\"dist\",\"distance\"])\n",
        "\n",
        "assert c_train and c_test, f\"near-dupe CSV missing expected columns. Found: {near.columns.tolist()}\"\n",
        "\n",
        "edges = []\n",
        "for _, r in near.iterrows():\n",
        "    a = str(r[c_train]); b = str(r[c_test])\n",
        "    if (a in all_paths) and (b in all_paths):\n",
        "        edges.append((a,b))\n",
        "\n",
        "print(\"Near-dup edges inside current dataset:\", len(edges))\n",
        "\n",
        "# Union-Find\n",
        "parent = {}\n",
        "rank = {}\n",
        "\n",
        "def find(x):\n",
        "    if parent[x] != x:\n",
        "        parent[x] = find(parent[x])\n",
        "    return parent[x]\n",
        "\n",
        "def union(a,b):\n",
        "    ra, rb = find(a), find(b)\n",
        "    if ra == rb:\n",
        "        return\n",
        "    if rank[ra] < rank[rb]:\n",
        "        parent[ra] = rb\n",
        "    elif rank[ra] > rank[rb]:\n",
        "        parent[rb] = ra\n",
        "    else:\n",
        "        parent[rb] = ra\n",
        "        rank[ra] += 1\n",
        "\n",
        "# init\n",
        "for p in all_df[\"path\"].tolist():\n",
        "    parent[p] = p\n",
        "    rank[p] = 0\n",
        "\n",
        "for a,b in edges:\n",
        "    union(a,b)\n",
        "\n",
        "# assign cluster ids\n",
        "root_to_id = {}\n",
        "cluster_id = []\n",
        "for p in all_df[\"path\"].tolist():\n",
        "    r = find(p)\n",
        "    if r not in root_to_id:\n",
        "        root_to_id[r] = len(root_to_id)\n",
        "    cluster_id.append(root_to_id[r])\n",
        "\n",
        "all_df[\"cluster\"] = cluster_id\n",
        "\n",
        "num_clusters = all_df[\"cluster\"].nunique()\n",
        "cluster_sizes = all_df[\"cluster\"].value_counts()\n",
        "print(\"Clusters:\", num_clusters)\n",
        "print(\"Clusters with size>1:\", int((cluster_sizes>1).sum()))\n",
        "\n",
        "# =========================\n",
        "# 3) GROUP-SAFE RESPLIT (no cluster crosses splits)\n",
        "# =========================\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
        "idx = np.arange(len(all_df))\n",
        "train_idx, test_idx = next(gss.split(idx, all_df[\"y\"].values, groups=all_df[\"cluster\"].values))\n",
        "\n",
        "new_train = all_df.iloc[train_idx].copy().reset_index(drop=True)\n",
        "new_test  = all_df.iloc[test_idx].copy().reset_index(drop=True)\n",
        "\n",
        "# sanity: ensure no overlap clusters\n",
        "overlap = set(new_train[\"cluster\"]).intersection(set(new_test[\"cluster\"]))\n",
        "print(\"\\n[GROUP SPLIT] cluster overlap:\", len(overlap))\n",
        "\n",
        "# class balance\n",
        "print(\"New split sizes:\", len(new_train), len(new_test))\n",
        "print(\"Train AI/REAL:\", int((new_train.y==1).sum()), \"/\", int((new_train.y==0).sum()))\n",
        "print(\"Test  AI/REAL:\", int((new_test.y==1).sum()),  \"/\", int((new_test.y==0).sum()))\n",
        "\n",
        "# save new manifests\n",
        "new_train_path = os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\")\n",
        "new_test_path  = os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\")\n",
        "new_train[[\"path\",\"y\",\"src\",\"cluster\"]].to_csv(new_train_path, index=False)\n",
        "new_test[[\"path\",\"y\",\"src\",\"cluster\"]].to_csv(new_test_path, index=False)\n",
        "print(\"Saved:\", new_train_path)\n",
        "print(\"Saved:\", new_test_path)\n",
        "\n",
        "# =========================\n",
        "# 4) Re-run leakage checks on NEW split (fast)\n",
        "# =========================\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if (not ok) or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path, (128,128))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def sha_set(df, name):\n",
        "    s = set()\n",
        "    bad = 0\n",
        "    for p in tqdm(df[\"path\"].tolist(), desc=f\"SHA1 {name}\"):\n",
        "        h = sha1_first_frame(p)\n",
        "        if h is None:\n",
        "            bad += 1\n",
        "            continue\n",
        "        s.add(h)\n",
        "    return s, bad\n",
        "\n",
        "train_sha, bad_tr = sha_set(new_train, \"train_cluster_safe\")\n",
        "test_sha,  bad_te = sha_set(new_test,  \"test_cluster_safe\")\n",
        "exact_overlap = len(train_sha.intersection(test_sha))\n",
        "print(\"\\n[POST-FIX CHECK] Exact SHA1 overlap:\", exact_overlap, \"| bad frames:\", bad_tr, bad_te)\n",
        "\n",
        "# =========================\n",
        "# 5) Metadata-only baseline again (should drop further)\n",
        "# =========================\n",
        "def video_metadata(path):\n",
        "    try:\n",
        "        size = os.path.getsize(path)\n",
        "    except:\n",
        "        size = np.nan\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return [size, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
        "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "    dur = frames/fps if (fps and fps>0) else np.nan\n",
        "    return [size,w,h,fps,frames,dur]\n",
        "\n",
        "def build_meta(df, name):\n",
        "    X, y = [], []\n",
        "    for p, lab in tqdm(df[[\"path\",\"y\"]].values, desc=f\"Metadata {name}\"):\n",
        "        X.append(video_metadata(p))\n",
        "        y.append(int(lab))\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    # fill NaNs with train medians later\n",
        "    return X, y\n",
        "\n",
        "Xtr, ytr = build_meta(new_train, \"train\")\n",
        "Xte, yte = build_meta(new_test,  \"test\")\n",
        "\n",
        "# fill NaNs using TRAIN medians\n",
        "for j in range(Xtr.shape[1]):\n",
        "    med = np.nanmedian(Xtr[:,j])\n",
        "    Xtr[:,j] = np.nan_to_num(Xtr[:,j], nan=med)\n",
        "    Xte[:,j] = np.nan_to_num(Xte[:,j], nan=med)\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "clf.fit(Xtr, ytr)\n",
        "proba = clf.predict_proba(Xte)[:,1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n[POST-FIX METADATA-ONLY]\")\n",
        "print(\"ACC:\", round(accuracy_score(yte,pred),4))\n",
        "print(\"F1 :\", round(f1_score(yte,pred),4))\n",
        "print(\"AUC:\", round(roc_auc_score(yte,proba),4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAoW-JIFeBNs",
        "outputId": "4def85f4-3943-40e6-d672-4af7062bf251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456\n",
            "near_csv: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/near_dupes_norm_multiframe.csv\n",
            "Loaded normalized manifests:\n",
            " train: 1897  test: 473  total: 2370\n",
            "Near-dup edges inside current dataset: 41\n",
            "Clusters: 2338\n",
            "Clusters with size>1: 17\n",
            "\n",
            "[GROUP SPLIT] cluster overlap: 0\n",
            "New split sizes: 1887 483\n",
            "Train AI/REAL: 943 / 944\n",
            "Test  AI/REAL: 248 / 235\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/train_manifest_cluster_safe.csv\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/test_manifest_cluster_safe.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHA1 train_cluster_safe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1887/1887 [09:36<00:00,  3.27it/s]\n",
            "SHA1 test_cluster_safe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [02:19<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[POST-FIX CHECK] Exact SHA1 overlap: 0 | bad frames: 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Metadata train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1887/1887 [00:16<00:00, 114.05it/s]\n",
            "Metadata test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:03<00:00, 129.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[POST-FIX METADATA-ONLY]\n",
            "ACC: 0.677\n",
            "F1 : 0.608\n",
            "AUC: 0.6936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_video(in_path, out_path):\n",
        "    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:\n",
        "        return True\n",
        "\n",
        "    # Stronger neutralization:\n",
        "    # - fixed fps\n",
        "    # - fixed size with pad\n",
        "    # - fixed duration\n",
        "    # - no audio\n",
        "    # - fixed GOP (keyframe interval)\n",
        "    # - force baseline profile for consistency\n",
        "    # - cap bitrate so size doesn't scream \"source\"\n",
        "    vf = (\n",
        "        f\"fps={TARGET_FPS},\"\n",
        "        f\"scale={TARGET_SIZE}:{TARGET_SIZE}:force_original_aspect_ratio=decrease,\"\n",
        "        f\"pad={TARGET_SIZE}:{TARGET_SIZE}:(ow-iw)/2:(oh-ih)/2\"\n",
        "    )\n",
        "\n",
        "    gop = TARGET_FPS * 2  # keyframe every ~2s\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-t\", str(TARGET_SEC),\n",
        "        \"-vf\", vf,\n",
        "        \"-an\",\n",
        "        \"-c:v\", \"libx264\",\n",
        "        \"-profile:v\", \"baseline\",\n",
        "        \"-level\", \"3.1\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        \"-g\", str(gop),\n",
        "        \"-keyint_min\", str(gop),\n",
        "        \"-sc_threshold\", \"0\",\n",
        "        \"-b:v\", \"1M\",       # bitrate cap\n",
        "        \"-maxrate\", \"1M\",\n",
        "        \"-bufsize\", \"2M\",\n",
        "        out_path\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "        return os.path.exists(out_path) and os.path.getsize(out_path) > 1000\n",
        "    except Exception:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "zGuNk18meBDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL DATA LEAKAGE CHECK PIPELINE (RESTART-SAFE)\n",
        "# - Loads dataset from folders\n",
        "# - Builds balanced AI vs REAL samples\n",
        "# - Train/Test split\n",
        "# - CHECK 1: Exact duplicates (SHA1)\n",
        "# - CHECK 2: Near-duplicates (dHash)\n",
        "# - CHECK 3: Metadata-only bias test\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm\n",
        "\n",
        "import os, cv2, hashlib, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1) LOAD VIDEO PATHS\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "\n",
        "assert len(real) > 0 and (len(ai_core)+len(sora)) > 0, \"Dataset paths incorrect\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) BUILD BALANCED DATASET\n",
        "#    AI = AI_CORE + SORA\n",
        "# -----------------------------\n",
        "AI = ai_core + sora\n",
        "N = min(len(AI), len(real))\n",
        "\n",
        "AI = random.sample(AI, N)\n",
        "REAL = random.sample(real, N)\n",
        "\n",
        "samples = [(p, 1) for p in AI] + [(p, 0) for p in REAL]\n",
        "random.shuffle(samples)\n",
        "\n",
        "print(\"\\nBalanced samples:\", len(samples),\n",
        "      \"(AI=\", N, \", REAL=\", N, \")\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) TRAIN / TEST SPLIT\n",
        "# -----------------------------\n",
        "train_samples, test_samples = train_test_split(\n",
        "    samples,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=[y for _, y in samples]\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_samples), \"Test:\", len(test_samples))\n",
        "\n",
        "# ============================================================\n",
        "# UTILITIES\n",
        "# ============================================================\n",
        "def get_first_frame(path, size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# ============================================================\n",
        "# CHECK 1: EXACT DUPLICATES (SHA1)\n",
        "# ============================================================\n",
        "def sha1_first_frame(path):\n",
        "    frame = get_first_frame(path, (128,128))\n",
        "    if frame is None:\n",
        "        return None\n",
        "    return hashlib.sha1(frame.tobytes()).hexdigest()\n",
        "\n",
        "def build_sha_map(samples, split):\n",
        "    hmap = defaultdict(list)\n",
        "    for p,y in tqdm(samples, desc=f\"SHA1 {split}\"):\n",
        "        h = sha1_first_frame(p)\n",
        "        if h:\n",
        "            hmap[h].append((split, y, p))\n",
        "    return hmap\n",
        "\n",
        "train_sha = build_sha_map(train_samples, \"train\")\n",
        "test_sha  = build_sha_map(test_samples,  \"test\")\n",
        "\n",
        "all_sha = defaultdict(list)\n",
        "for d in [train_sha, test_sha]:\n",
        "    for k,v in d.items():\n",
        "        all_sha[k].extend(v)\n",
        "\n",
        "exact_dupes = {k:v for k,v in all_sha.items()\n",
        "               if len(set(x[0] for x in v)) > 1}\n",
        "\n",
        "print(\"\\n[CHECK 1] Exact duplicates across splits:\", len(exact_dupes))\n",
        "\n",
        "# ============================================================\n",
        "# CHECK 2: NEAR DUPLICATES (dHash)\n",
        "# ============================================================\n",
        "def dhash(path, hash_size=8):\n",
        "    frame = get_first_frame(path, (hash_size+1, hash_size))\n",
        "    if frame is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return h\n",
        "\n",
        "def hamming(a,b):\n",
        "    return (a^b).bit_count()\n",
        "\n",
        "train_d = [(p,y,dhash(p)) for p,y in train_samples if dhash(p) is not None]\n",
        "test_d  = [(p,y,dhash(p)) for p,y in test_samples  if dhash(p) is not None]\n",
        "\n",
        "near = []\n",
        "for pa,ya,ha in train_d:\n",
        "    for pb,yb,hb in test_d:\n",
        "        if hamming(ha,hb) <= 3:\n",
        "            near.append((pa,pb))\n",
        "            if len(near) >= 50:\n",
        "                break\n",
        "\n",
        "print(\"[CHECK 2] Near-duplicate pairs (H<=3):\", len(near))\n",
        "\n",
        "# ============================================================\n",
        "# CHECK 3: METADATA-ONLY SOURCE CUE TEST\n",
        "# ============================================================\n",
        "def video_metadata(path):\n",
        "    size = os.path.getsize(path)\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return [size,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
        "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "    dur = frames/fps if fps>0 else np.nan\n",
        "    return [size,w,h,fps,frames,dur]\n",
        "\n",
        "cols = [\"size\",\"w\",\"h\",\"fps\",\"frames\",\"dur\"]\n",
        "\n",
        "def build_meta(samples):\n",
        "    X,Y = [],[]\n",
        "    for p,y in tqdm(samples, desc=\"Metadata\"):\n",
        "        X.append(video_metadata(p))\n",
        "        Y.append(y)\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "Xtr,Ytr = build_meta(train_samples)\n",
        "Xte,Yte = build_meta(test_samples)\n",
        "\n",
        "# fill NaNs\n",
        "for i in range(Xtr.shape[1]):\n",
        "    med = np.nanmedian(Xtr[:,i])\n",
        "    Xtr[:,i] = np.nan_to_num(Xtr[:,i], nan=med)\n",
        "    Xte[:,i] = np.nan_to_num(Xte[:,i], nan=med)\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "clf.fit(Xtr,Ytr)\n",
        "proba = clf.predict_proba(Xte)[:,1]\n",
        "pred  = (proba>=0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(Yte,pred)\n",
        "f1  = f1_score(Yte,pred)\n",
        "auc = roc_auc_score(Yte,proba)\n",
        "\n",
        "print(\"\\n[CHECK 3] Metadata-only baseline\")\n",
        "print(\"ACC:\", round(acc,4))\n",
        "print(\"F1 :\", round(f1,4))\n",
        "print(\"AUC:\", round(auc,4))\n",
        "\n",
        "print(\"\\n================ INTERPRETATION ================\")\n",
        "print(\"CHECK 1 > 0  => HARD leakage\")\n",
        "print(\"CHECK 2 many => RE-ENCODE leakage\")\n",
        "print(\"CHECK 3 AUC > ~0.75 => SOURCE CUE BIAS\")\n",
        "print(\"================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZxQf3RFoy6O",
        "outputId": "c2c578ef-9596-40f8-b4b6-c437729aaea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "Balanced samples: 2400 (AI= 1200 , REAL= 1200 )\n",
            "Train: 1920 Test: 480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHA1 train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1920/1920 [13:37<00:00,  2.35it/s]\n",
            "SHA1 test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [03:09<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CHECK 1] Exact duplicates across splits: 0\n",
            "[CHECK 2] Near-duplicate pairs (H<=3): 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1920/1920 [00:36<00:00, 53.07it/s]\n",
            "Metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:10<00:00, 45.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CHECK 3] Metadata-only baseline\n",
            "ACC: 0.9938\n",
            "F1 : 0.9937\n",
            "AUC: 1.0\n",
            "\n",
            "================ INTERPRETATION ================\n",
            "CHECK 1 > 0  => HARD leakage\n",
            "CHECK 2 many => RE-ENCODE leakage\n",
            "CHECK 3 AUC > ~0.75 => SOURCE CUE BIAS\n",
            "================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, hashlib\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_first_frame(path, target_size=(128, 128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if (not ok) or frame is None:\n",
        "        return None\n",
        "    frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
        "    return frame  # BGR uint8\n",
        "\n",
        "def sha1_first_frame(path, target_size=(128, 128)):\n",
        "    frame = get_first_frame(path, target_size)\n",
        "    if frame is None:\n",
        "        return None\n",
        "    # hash raw bytes of resized decoded frame (stable across re-encodes only if frame decodes identically)\n",
        "    h = hashlib.sha1(frame.tobytes()).hexdigest()\n",
        "    return h\n",
        "\n",
        "def build_hash_map(samples, split_name, target_size=(128,128)):\n",
        "    hmap = defaultdict(list)  # hash -> list of (split, path, label)\n",
        "    for p, y in samples:\n",
        "        h = sha1_first_frame(p, target_size)\n",
        "        if h is not None:\n",
        "            hmap[h].append((split_name, p, int(y)))\n",
        "    return hmap\n",
        "\n",
        "# If you don't have val_samples in your script, do: val_samples = []\n",
        "val_samples = globals().get(\"val_samples\", [])\n",
        "\n",
        "train_h = build_hash_map(train_samples, \"train\")\n",
        "val_h   = build_hash_map(val_samples, \"val\") if len(val_samples) else {}\n",
        "test_h  = build_hash_map(test_samples, \"test\")\n",
        "\n",
        "# merge\n",
        "all_h = defaultdict(list)\n",
        "for d in [train_h, val_h, test_h]:\n",
        "    for k, v in d.items():\n",
        "        all_h[k].extend(v)\n",
        "\n",
        "# find hashes that appear in multiple splits\n",
        "multi_split = {h: items for h, items in all_h.items()\n",
        "               if len(set(i[0] for i in items)) > 1}\n",
        "\n",
        "print(\"Hard-duplicate hashes across splits:\", len(multi_split))\n",
        "\n",
        "# show a few examples\n",
        "shown = 0\n",
        "for h, items in list(multi_split.items())[:10]:\n",
        "    print(\"\\nHASH:\", h)\n",
        "    for it in items:\n",
        "        print(\" \", it[0], \"|\", it[2], \"|\", it[1])\n",
        "    shown += 1\n",
        "print(\"\\n(Showing up to 10 hash collisions across splits.)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "N20-JTrNo8zE",
        "outputId": "80d34776-290d-40cb-b558-446fb6ed65a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_samples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1323362636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mval_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_samples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_hash_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mval_h\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mbuild_hash_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtest_h\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbuild_hash_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def video_metadata(path):\n",
        "    # returns numeric features; NaNs replaced later\n",
        "    size_bytes = os.path.getsize(path) if os.path.exists(path) else np.nan\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return {\n",
        "            \"size_bytes\": size_bytes,\n",
        "            \"w\": np.nan, \"h\": np.nan, \"fps\": np.nan, \"frames\": np.nan, \"duration\": np.nan\n",
        "        }\n",
        "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "    duration = (frames / fps) if (fps and fps > 0 and frames and frames > 0) else np.nan\n",
        "    return {\n",
        "        \"size_bytes\": size_bytes,\n",
        "        \"w\": w, \"h\": h, \"fps\": fps, \"frames\": frames, \"duration\": duration\n",
        "    }\n",
        "\n",
        "def build_meta_df(samples, split_name):\n",
        "    rows = []\n",
        "    for p, y in samples:\n",
        "        m = video_metadata(p)\n",
        "        m[\"path\"] = p\n",
        "        m[\"y\"] = int(y)\n",
        "        m[\"split\"] = split_name\n",
        "        rows.append(m)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "val_samples = globals().get(\"val_samples\", [])\n",
        "\n",
        "df = pd.concat([\n",
        "    build_meta_df(train_samples, \"train\"),\n",
        "    build_meta_df(val_samples, \"val\") if len(val_samples) else pd.DataFrame([]),\n",
        "    build_meta_df(test_samples, \"test\"),\n",
        "], ignore_index=True)\n",
        "\n",
        "# Fill NaNs with column medians (safe)\n",
        "for col in [\"size_bytes\",\"w\",\"h\",\"fps\",\"frames\",\"duration\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        med = df[col].median()\n",
        "        df[col] = df[col].fillna(med)\n",
        "\n",
        "feat_cols = [\"size_bytes\",\"w\",\"h\",\"fps\",\"frames\",\"duration\"]\n",
        "\n",
        "X_train = df[df.split==\"train\"][feat_cols].values\n",
        "y_train = df[df.split==\"train\"][\"y\"].values\n",
        "\n",
        "X_test  = df[df.split==\"test\"][feat_cols].values\n",
        "y_test  = df[df.split==\"test\"][\"y\"].values\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "proba = clf.predict_proba(X_test)[:,1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "f1  = f1_score(y_test, pred)\n",
        "auc = roc_auc_score(y_test, proba)\n",
        "\n",
        "print(\"\\n=== METADATA-ONLY BASELINE (source-cue test) ===\")\n",
        "print(\"acc:\", acc)\n",
        "print(\"f1 :\", f1)\n",
        "print(\"auc:\", auc)\n",
        "\n",
        "# Optional: see coefficient magnitudes (what cues it uses)\n",
        "coefs = clf.named_steps[\"lr\"].coef_[0]\n",
        "imp = sorted(zip(feat_cols, coefs), key=lambda x: abs(x[1]), reverse=True)\n",
        "print(\"\\nTop metadata cues (abs coef):\")\n",
        "for k,v in imp:\n",
        "    print(f\"{k:10s}  coef={v:+.4f}\")\n"
      ],
      "metadata": {
        "id": "wVHWEKVzoy23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9IrjIjELM7S",
        "outputId": "f36357f8-9198-42d1-f2fb-ea0667e88fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Saving to: /content/drive/MyDrive/ai_detection_eval/R2Res_GRAD_FINAL_20251212_185731\n",
            "AI_CORE: 1200\n",
            "SORA   : 1250\n",
            "REAL   : 1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checking AI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1490/1490 [04:14<00:00,  5.84it/s]\n",
            "Checking REAL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:03<00:00, 307.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Balanced to N=1200 per class.\n",
            "Valid AI  : 1200 Dropped AI : 0\n",
            "Valid REAL: 1200 Dropped REAL: 0\n",
            "\n",
            "Split sizes:\n",
            "Train: 1536\n",
            "Val  : 384\n",
            "Test : 480\n",
            "\n",
            "Sanity batch: torch.Size([4, 3, 8, 96, 96]) tensor([1, 1, 0, 1])\n",
            "\n",
            "Train counts: Counter({1: 768, 0: 768}) class_weights: [0.5, 0.5]\n",
            "\n",
            "Starting training for 20 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/20 | train_loss=0.6870 train_acc=0.576 | val_acc=0.773 val_f1=0.758 val_auc=0.849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02/20 | train_loss=0.5773 train_acc=0.703 | val_acc=0.799 val_f1=0.774 val_auc=0.901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03/20 | train_loss=0.5352 train_acc=0.749 | val_acc=0.839 val_f1=0.843 val_auc=0.920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04/20 | train_loss=0.5248 train_acc=0.755 | val_acc=0.859 val_f1=0.857 val_auc=0.924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05/20 | train_loss=0.5245 train_acc=0.747 | val_acc=0.872 val_f1=0.868 val_auc=0.936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06/20 | train_loss=0.5370 train_acc=0.748 | val_acc=0.867 val_f1=0.867 val_auc=0.941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07/20 | train_loss=0.5053 train_acc=0.778 | val_acc=0.875 val_f1=0.869 val_auc=0.941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08/20 | train_loss=0.4778 train_acc=0.791 | val_acc=0.880 val_f1=0.876 val_auc=0.947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09/20 | train_loss=0.4658 train_acc=0.799 | val_acc=0.893 val_f1=0.889 val_auc=0.949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 | train_loss=0.5011 train_acc=0.781 | val_acc=0.891 val_f1=0.886 val_auc=0.950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 | train_loss=0.4909 train_acc=0.794 | val_acc=0.893 val_f1=0.891 val_auc=0.951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 | train_loss=0.5031 train_acc=0.785 | val_acc=0.883 val_f1=0.883 val_auc=0.954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 | train_loss=0.4824 train_acc=0.793 | val_acc=0.888 val_f1=0.888 val_auc=0.953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 | train_loss=0.4688 train_acc=0.800 | val_acc=0.888 val_f1=0.889 val_auc=0.956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 | train_loss=0.4776 train_acc=0.803 | val_acc=0.875 val_f1=0.878 val_auc=0.955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 | train_loss=0.4733 train_acc=0.808 | val_acc=0.870 val_f1=0.874 val_auc=0.957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 | train_loss=0.4499 train_acc=0.816 | val_acc=0.880 val_f1=0.883 val_auc=0.957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 | train_loss=0.4767 train_acc=0.805 | val_acc=0.888 val_f1=0.888 val_auc=0.960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 | train_loss=0.4944 train_acc=0.796 | val_acc=0.885 val_f1=0.886 val_auc=0.961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 | train_loss=0.4912 train_acc=0.794 | val_acc=0.898 val_f1=0.899 val_auc=0.959\n",
            "\n",
            "=== FINAL TEST METRICS (BEST VAL-F1 CHECKPOINT) ===\n",
            "{\n",
            "  \"acc\": 0.8979166666666667,\n",
            "  \"precision\": 0.8745098039215686,\n",
            "  \"recall\": 0.9291666666666667,\n",
            "  \"f1\": 0.901010101010101,\n",
            "  \"roc_auc\": 0.9615277777777779,\n",
            "  \"ap\": 0.962301160831033\n",
            "}\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/R2Res_GRAD_FINAL_20251212_185731\n",
            "\n",
            "[ROBUST] Resolution sweep...\n",
            "  resize= 64 -> acc=0.840 f1=0.855 auc=0.931\n",
            "  resize= 80 -> acc=0.877 f1=0.883 auc=0.941\n",
            "  resize= 96 -> acc=0.898 f1=0.901 auc=0.962\n",
            "  resize=112 -> acc=0.892 f1=0.896 auc=0.961\n",
            "  resize=128 -> acc=0.906 f1=0.909 auc=0.966\n",
            "\n",
            "[ROBUST] JPEG compression sweep...\n",
            "  jpegQ= 10 -> acc=0.644 f1=0.464 auc=0.858\n",
            "  jpegQ= 20 -> acc=0.796 f1=0.768 auc=0.903\n",
            "  jpegQ= 30 -> acc=0.844 f1=0.838 auc=0.923\n",
            "  jpegQ= 40 -> acc=0.867 f1=0.867 auc=0.936\n",
            "  jpegQ= 50 -> acc=0.875 f1=0.877 auc=0.940\n",
            "  jpegQ= 70 -> acc=0.890 f1=0.893 auc=0.953\n",
            "  jpegQ= 90 -> acc=0.887 f1=0.893 auc=0.960\n",
            "\n",
            "[ROBUST] Saved robustness files into: /content/drive/MyDrive/ai_detection_eval/R2Res_GRAD_FINAL_20251212_185731\n",
            "DONE.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# FULL COLAB CODE (FINAL PAPER RUN PACKAGE)\n",
        "# R(2+1)D + ResNet18 fusion on GRAD stream (AI vs REAL videos)\n",
        "#\n",
        "# âœ… Balanced sampling (up to N per class) from:\n",
        "#   AI = AI_CORE_DIR + SORA_DIR\n",
        "#   REAL = REAL_DIR\n",
        "# âœ… Drops broken videos BEFORE sampling final balanced set\n",
        "# âœ… No leakage: single fixed split, stratified\n",
        "# âœ… Saves EVERYTHING into a new Drive folder (RUN_DIR):\n",
        "#   - best_model.pt, last_model.pt\n",
        "#   - config.json\n",
        "#   - splits.json (paths used)\n",
        "#   - history.csv (train/val curves)\n",
        "#   - metrics_test.json + metrics_val.json\n",
        "#   - confusion_matrix.png\n",
        "#   - roc_curve.png, pr_curve.png\n",
        "#   - training_curves.png\n",
        "#   - robustness_resolution.csv/png\n",
        "#   - robustness_jpeg.csv/png\n",
        "#\n",
        "# Robustness:\n",
        "#   - Resolution sweep (resize before GRAD)\n",
        "#   - JPEG compression sweep (proxy for compression artifacts)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch torchvision opencv-python-headless tqdm scikit-learn matplotlib\n",
        "\n",
        "import os, glob, json, time, random\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, roc_curve, auc,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "\n",
        "    # data\n",
        "    ai_core_dir: str = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "    sora_dir: str    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "    real_dir: str    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "    max_per_class: int = 1200         # <--- set to 1200 for your big balanced run\n",
        "    min_frames_valid: int = 2\n",
        "\n",
        "    # clip sampling\n",
        "    num_frames: int = 8\n",
        "    resize_hw: tuple = (96, 96)\n",
        "\n",
        "    # train split (no leakage)\n",
        "    test_size: float = 0.20           # 20% held-out test\n",
        "    val_size: float  = 0.20           # of remaining trainval (so overall ~16% val)\n",
        "    # i.e. train ~64%, val ~16%, test ~20%\n",
        "\n",
        "    # training\n",
        "    batch_size: int = 4\n",
        "    epochs: int = 20\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-3        # regularization\n",
        "    label_smoothing: float = 0.05     # regularization\n",
        "    grad_clip: float = 1.0            # stability\n",
        "    num_workers: int = 0\n",
        "\n",
        "    # model\n",
        "    dropout: float = 0.3\n",
        "    train_backbone: bool = False      # keep False for speed/stability\n",
        "\n",
        "    # robustness sweeps\n",
        "    rob_res_sizes: tuple = (64, 80, 96, 112, 128)\n",
        "    rob_jpeg_qual: tuple = (10, 20, 30, 40, 50, 70, 90)\n",
        "    rob_max_test_samples: int = None  # set e.g. 400 to speed it up\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "def seed_all(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_all(cfg.seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Run folder (new)\n",
        "# -----------------------------\n",
        "RUN_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "os.makedirs(RUN_ROOT, exist_ok=True)\n",
        "RUN_DIR = os.path.join(RUN_ROOT, f\"R2Res_GRAD_FINAL_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"Saving to:\", RUN_DIR)\n",
        "\n",
        "with open(os.path.join(RUN_DIR, \"config.json\"), \"w\") as f:\n",
        "    json.dump(asdict(cfg), f, indent=2)\n",
        "\n",
        "# ============================================================\n",
        "# Utils: list videos + validity check\n",
        "# ============================================================\n",
        "def list_videos(folder):\n",
        "    vids = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        vids.extend(glob.glob(os.path.join(folder, ext)))\n",
        "    return sorted(vids)\n",
        "\n",
        "def is_valid_video(path, min_frames=2):\n",
        "    if not os.path.exists(path):\n",
        "        return False\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return False\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "    return n >= min_frames\n",
        "\n",
        "ai_core_all = list_videos(cfg.ai_core_dir)\n",
        "sora_all    = list_videos(cfg.sora_dir)\n",
        "real_all    = list_videos(cfg.real_dir)\n",
        "\n",
        "print(\"AI_CORE:\", len(ai_core_all))\n",
        "print(\"SORA   :\", len(sora_all))\n",
        "print(\"REAL   :\", len(real_all))\n",
        "\n",
        "ai_all = ai_core_all + sora_all\n",
        "random.shuffle(ai_all)\n",
        "random.shuffle(real_all)\n",
        "\n",
        "# ============================================================\n",
        "# Step 1: Pre-filter valid videos (fast check) then balance\n",
        "# ============================================================\n",
        "def filter_valid(paths, tag):\n",
        "    ok = []\n",
        "    bad = []\n",
        "    for p in tqdm(paths, desc=f\"Checking {tag}\"):\n",
        "        if is_valid_video(p, min_frames=cfg.min_frames_valid):\n",
        "            ok.append(p)\n",
        "        else:\n",
        "            bad.append(p)\n",
        "    return ok, bad\n",
        "\n",
        "# We first take a larger candidate pool to avoid ending short after dropping\n",
        "candidate_ai   = ai_all[: min(len(ai_all), int(cfg.max_per_class * 1.2) + 50)]\n",
        "candidate_real = real_all[: min(len(real_all), int(cfg.max_per_class * 1.2) + 50)]\n",
        "\n",
        "valid_ai, bad_ai = filter_valid(candidate_ai, \"AI\")\n",
        "valid_real, bad_real = filter_valid(candidate_real, \"REAL\")\n",
        "\n",
        "N = min(cfg.max_per_class, len(valid_ai), len(valid_real))\n",
        "valid_ai = valid_ai[:N]\n",
        "valid_real = valid_real[:N]\n",
        "\n",
        "print(f\"\\nBalanced to N={N} per class.\")\n",
        "print(\"Valid AI  :\", len(valid_ai), \"Dropped AI :\", len(bad_ai))\n",
        "print(\"Valid REAL:\", len(valid_real), \"Dropped REAL:\", len(bad_real))\n",
        "\n",
        "# build labeled samples\n",
        "samples = [(p, 1) for p in valid_ai] + [(p, 0) for p in valid_real]\n",
        "random.shuffle(samples)\n",
        "\n",
        "# ============================================================\n",
        "# Step 2: No leakage split (stratified)\n",
        "# ============================================================\n",
        "X = [p for p, y in samples]\n",
        "y = [y for p, y in samples]\n",
        "\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
        ")\n",
        "\n",
        "# val_size is fraction of trainval\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=cfg.val_size, random_state=cfg.seed, stratify=y_trainval\n",
        ")\n",
        "\n",
        "train_samples = list(zip(X_train, y_train))\n",
        "val_samples   = list(zip(X_val, y_val))\n",
        "test_samples  = list(zip(X_test, y_test))\n",
        "\n",
        "print(\"\\nSplit sizes:\")\n",
        "print(\"Train:\", len(train_samples))\n",
        "print(\"Val  :\", len(val_samples))\n",
        "print(\"Test :\", len(test_samples))\n",
        "\n",
        "# save splits for paper reproducibility\n",
        "def dump_split(name, split):\n",
        "    with open(os.path.join(RUN_DIR, f\"{name}.json\"), \"w\") as f:\n",
        "        json.dump([{\"path\": p, \"label\": int(y)} for p, y in split], f, indent=2)\n",
        "\n",
        "dump_split(\"train_split\", train_samples)\n",
        "dump_split(\"val_split\", val_samples)\n",
        "dump_split(\"test_split\", test_samples)\n",
        "\n",
        "# ============================================================\n",
        "# Video loader (OpenCV) + optional JPEG proxy compression\n",
        "# ============================================================\n",
        "def jpeg_compress_rgb_frame(rgb_uint8, quality=30):\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)]\n",
        "    ok, enc = cv2.imencode(\".jpg\", cv2.cvtColor(rgb_uint8, cv2.COLOR_RGB2BGR), encode_param)\n",
        "    if not ok:\n",
        "        return rgb_uint8\n",
        "    dec = cv2.imdecode(enc, cv2.IMREAD_COLOR)\n",
        "    if dec is None:\n",
        "        return rgb_uint8\n",
        "    return cv2.cvtColor(dec, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def load_video_clip_opencv(path, num_frames=8, size=(96, 96), jpeg_quality=None):\n",
        "    H, W = size\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if frame_count <= 0:\n",
        "        cap.release()\n",
        "        return torch.zeros(3, num_frames, H, W)\n",
        "\n",
        "    if frame_count >= num_frames:\n",
        "        idxs = np.linspace(0, frame_count - 1, num_frames).astype(int)\n",
        "    else:\n",
        "        reps = int(np.ceil(num_frames / frame_count))\n",
        "        idxs = np.tile(np.arange(frame_count), reps)[:num_frames]\n",
        "\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, frame = cap.read()\n",
        "        if not ok or frame is None:\n",
        "            frame_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_rgb = cv2.resize(frame_rgb, (W, H), interpolation=cv2.INTER_AREA)\n",
        "            if jpeg_quality is not None:\n",
        "                frame_rgb = jpeg_compress_rgb_frame(frame_rgb.astype(np.uint8), quality=jpeg_quality)\n",
        "\n",
        "        frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    cap.release()\n",
        "    return torch.stack(frames, dim=1)  # [C,T,H,W]\n",
        "\n",
        "# ============================================================\n",
        "# GRAD feature stream + Kinetics normalization\n",
        "# ============================================================\n",
        "KIN_MEAN = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3, 1, 1, 1)\n",
        "KIN_STD  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3, 1, 1, 1)\n",
        "\n",
        "def normalize_kinetics(frames):\n",
        "    return (frames - KIN_MEAN) / KIN_STD\n",
        "\n",
        "def gradient_magnitude(frames):\n",
        "    # frames: [C,T,H,W] in [0,1]\n",
        "    C, T, H, W = frames.shape\n",
        "    out = torch.zeros_like(frames)\n",
        "    for t in range(T):\n",
        "        frame = frames[:, t]\n",
        "        r, g, b = frame[0], frame[1], frame[2]\n",
        "        gray = (0.299 * r + 0.587 * g + 0.114 * b).cpu().numpy().astype(np.float32)\n",
        "\n",
        "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        if mag.max() > 0:\n",
        "            mag = mag / (mag.max() + 1e-6)\n",
        "        mag = np.clip(mag, 0.0, 1.0)\n",
        "        mag3 = np.stack([mag, mag, mag], axis=0)\n",
        "        out[:, t] = torch.from_numpy(mag3)\n",
        "    return out\n",
        "\n",
        "# ============================================================\n",
        "# Dataset (GRAD only; still returns dict for model)\n",
        "# ============================================================\n",
        "class GradStreamDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=8, size=(96,96), jpeg_quality=None, max_items=None):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "        self.jpeg_quality = jpeg_quality\n",
        "\n",
        "        if max_items is not None and max_items < len(self.samples):\n",
        "            rr = random.Random(cfg.seed + 999)\n",
        "            idxs = list(range(len(self.samples)))\n",
        "            rr.shuffle(idxs)\n",
        "            idxs = idxs[:max_items]\n",
        "            self.samples = [self.samples[i] for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_clip_opencv(path, self.num_frames, self.size, jpeg_quality=self.jpeg_quality)\n",
        "        grad = normalize_kinetics(gradient_magnitude(frames))\n",
        "        return {\"grad\": grad}, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = GradStreamDataset(train_samples, cfg.num_frames, cfg.resize_hw)\n",
        "val_ds   = GradStreamDataset(val_samples,   cfg.num_frames, cfg.resize_hw)\n",
        "test_ds  = GradStreamDataset(test_samples,  cfg.num_frames, cfg.resize_hw)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "# sanity batch\n",
        "streams, labels = next(iter(train_loader))\n",
        "print(\"\\nSanity batch:\", streams[\"grad\"].shape, labels[:8])\n",
        "\n",
        "# ============================================================\n",
        "# Class weights (based on TRAIN only)\n",
        "# ============================================================\n",
        "train_counts = Counter([y for _, y in train_samples])\n",
        "real_count = train_counts.get(0, 0)\n",
        "ai_count   = train_counts.get(1, 0)\n",
        "w_real = 1.0 / max(real_count, 1)\n",
        "w_ai   = 1.0 / max(ai_count, 1)\n",
        "s = w_real + w_ai\n",
        "w_real /= s; w_ai /= s\n",
        "class_weights = torch.tensor([w_real, w_ai], dtype=torch.float32).to(device)\n",
        "print(\"\\nTrain counts:\", train_counts, \"class_weights:\", class_weights.tolist())\n",
        "\n",
        "# ============================================================\n",
        "# Model: R(2+1)D + ResNet18 fusion (GRAD)\n",
        "# ============================================================\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "def make_r2plus1d_backbone():\n",
        "    model = r2plus1d_18(weights=R2Plus1D_18_Weights.KINETICS400_V1)\n",
        "    d = model.fc.in_features\n",
        "    model.fc = nn.Identity()\n",
        "    return model, d\n",
        "\n",
        "def make_resnet_backbone():\n",
        "    net = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    d = net.fc.in_features\n",
        "    net.fc = nn.Identity()\n",
        "    return net, d\n",
        "\n",
        "class R2PlusResNetFusion(nn.Module):\n",
        "    def __init__(self, num_classes=2, dropout=0.3, train_backbone=False):\n",
        "        super().__init__()\n",
        "        self.r2, d_r2 = make_r2plus1d_backbone()\n",
        "        self.resnet, d_res = make_resnet_backbone()\n",
        "\n",
        "        if not train_backbone:\n",
        "            for p in self.r2.parameters(): p.requires_grad = False\n",
        "            for p in self.resnet.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_r2 + d_res, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, streams):\n",
        "        x = streams[\"grad\"]  # [B,C,T,H,W]\n",
        "\n",
        "        # R2+1D features\n",
        "        r2_feats = self.r2(x)  # [B, d_r2]\n",
        "\n",
        "        # ResNet per-frame features -> temporal mean\n",
        "        B, C, T, H, W = x.shape\n",
        "        frames = x.permute(0,2,1,3,4).contiguous().view(B*T, C, H, W)\n",
        "        if (H, W) != (224, 224):\n",
        "            frames = F.interpolate(frames, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        if self.resnet.fc is None:\n",
        "            res_feats = self.resnet(frames)\n",
        "        else:\n",
        "            res_feats = self.resnet(frames)\n",
        "\n",
        "        res_feats = res_feats.view(B, T, -1).mean(dim=1)  # [B, d_res]\n",
        "\n",
        "        fused = torch.cat([r2_feats, res_feats], dim=1)\n",
        "        return self.head(fused)\n",
        "\n",
        "model = R2PlusResNetFusion(num_classes=2, dropout=cfg.dropout, train_backbone=cfg.train_backbone).to(device)\n",
        "\n",
        "# ============================================================\n",
        "# Metrics helpers\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def predict_probs(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_prob, all_pred = [], [], []\n",
        "    for streams, y in loader:\n",
        "        streams[\"grad\"] = streams[\"grad\"].to(device)\n",
        "        y = y.to(device)\n",
        "        logits = model(streams)\n",
        "        prob = torch.softmax(logits, dim=1)[:, 1]\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        all_y.extend(y.cpu().numpy().tolist())\n",
        "        all_prob.extend(prob.cpu().numpy().tolist())\n",
        "        all_pred.extend(pred.cpu().numpy().tolist())\n",
        "    return np.array(all_y), np.array(all_prob), np.array(all_pred)\n",
        "\n",
        "def compute_metrics(y_true, y_prob, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    # ROC-AUC may fail if only one class present\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    except Exception:\n",
        "        fpr, tpr, roc_auc = None, None, float(\"nan\")\n",
        "    try:\n",
        "        ap = average_precision_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        ap = float(\"nan\")\n",
        "    return {\n",
        "        \"acc\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1\": float(f1),\n",
        "        \"roc_auc\": float(roc_auc),\n",
        "        \"ap\": float(ap),\n",
        "    }\n",
        "\n",
        "def save_confusion_matrix(y_true, y_pred, out_png):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(cm)\n",
        "    plt.title(\"Confusion Matrix (0=Real, 1=AI)\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.xticks([0,1], [\"Real\",\"AI\"]); plt.yticks([0,1], [\"Real\",\"AI\"])\n",
        "    for (i,j), v in np.ndenumerate(cm):\n",
        "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_roc_pr_curves(y_true, y_prob, roc_png, pr_png):\n",
        "    # ROC\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        fig = plt.figure()\n",
        "        plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
        "        plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\")\n",
        "        plt.legend()\n",
        "        fig.savefig(roc_png, dpi=200, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # PR\n",
        "    try:\n",
        "        p, r, _ = precision_recall_curve(y_true, y_prob)\n",
        "        ap = average_precision_score(y_true, y_prob)\n",
        "        fig = plt.figure()\n",
        "        plt.plot(r, p, label=f\"AP={ap:.3f}\")\n",
        "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precisionâ€“Recall Curve\")\n",
        "        plt.legend()\n",
        "        fig.savefig(pr_png, dpi=200, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ============================================================\n",
        "# Training loop (best on VAL by F1)\n",
        "# ============================================================\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=cfg.label_smoothing).to(device)\n",
        "\n",
        "# trainable params\n",
        "trainable = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(trainable, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "history = []\n",
        "\n",
        "best_val_f1 = -1.0\n",
        "best_path = os.path.join(RUN_DIR, \"best_model.pt\")\n",
        "last_path = os.path.join(RUN_DIR, \"last_model.pt\")\n",
        "\n",
        "print(f\"\\nStarting training for {cfg.epochs} epochs...\")\n",
        "\n",
        "for epoch in range(1, cfg.epochs + 1):\n",
        "    model.train()\n",
        "    run_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for streams, y in tqdm(train_loader, desc=f\"Epoch {epoch:02d}/{cfg.epochs}\", leave=False):\n",
        "        streams[\"grad\"] = streams[\"grad\"].to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(streams)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        if cfg.grad_clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(trainable, cfg.grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        run_loss += loss.item() * y.size(0)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_loss = run_loss / max(total, 1)\n",
        "    train_acc  = correct / max(total, 1)\n",
        "\n",
        "    # validate\n",
        "    yv, pv, yv_pred = predict_probs(model, val_loader)\n",
        "    val_m = compute_metrics(yv, pv, yv_pred)\n",
        "\n",
        "    row = {\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": float(train_loss),\n",
        "        \"train_acc\": float(train_acc),\n",
        "        \"val_acc\": val_m[\"acc\"],\n",
        "        \"val_precision\": val_m[\"precision\"],\n",
        "        \"val_recall\": val_m[\"recall\"],\n",
        "        \"val_f1\": val_m[\"f1\"],\n",
        "        \"val_roc_auc\": val_m[\"roc_auc\"],\n",
        "        \"val_ap\": val_m[\"ap\"],\n",
        "    }\n",
        "    history.append(row)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
        "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} | \"\n",
        "        f\"val_acc={val_m['acc']:.3f} val_f1={val_m['f1']:.3f} val_auc={val_m['roc_auc']:.3f}\"\n",
        "    )\n",
        "\n",
        "    # save last\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"cfg\": asdict(cfg)\n",
        "    }, last_path)\n",
        "\n",
        "    # save best by val_f1\n",
        "    if val_m[\"f1\"] > best_val_f1:\n",
        "        best_val_f1 = val_m[\"f1\"]\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"cfg\": asdict(cfg),\n",
        "            \"best_val_f1\": best_val_f1\n",
        "        }, best_path)\n",
        "\n",
        "# save history.csv\n",
        "hist_csv = os.path.join(RUN_DIR, \"history.csv\")\n",
        "with open(hist_csv, \"w\") as f:\n",
        "    cols = list(history[0].keys())\n",
        "    f.write(\",\".join(cols) + \"\\n\")\n",
        "    for r in history:\n",
        "        f.write(\",\".join(str(r[c]) for c in cols) + \"\\n\")\n",
        "\n",
        "# training curves plot\n",
        "fig = plt.figure()\n",
        "plt.plot([h[\"epoch\"] for h in history], [h[\"train_loss\"] for h in history], label=\"Train loss\")\n",
        "plt.plot([h[\"epoch\"] for h in history], [h[\"val_f1\"] for h in history], label=\"Val F1\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.title(\"Training Curves\")\n",
        "plt.legend()\n",
        "fig.savefig(os.path.join(RUN_DIR, \"training_curves.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "\n",
        "# ============================================================\n",
        "# Final evaluation on TEST using BEST checkpoint\n",
        "# ============================================================\n",
        "ckpt = torch.load(best_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.to(device).eval()\n",
        "\n",
        "yt, pt, yt_pred = predict_probs(model, test_loader)\n",
        "test_m = compute_metrics(yt, pt, yt_pred)\n",
        "\n",
        "with open(os.path.join(RUN_DIR, \"metrics_test.json\"), \"w\") as f:\n",
        "    json.dump(test_m, f, indent=2)\n",
        "\n",
        "# save confusion + ROC/PR\n",
        "save_confusion_matrix(yt, yt_pred, os.path.join(RUN_DIR, \"confusion_matrix.png\"))\n",
        "save_roc_pr_curves(\n",
        "    yt, pt,\n",
        "    os.path.join(RUN_DIR, \"roc_curve.png\"),\n",
        "    os.path.join(RUN_DIR, \"pr_curve.png\")\n",
        ")\n",
        "\n",
        "print(\"\\n=== FINAL TEST METRICS (BEST VAL-F1 CHECKPOINT) ===\")\n",
        "print(json.dumps(test_m, indent=2))\n",
        "print(\"Run folder:\", RUN_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# ROBUSTNESS EVAL (same RUN_DIR)\n",
        "#   1) Resolution sweep\n",
        "#   2) JPEG compression sweep (proxy)\n",
        "# ============================================================\n",
        "def eval_loader_metrics(loader):\n",
        "    y, p, ypred = predict_probs(model, loader)\n",
        "    return compute_metrics(y, p, ypred)\n",
        "\n",
        "robust = {\"resolution_sweep\": [], \"jpeg_quality_sweep\": []}\n",
        "\n",
        "print(\"\\n[ROBUST] Resolution sweep...\")\n",
        "for s in cfg.rob_res_sizes:\n",
        "    ds = GradStreamDataset(\n",
        "        test_samples, cfg.num_frames, (s, s),\n",
        "        jpeg_quality=None,\n",
        "        max_items=cfg.rob_max_test_samples\n",
        "    )\n",
        "    ld = DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
        "    m = eval_loader_metrics(ld)\n",
        "    m[\"resize\"] = int(s)\n",
        "    robust[\"resolution_sweep\"].append(m)\n",
        "    print(f\"  resize={s:>3} -> acc={m['acc']:.3f} f1={m['f1']:.3f} auc={m['roc_auc']:.3f}\")\n",
        "\n",
        "print(\"\\n[ROBUST] JPEG compression sweep...\")\n",
        "for q in cfg.rob_jpeg_qual:\n",
        "    ds = GradStreamDataset(\n",
        "        test_samples, cfg.num_frames, cfg.resize_hw,\n",
        "        jpeg_quality=int(q),\n",
        "        max_items=cfg.rob_max_test_samples\n",
        "    )\n",
        "    ld = DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
        "    m = eval_loader_metrics(ld)\n",
        "    m[\"jpeg_quality\"] = int(q)\n",
        "    robust[\"jpeg_quality_sweep\"].append(m)\n",
        "    print(f\"  jpegQ={q:>3} -> acc={m['acc']:.3f} f1={m['f1']:.3f} auc={m['roc_auc']:.3f}\")\n",
        "\n",
        "# save robustness outputs\n",
        "with open(os.path.join(RUN_DIR, \"robustness.json\"), \"w\") as f:\n",
        "    json.dump(robust, f, indent=2)\n",
        "\n",
        "# CSVs\n",
        "with open(os.path.join(RUN_DIR, \"robustness_resolution.csv\"), \"w\") as f:\n",
        "    f.write(\"resize,acc,f1,roc_auc,ap,precision,recall\\n\")\n",
        "    for r in robust[\"resolution_sweep\"]:\n",
        "        f.write(f\"{r['resize']},{r['acc']},{r['f1']},{r['roc_auc']},{r['ap']},{r['precision']},{r['recall']}\\n\")\n",
        "\n",
        "with open(os.path.join(RUN_DIR, \"robustness_jpeg.csv\"), \"w\") as f:\n",
        "    f.write(\"jpeg_quality,acc,f1,roc_auc,ap,precision,recall\\n\")\n",
        "    for r in robust[\"jpeg_quality_sweep\"]:\n",
        "        f.write(f\"{r['jpeg_quality']},{r['acc']},{r['f1']},{r['roc_auc']},{r['ap']},{r['precision']},{r['recall']}\\n\")\n",
        "\n",
        "# Plots\n",
        "# Resolution plot\n",
        "res_x = [r[\"resize\"] for r in robust[\"resolution_sweep\"]]\n",
        "res_acc = [r[\"acc\"] for r in robust[\"resolution_sweep\"]]\n",
        "res_f1  = [r[\"f1\"] for r in robust[\"resolution_sweep\"]]\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(res_x, res_acc, marker=\"o\", label=\"Accuracy\")\n",
        "plt.plot(res_x, res_f1,  marker=\"o\", label=\"F1\")\n",
        "plt.xlabel(\"Resize (square)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Robustness vs Resolution (GRAD stream)\")\n",
        "plt.legend()\n",
        "fig.savefig(os.path.join(RUN_DIR, \"robustness_resolution.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "\n",
        "# JPEG plot\n",
        "jpg_x = [r[\"jpeg_quality\"] for r in robust[\"jpeg_quality_sweep\"]]\n",
        "jpg_acc = [r[\"acc\"] for r in robust[\"jpeg_quality_sweep\"]]\n",
        "jpg_f1  = [r[\"f1\"] for r in robust[\"jpeg_quality_sweep\"]]\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(jpg_x, jpg_acc, marker=\"o\", label=\"Accuracy\")\n",
        "plt.plot(jpg_x, jpg_f1,  marker=\"o\", label=\"F1\")\n",
        "plt.xlabel(\"JPEG quality (lower = more artifacts)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Robustness vs Compression (JPEG proxy)\")\n",
        "plt.legend()\n",
        "fig.savefig(os.path.join(RUN_DIR, \"robustness_compression.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"\\n[ROBUST] Saved robustness files into:\", RUN_DIR)\n",
        "print(\"DONE.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy pandas scikit-learn opencv-python-headless tqdm\n",
        "\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "RUN_DIR = \"/content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456\"\n",
        "train_csv = os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\")\n",
        "test_csv  = os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\")\n",
        "assert os.path.exists(train_csv) and os.path.exists(test_csv)\n",
        "\n",
        "OUT_JSON = os.path.join(RUN_DIR, \"pca_metrics_cluster_safe.json\")\n",
        "OUT_SCORES = os.path.join(RUN_DIR, \"pca_scores_cluster_safe.csv\")\n",
        "\n",
        "# Feature extraction params\n",
        "NUM_FRAMES = 16          # sample evenly across video\n",
        "RESIZE = (112, 112)      # small but stable\n",
        "EDGE_BINS = 256          # gradient magnitude histogram bins\n",
        "USE_ORIENT = True        # add orientation histogram too\n",
        "ORIENT_BINS = 36\n",
        "\n",
        "PCA_COMPONENTS = 128     # adjust (64/128/256) depending on compute\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# =========================\n",
        "# Load manifests\n",
        "# =========================\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df  = pd.read_csv(test_csv)\n",
        "\n",
        "# allow flexible column names\n",
        "def normalize_cols(df):\n",
        "    colmap = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if lc in [\"path\",\"filepath\",\"file\",\"video_path\"]:\n",
        "            colmap[c] = \"path\"\n",
        "        elif lc in [\"y\",\"label\",\"target\"]:\n",
        "            colmap[c] = \"y\"\n",
        "    df = df.rename(columns=colmap)\n",
        "    assert \"path\" in df.columns and \"y\" in df.columns\n",
        "    df[\"path\"] = df[\"path\"].astype(str)\n",
        "    df[\"y\"] = df[\"y\"].astype(int)\n",
        "    return df\n",
        "\n",
        "train_df = normalize_cols(train_df)\n",
        "test_df  = normalize_cols(test_df)\n",
        "\n",
        "print(\"Cluster-safe split:\")\n",
        "print(\" Train:\", len(train_df), \" Test:\", len(test_df))\n",
        "print(\" Train AI/REAL:\", int((train_df.y==1).sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\" Test  AI/REAL:\", int((test_df.y==1).sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "# =========================\n",
        "# Video utilities\n",
        "# =========================\n",
        "def read_uniform_frames(path, num_frames=16, resize=(112,112)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
        "    if n <= 0:\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    idxs = np.linspace(0, max(n-1, 0), num_frames).astype(int)\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = cv2.resize(fr, resize, interpolation=cv2.INTER_AREA)\n",
        "        fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "        frames.append(fr.astype(np.float32) / 255.0)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0)  # (T,H,W)\n",
        "\n",
        "def temporal_gradient_features(frames, edge_bins=256, use_orient=True, orient_bins=36):\n",
        "    \"\"\"\n",
        "    frames: (T,H,W) float32 in [0,1]\n",
        "    Output: fixed-length vector\n",
        "      - per timestep feature = [mag_hist (edge_bins), orient_hist (orient_bins)] (if enabled)\n",
        "      - aggregate across time with mean+std => 2 * D\n",
        "    \"\"\"\n",
        "    T, H, W = frames.shape\n",
        "\n",
        "    # temporal difference\n",
        "    dt = frames[1:] - frames[:-1]   # (T-1,H,W)\n",
        "    if dt.shape[0] == 0:\n",
        "        # edge case if video has <2 frames somehow\n",
        "        D = edge_bins + (orient_bins if use_orient else 0)\n",
        "        return np.zeros((2 * D,), dtype=np.float32)\n",
        "\n",
        "    per_t = []\n",
        "    for t in range(dt.shape[0]):\n",
        "        img = dt[t]\n",
        "\n",
        "        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
        "\n",
        "        mag = np.sqrt(gx*gx + gy*gy)\n",
        "        # normalize/clamp to stable histogram range\n",
        "        mag = np.clip(mag, 0, 1.0)\n",
        "\n",
        "        h_mag, _ = np.histogram(\n",
        "            mag.flatten(),\n",
        "            bins=edge_bins,\n",
        "            range=(0.0, 1.0),\n",
        "            density=True\n",
        "        )\n",
        "        h_mag = h_mag.astype(np.float32)\n",
        "\n",
        "        if use_orient:\n",
        "            ang = np.arctan2(gy, gx)              # [-pi, pi]\n",
        "            ang = (ang + np.pi) / (2*np.pi)       # [0, 1]\n",
        "            h_ang, _ = np.histogram(\n",
        "                ang.flatten(),\n",
        "                bins=orient_bins,\n",
        "                range=(0.0, 1.0),\n",
        "                density=True\n",
        "            )\n",
        "            h_ang = h_ang.astype(np.float32)\n",
        "            f_t = np.concatenate([h_mag, h_ang], axis=0)  # (edge_bins + orient_bins,)\n",
        "        else:\n",
        "            f_t = h_mag  # (edge_bins,)\n",
        "\n",
        "        per_t.append(f_t)\n",
        "\n",
        "    per_t = np.stack(per_t, axis=0)  # (T-1, D) fixed D now âœ…\n",
        "    mu = per_t.mean(axis=0)\n",
        "    sd = per_t.std(axis=0)\n",
        "    return np.concatenate([mu, sd], axis=0).astype(np.float32)\n",
        "\n",
        "def extract_features(df, desc):\n",
        "    X, y, bad = [], [], 0\n",
        "    for p, lab in tqdm(df[[\"path\",\"y\"]].values, desc=desc):\n",
        "        fr = read_uniform_frames(p, num_frames=NUM_FRAMES, resize=RESIZE)\n",
        "        if fr is None:\n",
        "            bad += 1\n",
        "            continue\n",
        "        f = temporal_gradient_features(fr, edge_bins=EDGE_BINS, use_orient=USE_ORIENT, orient_bins=ORIENT_BINS)\n",
        "        X.append(f)\n",
        "        y.append(int(lab))\n",
        "    X = np.stack(X, axis=0) if len(X) else np.zeros((0,1), dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    return X, y, bad\n",
        "\n",
        "# =========================\n",
        "# Extract feats\n",
        "# =========================\n",
        "Xtr, ytr, bad_tr = extract_features(train_df, \"PCA train feats\")\n",
        "Xte, yte, bad_te = extract_features(test_df,  \"PCA test feats\")\n",
        "print(\"\\nFeature shapes:\", Xtr.shape, Xte.shape, \"| bad:\", bad_tr, bad_te)\n",
        "\n",
        "assert Xtr.shape[0] > 10 and Xte.shape[0] > 10, \"Too many failed reads; check video paths.\"\n",
        "\n",
        "# =========================\n",
        "# PCA detector: fit on TRAIN ONLY\n",
        "# Score = reconstruction error in PCA subspace\n",
        "# =========================\n",
        "# Standardize -> PCA\n",
        "scaler = StandardScaler()\n",
        "Xtr_s = scaler.fit_transform(Xtr)\n",
        "Xte_s = scaler.transform(Xte)\n",
        "\n",
        "pca = PCA(n_components=min(PCA_COMPONENTS, Xtr_s.shape[1], Xtr_s.shape[0]-1), random_state=SEED)\n",
        "Ztr = pca.fit_transform(Xtr_s)\n",
        "Zte = pca.transform(Xte_s)\n",
        "\n",
        "Xtr_rec = pca.inverse_transform(Ztr)\n",
        "Xte_rec = pca.inverse_transform(Zte)\n",
        "\n",
        "# reconstruction error (higher => \"more out-of-subspace\")\n",
        "err_tr = np.mean((Xtr_s - Xtr_rec)**2, axis=1)\n",
        "err_te = np.mean((Xte_s - Xte_rec)**2, axis=1)\n",
        "\n",
        "# Convert error into probability via logistic regression trained on TRAIN scores only\n",
        "# (This avoids hand-picking thresholds)\n",
        "lr = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=SEED)\n",
        "lr.fit(err_tr.reshape(-1,1), ytr)\n",
        "\n",
        "proba = lr.predict_proba(err_te.reshape(-1,1))[:,1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(yte, pred)\n",
        "f1  = f1_score(yte, pred)\n",
        "auc = roc_auc_score(yte, proba)\n",
        "ap  = average_precision_score(yte, proba)\n",
        "\n",
        "print(\"\\n=== PCA DETECTOR (Cluster-safe split) ===\")\n",
        "print(\"ACC:\", round(acc,4))\n",
        "print(\"F1 :\", round(f1,4))\n",
        "print(\"AUC:\", round(auc,4))\n",
        "print(\"AP :\", round(ap,4))\n",
        "\n",
        "# save scores\n",
        "out = pd.DataFrame({\"path\": test_df[\"path\"].values[:len(proba)], \"y\": yte, \"score\": proba})\n",
        "out.to_csv(OUT_SCORES, index=False)\n",
        "\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"acc\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"roc_auc\": float(auc),\n",
        "        \"ap\": float(ap),\n",
        "        \"num_train\": int(Xtr.shape[0]),\n",
        "        \"num_test\": int(Xte.shape[0]),\n",
        "        \"bad_train\": int(bad_tr),\n",
        "        \"bad_test\": int(bad_te),\n",
        "        \"pca_components\": int(pca.n_components_),\n",
        "        \"feat_dim\": int(Xtr.shape[1]),\n",
        "        \"num_frames\": int(NUM_FRAMES),\n",
        "        \"resize\": list(RESIZE),\n",
        "        \"edge_bins\": int(EDGE_BINS),\n",
        "        \"use_orient\": bool(USE_ORIENT),\n",
        "        \"orient_bins\": int(ORIENT_BINS)\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" \", OUT_SCORES)\n",
        "print(\" \", OUT_JSON)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYZo-sbjX8y",
        "outputId": "53caab14-d52f-4fc9-9b6a-73d059671ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster-safe split:\n",
            " Train: 1887  Test: 483\n",
            " Train AI/REAL: 943 / 944\n",
            " Test  AI/REAL: 248 / 235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PCA train feats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1887/1887 [28:16<00:00,  1.11it/s]\n",
            "PCA test feats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [07:07<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature shapes: (1887, 584) (483, 584) | bad: 0 0\n",
            "\n",
            "=== PCA DETECTOR (Cluster-safe split) ===\n",
            "ACC: 0.5072\n",
            "F1 : 0.464\n",
            "AUC: 0.5391\n",
            "AP : 0.5662\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/pca_scores_cluster_safe.csv\n",
            "  /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/pca_metrics_cluster_safe.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BENCHMARK_NEUTRAL_ALLMETHODS (MEANINGFUL SIZE, FIXED STREAMS)\n",
        "# - NO PCA, NO GINC\n",
        "# - Uses corrected streams:\n",
        "#     midtone, midgrad (grad on midtones), grad_hf (HF of grad), tgrad, freq\n",
        "# - disallow freq+high combos (not included)\n",
        "# - Larger dataset for meaningful results\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm transformers accelerate\n",
        "\n",
        "import os, cv2, json, random, hashlib, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models as tvm\n",
        "\n",
        "from transformers import TimesformerForVideoClassification\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "print(\"Torch device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "RUN_NAME = \"BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC\"\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "\n",
        "# âœ… meaningful size (change to 50, 100, 150)\n",
        "N_EACH = 100\n",
        "\n",
        "# oversample factor to survive dedup (then rebalance back to exact N_EACH each)\n",
        "RAW_MULT = 1.4\n",
        "RAW_EACH = int(np.ceil(N_EACH * RAW_MULT))\n",
        "\n",
        "# Normalization (metadata-neutral)\n",
        "NORM_FPS   = 25\n",
        "NORM_W     = 256\n",
        "NORM_H     = 256\n",
        "NORM_CODEC = \"libx264\"\n",
        "NORM_CRF   = 23\n",
        "NORM_PIX   = \"yuv420p\"\n",
        "NORM_AUDIO = False\n",
        "\n",
        "# Model input sampling\n",
        "T = 8\n",
        "FRAME_STRIDE = 3\n",
        "TARGET_SIZE  = 224\n",
        "BATCH_SIZE   = 6          # bump slightly since dataset is larger\n",
        "EPOCHS       = 3          # keep 3 for speed; increase to 5 later\n",
        "LR           = 3e-4\n",
        "NUM_WORKERS  = 2\n",
        "\n",
        "# Midtone thresholds\n",
        "MID_LO = 0.25\n",
        "MID_HI = 0.75\n",
        "\n",
        "# Streams to test (NO freq+high)\n",
        "STREAMS_FOR_R2RES = [\"mid\", \"midtone\", \"midgrad\", \"grad_hf\", \"freq\", \"tgrad\"]\n",
        "STREAMS_FOR_TRANSFORMERS = [\"mid\", \"freq\", \"midtone\", \"midgrad\", \"grad_hf\", \"tgrad\"]\n",
        "\n",
        "# Fusion runs that make sense (NO high+freq)\n",
        "FUSION_RUNS = [\n",
        "    [\"mid\", \"midgrad\"],\n",
        "    [\"midgrad\", \"freq\"],\n",
        "    [\"midgrad\", \"grad_hf\"],\n",
        "    [\"midgrad\", \"tgrad\"],\n",
        "    [\"mid\", \"freq\", \"midgrad\", \"grad_hf\", \"tgrad\"],\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# 1) LOAD VIDEO PATHS (RAW_EACH + RAW_EACH)\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\",\".webm\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "assert len(real) >= RAW_EACH and (len(ai_core)+len(sora)) >= RAW_EACH, \"Not enough videos.\"\n",
        "\n",
        "AI_ALL = ai_core + sora\n",
        "AI = random.sample(AI_ALL, RAW_EACH)\n",
        "REAL = random.sample(real, RAW_EACH)\n",
        "\n",
        "raw_df = pd.DataFrame({\n",
        "    \"path\": AI + REAL,\n",
        "    \"y\":    [1]*len(AI) + [0]*len(REAL),\n",
        "    \"src\":  [\"AI\"]*len(AI) + [\"REAL\"]*len(REAL),\n",
        "}).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "raw_manifest = os.path.join(RUN_DIR, f\"raw_manifest_{RAW_EACH}each.csv\")\n",
        "raw_df.to_csv(raw_manifest, index=False)\n",
        "print(\"\\nRAW pool:\", len(raw_df), \"total | AI=\", (raw_df.y==1).sum(), \"REAL=\", (raw_df.y==0).sum())\n",
        "print(\"Saved:\", raw_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) HASHING FOR DEDUP\n",
        "# -----------------------------\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path)\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    fr = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return h\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "# -----------------------------\n",
        "# 3) DEDUP (Exact + Near)\n",
        "# -----------------------------\n",
        "print(\"\\nComputing hashes...\")\n",
        "sha_list, dh_list = [], []\n",
        "for p in tqdm(raw_df.path.tolist(), desc=\"Hashing\"):\n",
        "    sha_list.append(sha1_first_frame(p))\n",
        "    dh_list.append(dhash_first_frame(p))\n",
        "\n",
        "df = raw_df.copy()\n",
        "df[\"sha1\"] = sha_list\n",
        "df[\"dhash\"] = dh_list\n",
        "df = df.dropna(subset=[\"sha1\",\"dhash\"]).reset_index(drop=True)\n",
        "\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"sha1\"]).reset_index(drop=True)\n",
        "print(\"Exact dupes removed:\", before - len(df))\n",
        "\n",
        "def uf_make(n):\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb: return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "    return find, union\n",
        "\n",
        "def bucket_key(h, bits=12):\n",
        "    return int(h) >> (64 - bits)\n",
        "\n",
        "bucket = defaultdict(list)\n",
        "for i, h in enumerate(df.dhash.values):\n",
        "    bucket[bucket_key(h)].append(i)\n",
        "\n",
        "find, union = uf_make(len(df))\n",
        "for _, idxs in bucket.items():\n",
        "    if len(idxs) < 2:\n",
        "        continue\n",
        "    # keep fast if a bucket is huge\n",
        "    if len(idxs) > 250:\n",
        "        idxs = random.sample(idxs, 250)\n",
        "\n",
        "    for ii in range(len(idxs)):\n",
        "        a = idxs[ii]\n",
        "        ha = int(df.loc[a,\"dhash\"])\n",
        "        for jj in range(ii+1, len(idxs)):\n",
        "            b = idxs[jj]\n",
        "            hb = int(df.loc[b,\"dhash\"])\n",
        "            if hamming(ha, hb) <= 3:\n",
        "                union(a, b)\n",
        "\n",
        "df[\"cluster\"] = [find(i) for i in range(len(df))]\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"cluster\"]).reset_index(drop=True)\n",
        "print(\"Near-dupes removed:\", before - len(df))\n",
        "print(\"After dedup:\", len(df))\n",
        "\n",
        "# âœ… rebalance AFTER dedup to exact N_EACH each\n",
        "df_ai = df[df.y==1].sample(n=min(N_EACH, (df.y==1).sum()), random_state=SEED)\n",
        "df_rl = df[df.y==0].sample(n=min(N_EACH, (df.y==0).sum()), random_state=SEED)\n",
        "\n",
        "assert len(df_ai) >= N_EACH and len(df_rl) >= N_EACH, (\n",
        "    f\"After dedup you have AI={len(df_ai)} REAL={len(df_rl)}. \"\n",
        "    f\"Increase RAW_MULT or lower N_EACH.\"\n",
        ")\n",
        "\n",
        "df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "print(\"Rebalanced:\", len(df), \"total | AI=\", (df.y==1).sum(), \"REAL=\", (df.y==0).sum())\n",
        "\n",
        "dedup_manifest = os.path.join(RUN_DIR, \"dedup_manifest.csv\")\n",
        "df.to_csv(dedup_manifest, index=False)\n",
        "print(\"Saved:\", dedup_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) NORMALIZE\n",
        "# -----------------------------\n",
        "NORM_DIR = os.path.join(RUN_DIR, \"normalized\")\n",
        "os.makedirs(NORM_DIR, exist_ok=True)\n",
        "\n",
        "def norm_path_for(i):\n",
        "    return os.path.join(NORM_DIR, f\"v_{i:05d}.mp4\")\n",
        "\n",
        "def ffmpeg_normalize(in_path, out_path):\n",
        "    aflag = [\"-an\"] if not NORM_AUDIO else []\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-vf\", f\"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,fps={NORM_FPS}\",\n",
        "        \"-vsync\",\"cfr\",\n",
        "        *aflag,\n",
        "        \"-c:v\", NORM_CODEC,\n",
        "        \"-crf\", str(NORM_CRF),\n",
        "        \"-pix_fmt\", NORM_PIX,\n",
        "        out_path\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "\n",
        "print(\"\\nNormalizing...\")\n",
        "norm_paths = []\n",
        "for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=\"Normalize\")):\n",
        "    outp = norm_path_for(i)\n",
        "    if not os.path.exists(outp):\n",
        "        ffmpeg_normalize(row.path, outp)\n",
        "    norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)\n",
        "\n",
        "ndf = df.copy()\n",
        "ndf[\"norm_path\"] = norm_paths\n",
        "ndf = ndf.dropna(subset=[\"norm_path\"]).reset_index(drop=True)\n",
        "\n",
        "norm_manifest = os.path.join(RUN_DIR, \"normalized_manifest.csv\")\n",
        "ndf.to_csv(norm_manifest, index=False)\n",
        "print(\"Normalized:\", len(ndf), \"Saved:\", norm_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) CLUSTER-SAFE SPLIT\n",
        "# -----------------------------\n",
        "clusters = ndf.cluster.values\n",
        "cluster_to_rows = defaultdict(list)\n",
        "for i,c in enumerate(clusters):\n",
        "    cluster_to_rows[int(c)].append(i)\n",
        "\n",
        "cluster_ids = list(cluster_to_rows.keys())\n",
        "cluster_labels = []\n",
        "for c in cluster_ids:\n",
        "    ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]\n",
        "    cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)\n",
        "\n",
        "c_train, c_test = train_test_split(\n",
        "    cluster_ids, test_size=0.3, random_state=SEED, stratify=cluster_labels\n",
        ")\n",
        "\n",
        "train_idx, test_idx = [], []\n",
        "for c in c_train: train_idx.extend(cluster_to_rows[c])\n",
        "for c in c_test:  test_idx.extend(cluster_to_rows[c])\n",
        "\n",
        "train_df = ndf.iloc[train_idx].reset_index(drop=True)\n",
        "test_df  = ndf.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCluster-safe split:\")\n",
        "print(\" Train:\", len(train_df), \" Test:\", len(test_df))\n",
        "print(\" Train AI/REAL:\", int((train_df.y==1).sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\" Test  AI/REAL:\", int((test_df.y==1).sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "train_df.to_csv(os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) STREAMS (CORRECTED)\n",
        "# -----------------------------\n",
        "def to_rgb(frame_bgr):\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def resize_center(frame, size):\n",
        "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def luma01_from_bgr(fr_bgr):\n",
        "    fr = fr_bgr.astype(np.float32) / 255.0\n",
        "    b,g,r = fr[:,:,0], fr[:,:,1], fr[:,:,2]\n",
        "    return (0.114*b + 0.587*g + 0.299*r).astype(np.float32)\n",
        "\n",
        "def midtone_mask(luma01, lo=MID_LO, hi=MID_HI):\n",
        "    return ((luma01 >= lo) & (luma01 <= hi)).astype(np.float32)\n",
        "\n",
        "def sobel_mag01(gray01):\n",
        "    g8 = (gray01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    gx = cv2.Sobel(g8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(g8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    mag = cv2.magnitude(gx, gy)\n",
        "    mag = mag / (np.max(mag) + 1e-8)\n",
        "    return mag.astype(np.float32)\n",
        "\n",
        "def fft_highpass_map01(img01, frac_hi=0.45):\n",
        "    f = np.fft.fft2(img01.astype(np.float32))\n",
        "    f = np.fft.fftshift(f)\n",
        "    H,W = img01.shape\n",
        "    cy,cx = H//2, W//2\n",
        "    yy,xx = np.ogrid[:H,:W]\n",
        "    r = np.sqrt((yy-cy)**2 + (xx-cx)**2)\n",
        "    rmax = np.max(r) + 1e-8\n",
        "    hp = (r >= frac_hi*rmax).astype(np.float32)\n",
        "    f_hp = f * hp\n",
        "    f_hp = np.fft.ifftshift(f_hp)\n",
        "    out = np.fft.ifft2(f_hp)\n",
        "    out = np.real(out).astype(np.float32)\n",
        "    out = out - np.min(out)\n",
        "    out = out / (np.max(out) + 1e-8)\n",
        "    return out\n",
        "\n",
        "def map01_to_bgr(map01):\n",
        "    x = (map01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    return cv2.cvtColor(x, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def stream_mid(fr):\n",
        "    return fr\n",
        "\n",
        "def stream_midtone(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    return map01_to_bgr(mid)\n",
        "\n",
        "def stream_midgrad(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    return map01_to_bgr(mag)\n",
        "\n",
        "def stream_grad_hf(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    hf = fft_highpass_map01(mag, frac_hi=0.45)\n",
        "    return map01_to_bgr(hf)\n",
        "\n",
        "def stream_freq(fr):\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    mag = np.log1p(np.abs(fshift))\n",
        "    mag = mag / (mag.max() + 1e-6)\n",
        "    return map01_to_bgr(mag.astype(np.float32))\n",
        "\n",
        "def stream_tgrad(fr_prev, fr_cur):\n",
        "    g_prev = cv2.cvtColor(stream_midgrad(fr_prev), cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    g_cur  = cv2.cvtColor(stream_midgrad(fr_cur),  cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    diff = np.abs(g_cur - g_prev)\n",
        "    diff = diff / (diff.max() + 1e-8)\n",
        "    return map01_to_bgr(diff)\n",
        "\n",
        "STREAM_FN = {\n",
        "    \"mid\": stream_mid,\n",
        "    \"midtone\": stream_midtone,\n",
        "    \"midgrad\": stream_midgrad,\n",
        "    \"grad_hf\": stream_grad_hf,\n",
        "    \"freq\": stream_freq,\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 7) DATASETS\n",
        "# -----------------------------\n",
        "def open_video(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    return cap if cap.isOpened() else None\n",
        "\n",
        "def sample_clip_frames(path, T=8, stride=3, size=224):\n",
        "    cap = open_video(path)\n",
        "    if cap is None:\n",
        "        return None\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    if n <= 0:\n",
        "        cap.release()\n",
        "        return None\n",
        "    max_start = max(0, n - (T-1)*stride - 1)\n",
        "    start = random.randint(0, max_start) if max_start > 0 else 0\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        idx = start + i*stride\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = resize_center(fr, size)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "class VideoStreamDataset(Dataset):\n",
        "    def __init__(self, df, stream=\"mid\", T=8, stride=3, size=224):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.stream = stream\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.norm_path.iloc[idx]\n",
        "        y = int(self.df.y.iloc[idx])\n",
        "        frames = sample_clip_frames(path, T=self.T, stride=self.stride, size=self.size)\n",
        "\n",
        "        if frames is None:\n",
        "            x = np.zeros((self.T, 3, self.size, self.size), dtype=np.float32)\n",
        "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        if self.stream in STREAM_FN:\n",
        "            proc = [STREAM_FN[self.stream](fr) for fr in frames]\n",
        "        elif self.stream == \"tgrad\":\n",
        "            proc = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "        else:\n",
        "            proc = frames\n",
        "\n",
        "        arr = np.stack([to_rgb(fr) for fr in proc], axis=0).astype(np.float32)/255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))\n",
        "        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class MultiStreamDataset(Dataset):\n",
        "    def __init__(self, df, streams, T=8, stride=3, size=224):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.streams = list(streams)\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.norm_path.iloc[idx]\n",
        "        y = int(self.df.y.iloc[idx])\n",
        "        frames = sample_clip_frames(path, T=self.T, stride=self.stride, size=self.size)\n",
        "\n",
        "        z = torch.zeros((self.T,3,self.size,self.size), dtype=torch.float32)\n",
        "        if frames is None:\n",
        "            out = {s: z for s in self.streams}\n",
        "            return out, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        out = {}\n",
        "        for s in self.streams:\n",
        "            if s == \"tgrad\":\n",
        "                seq = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "            else:\n",
        "                seq = [STREAM_FN[s](fr) for fr in frames]\n",
        "\n",
        "            arr = np.stack([to_rgb(fr) for fr in seq], axis=0).astype(np.float32)/255.0\n",
        "            arr = np.transpose(arr, (0,3,1,2))\n",
        "            out[s] = torch.from_numpy(arr)\n",
        "\n",
        "        return out, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) MODELS\n",
        "# -----------------------------\n",
        "class R2ResNetFusion(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)\n",
        "        self.r2.fc = nn.Identity()\n",
        "        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "        self.res.fc = nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        xr2 = x.permute(0,2,1,3,4)\n",
        "        f_r2 = self.r2(xr2)\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.res(xf).view(B, T, -1).mean(dim=1)\n",
        "        feat = torch.cat([f_r2, f], dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "class MultiResNetFusion(nn.Module):\n",
        "    def __init__(self, streams, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.streams = list(streams)\n",
        "        self.backbones = nn.ModuleDict()\n",
        "        for s in self.streams:\n",
        "            res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "            res.fc = nn.Identity()\n",
        "            self.backbones[s] = res\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512*len(self.streams), 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_dict):\n",
        "        feats = []\n",
        "        for s in self.streams:\n",
        "            x = batch_dict[s]\n",
        "            B,T,C,H,W = x.shape\n",
        "            xf = x.reshape(B*T, C, H, W)\n",
        "            f = self.backbones[s](xf).view(B, T, -1).mean(dim=1)\n",
        "            feats.append(f)\n",
        "        feat = torch.cat(feats, dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "class ViTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.vit = tvm.vit_b_16(weights=tvm.ViT_B_16_Weights.DEFAULT)\n",
        "        self.vit.heads = nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.vit(xf).view(B, T, -1).mean(dim=1)\n",
        "        return self.head(f)\n",
        "\n",
        "class TimeSformerClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.model = TimesformerForVideoClassification.from_pretrained(\n",
        "            \"facebook/timesformer-base-finetuned-k400\",\n",
        "            num_labels=num_classes,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(pixel_values=x).logits\n",
        "\n",
        "# -----------------------------\n",
        "# 9) TRAIN / EVAL\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, proba):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, pred)),\n",
        "        \"f1\": float(f1_score(y_true, pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"ap\": float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader, is_multi=False):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for batch in loader:\n",
        "        if is_multi:\n",
        "            x_dict, y = batch\n",
        "            x_dict = {k:v.to(DEVICE) for k,v in x_dict.items()}\n",
        "            logits = model(x_dict)\n",
        "        else:\n",
        "            x, y = batch\n",
        "            x = x.to(DEVICE)\n",
        "            logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy())\n",
        "        all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return compute_metrics(y_true, proba)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=3, is_multi=False, name=\"model\"):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses=[]\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            if is_multi:\n",
        "                x_dict, y = batch\n",
        "                x_dict = {k:v.to(DEVICE) for k,v in x_dict.items()}\n",
        "                y = y.to(DEVICE)\n",
        "                logits = model(x_dict)\n",
        "            else:\n",
        "                x, y = batch\n",
        "                x = x.to(DEVICE)\n",
        "                y = y.to(DEVICE)\n",
        "                logits = model(x)\n",
        "\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        m = eval_model(model, test_loader, is_multi=is_multi)\n",
        "        print(f\"{name} | ep {ep}/{epochs} | loss={np.mean(losses):.4f} | test_f1={m['f1']:.3f} auc={m['roc_auc']:.3f} acc={m['acc']:.3f}\")\n",
        "\n",
        "        if m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return eval_model(model, test_loader, is_multi=is_multi)\n",
        "\n",
        "# -----------------------------\n",
        "# 10) RUN BENCHMARKS\n",
        "# -----------------------------\n",
        "results = []\n",
        "\n",
        "def run_single_stream(model_kind, stream):\n",
        "    ds_tr = VideoStreamDataset(train_df, stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = VideoStreamDataset(test_df,  stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    if model_kind == \"r2res\":\n",
        "        model = R2ResNetFusion()\n",
        "        name = f\"R2Res_{stream}\"\n",
        "    elif model_kind == \"vit\":\n",
        "        model = ViTClassifier()\n",
        "        name = f\"ViT_{stream}\"\n",
        "    elif model_kind == \"timesformer\":\n",
        "        model = TimeSformerClassifier()\n",
        "        name = f\"TS_{stream}\"\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    print(\"\\n===================== RUN:\", name, \"=====================\")\n",
        "    met = train_model(model, tr, te, epochs=EPOCHS, is_multi=False, name=name)\n",
        "    met[\"model\"] = name\n",
        "    results.append(met)\n",
        "\n",
        "def run_fusion(streams):\n",
        "    ds_tr = MultiStreamDataset(train_df, streams=streams, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = MultiStreamDataset(test_df,  streams=streams, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    model = MultiResNetFusion(streams=streams)\n",
        "    name = \"FUSION_\" + \"+\".join(streams)\n",
        "\n",
        "    print(\"\\n===================== RUN:\", name, \"=====================\")\n",
        "    met = train_model(model, tr, te, epochs=EPOCHS, is_multi=True, name=name)\n",
        "    met[\"model\"] = name\n",
        "    results.append(met)\n",
        "\n",
        "for s in STREAMS_FOR_R2RES:\n",
        "    run_single_stream(\"r2res\", s)\n",
        "\n",
        "for s in STREAMS_FOR_TRANSFORMERS:\n",
        "    run_single_stream(\"vit\", s)\n",
        "\n",
        "for s in STREAMS_FOR_TRANSFORMERS:\n",
        "    run_single_stream(\"timesformer\", s)\n",
        "\n",
        "for streams in FUSION_RUNS:\n",
        "    run_fusion(streams)\n",
        "\n",
        "# -----------------------------\n",
        "# 11) SAVE SUMMARY\n",
        "# -----------------------------\n",
        "res_df = pd.DataFrame(results).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
        "summary_csv = os.path.join(RUN_DIR, \"SUMMARY.csv\")\n",
        "res_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "print(\"\\n==== FINAL SUMMARY (sorted by F1) ====\")\n",
        "print(res_df[[\"model\",\"acc\",\"f1\",\"roc_auc\",\"ap\"]].head(50))\n",
        "print(\"\\nSaved summary to:\", summary_csv)\n",
        "print(\"Run folder:\", RUN_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "682074df702d4d3795e274fa505ee326",
            "7bed3d2411d845519ad123b1ce88f851",
            "a997e5adfdab40e297039c2de8d5ffe9",
            "5211205ba45b4fa7aa4bdfe283630da2",
            "da7ed0baf11b490fbb8ac28af7565c1d",
            "cf8385ba69874f2085a94b27ddf7d66e",
            "04c3800e6bff49928b7103059f1b2142",
            "87c24ef86e254453aa5036c0ce03a314",
            "a4a6beca900249ce934c99e857a2800a",
            "ca8a20c44a6e4286863cd2e0dd0b8972",
            "3c3970a5f776461f989c99b2402302ac",
            "03c6743e8627466688900eee7fc14232",
            "9ce747e04020442ca9a2b5e971158bb3",
            "26a4f5d7961d4e66819a4c87d50fbd74",
            "f8f1d8d88fc44fb585abfe4dad945995",
            "c41472a91a4b4926973fb2ba23f1dfd7",
            "33d4adcc05dd4db7ab9c116fce296607",
            "8838fc22c20f4e07871e9145108951e7",
            "297bdcd3b8474ef4af49f14894ecdf49",
            "248dd61e709843e78b7bcf18f81502b3",
            "7d05f8c034144fb2a6b3f53ab4b51232",
            "74b3562c68694e33b2b3437af9fef979",
            "928db6d483464f60b9eef1b0d7888f8b",
            "5ccdf8c51fcb487097fa35a8ea2c570a",
            "0ec462c589d34d66a5b98f1163ebb9b3",
            "c21cc744ceb04b8ab3f9c52a7071d2aa",
            "fd8927427a2e438984d01be2ec8a3876",
            "a798b64eb2d8429e8b6a3970f9c00f87",
            "8795b643af4d46ebb901a248ebdfad99",
            "d484cba1615f45418294fdf5ad5f622d",
            "b67a6ed8e47e4165a3ee1c00807be46c",
            "b97716577e01445c9c731c6edafd0249",
            "3ba94841c8c1407f8a904ca7a9a21f72"
          ]
        },
        "id": "xL-KO9HXJ4UP",
        "outputId": "90623e3c-daf5-431d-ec5f-9ad5da288283"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "GPU: NVIDIA L4\n",
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "RAW pool: 280 total | AI= 140 REAL= 140\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC/raw_manifest_140each.csv\n",
            "\n",
            "Computing hashes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280/280 [07:38<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact dupes removed: 0\n",
            "Near-dupes removed: 1\n",
            "After dedup: 278\n",
            "Rebalanced: 200 total | AI= 100 REAL= 100\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC/dedup_manifest.csv\n",
            "\n",
            "Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 105.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: 200 Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC/normalized_manifest.csv\n",
            "\n",
            "Cluster-safe split:\n",
            " Train: 140  Test: 60\n",
            " Train AI/REAL: 70 / 70\n",
            " Test  AI/REAL: 30 / 30\n",
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 208MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: R2Res_mid =====================\n",
            "R2Res_mid | ep 1/3 | loss=0.5849 | test_f1=0.836 auc=0.968 acc=0.850\n",
            "R2Res_mid | ep 2/3 | loss=0.1914 | test_f1=0.857 auc=1.000 acc=0.833\n",
            "R2Res_mid | ep 3/3 | loss=0.1875 | test_f1=0.912 auc=0.988 acc=0.917\n",
            "\n",
            "===================== RUN: R2Res_midtone =====================\n",
            "R2Res_midtone | ep 1/3 | loss=0.4459 | test_f1=0.873 auc=0.963 acc=0.883\n",
            "R2Res_midtone | ep 2/3 | loss=0.2965 | test_f1=0.929 auc=0.953 acc=0.933\n",
            "R2Res_midtone | ep 3/3 | loss=0.4200 | test_f1=0.923 auc=1.000 acc=0.917\n",
            "\n",
            "===================== RUN: R2Res_midgrad =====================\n",
            "R2Res_midgrad | ep 1/3 | loss=0.5342 | test_f1=0.743 auc=0.927 acc=0.700\n",
            "R2Res_midgrad | ep 2/3 | loss=0.4967 | test_f1=0.912 auc=0.980 acc=0.917\n",
            "R2Res_midgrad | ep 3/3 | loss=0.2552 | test_f1=0.909 auc=0.997 acc=0.917\n",
            "\n",
            "===================== RUN: R2Res_grad_hf =====================\n",
            "R2Res_grad_hf | ep 1/3 | loss=0.5189 | test_f1=0.735 auc=0.882 acc=0.700\n",
            "R2Res_grad_hf | ep 2/3 | loss=0.3370 | test_f1=0.881 auc=0.951 acc=0.883\n",
            "R2Res_grad_hf | ep 3/3 | loss=0.3656 | test_f1=0.885 auc=0.956 acc=0.883\n",
            "\n",
            "===================== RUN: R2Res_freq =====================\n",
            "R2Res_freq | ep 1/3 | loss=0.5276 | test_f1=0.421 auc=0.893 acc=0.633\n",
            "R2Res_freq | ep 2/3 | loss=0.4406 | test_f1=0.779 auc=0.973 acc=0.717\n",
            "R2Res_freq | ep 3/3 | loss=0.4404 | test_f1=0.846 auc=0.971 acc=0.867\n",
            "\n",
            "===================== RUN: R2Res_tgrad =====================\n",
            "R2Res_tgrad | ep 1/3 | loss=0.5005 | test_f1=0.696 auc=0.949 acc=0.767\n",
            "R2Res_tgrad | ep 2/3 | loss=0.4193 | test_f1=0.833 auc=0.964 acc=0.800\n",
            "R2Res_tgrad | ep 3/3 | loss=0.2911 | test_f1=0.893 auc=0.972 acc=0.900\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330M/330M [00:02<00:00, 164MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: ViT_mid =====================\n",
            "ViT_mid | ep 1/3 | loss=0.7297 | test_f1=0.286 auc=0.712 acc=0.583\n",
            "ViT_mid | ep 2/3 | loss=0.4620 | test_f1=0.868 auc=0.890 acc=0.883\n",
            "ViT_mid | ep 3/3 | loss=0.4821 | test_f1=0.757 auc=0.676 acc=0.700\n",
            "\n",
            "===================== RUN: ViT_freq =====================\n",
            "ViT_freq | ep 1/3 | loss=0.7011 | test_f1=0.286 auc=0.592 acc=0.583\n",
            "ViT_freq | ep 2/3 | loss=0.6755 | test_f1=0.286 auc=0.524 acc=0.583\n",
            "ViT_freq | ep 3/3 | loss=0.6560 | test_f1=0.286 auc=0.736 acc=0.583\n",
            "\n",
            "===================== RUN: ViT_midtone =====================\n",
            "ViT_midtone | ep 1/3 | loss=0.7774 | test_f1=0.000 auc=0.737 acc=0.500\n",
            "ViT_midtone | ep 2/3 | loss=0.6613 | test_f1=0.649 auc=0.770 acc=0.567\n",
            "ViT_midtone | ep 3/3 | loss=0.5275 | test_f1=0.784 auc=0.850 acc=0.817\n",
            "\n",
            "===================== RUN: ViT_midgrad =====================\n",
            "ViT_midgrad | ep 1/3 | loss=0.7330 | test_f1=0.000 auc=0.886 acc=0.500\n",
            "ViT_midgrad | ep 2/3 | loss=0.6073 | test_f1=0.776 auc=0.903 acc=0.750\n",
            "ViT_midgrad | ep 3/3 | loss=0.4390 | test_f1=0.868 auc=0.907 acc=0.883\n",
            "\n",
            "===================== RUN: ViT_grad_hf =====================\n",
            "ViT_grad_hf | ep 1/3 | loss=0.7234 | test_f1=0.667 auc=0.514 acc=0.500\n",
            "ViT_grad_hf | ep 2/3 | loss=0.6719 | test_f1=0.286 auc=0.483 acc=0.583\n",
            "ViT_grad_hf | ep 3/3 | loss=0.6360 | test_f1=0.286 auc=0.479 acc=0.583\n",
            "\n",
            "===================== RUN: ViT_tgrad =====================\n",
            "ViT_tgrad | ep 1/3 | loss=0.7336 | test_f1=0.000 auc=0.742 acc=0.500\n",
            "ViT_tgrad | ep 2/3 | loss=0.7005 | test_f1=0.667 auc=0.692 acc=0.500\n",
            "ViT_tgrad | ep 3/3 | loss=0.7182 | test_f1=0.658 auc=0.669 acc=0.567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "682074df702d4d3795e274fa505ee326"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03c6743e8627466688900eee7fc14232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_mid =====================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "928db6d483464f60b9eef1b0d7888f8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TS_mid | ep 1/3 | loss=0.8835 | test_f1=0.909 auc=0.971 acc=0.917\n",
            "TS_mid | ep 2/3 | loss=0.2310 | test_f1=0.912 auc=0.982 acc=0.917\n",
            "TS_mid | ep 3/3 | loss=0.1592 | test_f1=0.966 auc=1.000 acc=0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_freq =====================\n",
            "TS_freq | ep 1/3 | loss=0.8713 | test_f1=0.000 auc=0.594 acc=0.500\n",
            "TS_freq | ep 2/3 | loss=0.7015 | test_f1=0.286 auc=0.561 acc=0.583\n",
            "TS_freq | ep 3/3 | loss=0.7033 | test_f1=0.378 auc=0.539 acc=0.617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_midtone =====================\n",
            "TS_midtone | ep 1/3 | loss=0.6982 | test_f1=0.421 auc=0.786 acc=0.633\n",
            "TS_midtone | ep 2/3 | loss=0.8324 | test_f1=0.652 auc=0.727 acc=0.483\n",
            "TS_midtone | ep 3/3 | loss=0.7423 | test_f1=0.667 auc=0.776 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_midgrad =====================\n",
            "TS_midgrad | ep 1/3 | loss=0.8820 | test_f1=0.690 auc=0.940 acc=0.550\n",
            "TS_midgrad | ep 2/3 | loss=0.4195 | test_f1=0.915 auc=0.990 acc=0.917\n",
            "TS_midgrad | ep 3/3 | loss=0.3251 | test_f1=0.938 auc=0.997 acc=0.933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_grad_hf =====================\n",
            "TS_grad_hf | ep 1/3 | loss=0.8609 | test_f1=0.000 auc=0.570 acc=0.500\n",
            "TS_grad_hf | ep 2/3 | loss=0.7033 | test_f1=0.333 auc=0.538 acc=0.600\n",
            "TS_grad_hf | ep 3/3 | loss=0.6950 | test_f1=0.286 auc=0.694 acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_tgrad =====================\n",
            "TS_tgrad | ep 1/3 | loss=0.8814 | test_f1=0.762 auc=0.853 acc=0.750\n",
            "TS_tgrad | ep 2/3 | loss=0.5527 | test_f1=0.571 auc=0.921 acc=0.700\n",
            "TS_tgrad | ep 3/3 | loss=0.4997 | test_f1=0.759 auc=0.940 acc=0.683\n",
            "\n",
            "===================== RUN: FUSION_mid+midgrad =====================\n",
            "FUSION_mid+midgrad | ep 1/3 | loss=0.6176 | test_f1=0.921 auc=0.960 acc=0.917\n",
            "FUSION_mid+midgrad | ep 2/3 | loss=0.2959 | test_f1=0.947 auc=0.994 acc=0.950\n",
            "FUSION_mid+midgrad | ep 3/3 | loss=0.3776 | test_f1=0.947 auc=0.993 acc=0.950\n",
            "\n",
            "===================== RUN: FUSION_midgrad+freq =====================\n",
            "FUSION_midgrad+freq | ep 1/3 | loss=0.4978 | test_f1=0.846 auc=0.967 acc=0.867\n",
            "FUSION_midgrad+freq | ep 2/3 | loss=0.3791 | test_f1=0.935 auc=0.977 acc=0.933\n",
            "FUSION_midgrad+freq | ep 3/3 | loss=0.2584 | test_f1=0.952 auc=0.994 acc=0.950\n",
            "\n",
            "===================== RUN: FUSION_midgrad+grad_hf =====================\n",
            "FUSION_midgrad+grad_hf | ep 1/3 | loss=0.5307 | test_f1=0.723 auc=0.881 acc=0.617\n",
            "FUSION_midgrad+grad_hf | ep 2/3 | loss=0.3872 | test_f1=0.857 auc=0.940 acc=0.850\n",
            "FUSION_midgrad+grad_hf | ep 3/3 | loss=0.2395 | test_f1=0.933 auc=0.987 acc=0.933\n",
            "\n",
            "===================== RUN: FUSION_midgrad+tgrad =====================\n",
            "FUSION_midgrad+tgrad | ep 1/3 | loss=0.4616 | test_f1=0.889 auc=0.971 acc=0.900\n",
            "FUSION_midgrad+tgrad | ep 2/3 | loss=0.3279 | test_f1=0.949 auc=0.981 acc=0.950\n",
            "FUSION_midgrad+tgrad | ep 3/3 | loss=0.3251 | test_f1=0.968 auc=0.998 acc=0.967\n",
            "\n",
            "===================== RUN: FUSION_mid+freq+midgrad+grad_hf+tgrad =====================\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 1/3 | loss=0.5769 | test_f1=0.622 auc=0.934 acc=0.717\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 2/3 | loss=0.3076 | test_f1=0.870 auc=0.997 acc=0.850\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 3/3 | loss=0.4311 | test_f1=0.949 auc=0.997 acc=0.950\n",
            "\n",
            "==== FINAL SUMMARY (sorted by F1) ====\n",
            "                                    model       acc        f1   roc_auc  \\\n",
            "0                     FUSION_midgrad+freq  0.950000  0.952381  0.998889   \n",
            "1                    FUSION_midgrad+tgrad  0.950000  0.952381  0.998889   \n",
            "2                  FUSION_midgrad+grad_hf  0.933333  0.933333  0.988889   \n",
            "3   FUSION_mid+freq+midgrad+grad_hf+tgrad  0.933333  0.933333  0.976667   \n",
            "4                             R2Res_tgrad  0.933333  0.928571  0.977778   \n",
            "5                      FUSION_mid+midgrad  0.933333  0.928571  0.993333   \n",
            "6                                  TS_mid  0.933333  0.928571  0.998889   \n",
            "7                              TS_midgrad  0.916667  0.923077  0.997778   \n",
            "8                           R2Res_grad_hf  0.916667  0.918033  0.950000   \n",
            "9                               R2Res_mid  0.916667  0.912281  0.986667   \n",
            "10                          R2Res_midgrad  0.916667  0.912281  0.981111   \n",
            "11                            ViT_midgrad  0.900000  0.888889  0.903333   \n",
            "12                          R2Res_midtone  0.883333  0.877193  0.938889   \n",
            "13                             R2Res_freq  0.883333  0.872727  0.983333   \n",
            "14                                ViT_mid  0.883333  0.867925  0.885556   \n",
            "15                            ViT_midtone  0.833333  0.800000  0.854444   \n",
            "16                               TS_tgrad  0.750000  0.769231  0.836667   \n",
            "17                            ViT_grad_hf  0.500000  0.666667  0.460000   \n",
            "18                             TS_midtone  0.500000  0.666667  0.782222   \n",
            "19                              ViT_tgrad  0.500000  0.666667  0.633333   \n",
            "20                                TS_freq  0.616667  0.378378  0.550000   \n",
            "21                             TS_grad_hf  0.600000  0.333333  0.517778   \n",
            "22                               ViT_freq  0.583333  0.285714  0.575556   \n",
            "\n",
            "          ap  \n",
            "0   0.998925  \n",
            "1   0.998925  \n",
            "2   0.989379  \n",
            "3   0.972209  \n",
            "4   0.982276  \n",
            "5   0.993544  \n",
            "6   0.998925  \n",
            "7   0.997814  \n",
            "8   0.967460  \n",
            "9   0.987429  \n",
            "10  0.984509  \n",
            "11  0.939006  \n",
            "12  0.957636  \n",
            "13  0.983555  \n",
            "14  0.922500  \n",
            "15  0.905015  \n",
            "16  0.828391  \n",
            "17  0.620960  \n",
            "18  0.826611  \n",
            "19  0.630734  \n",
            "20  0.683560  \n",
            "21  0.652686  \n",
            "22  0.645079  \n",
            "\n",
            "Saved summary to: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC/SUMMARY.csv\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_NOPCA_NOGINC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BENCHMARK_NEUTRAL_ALLMETHODS (MEANINGFUL SIZE, FIXED CLIPS)\n",
        "# - NO PCA, NO GINC\n",
        "# - Uses corrected streams:\n",
        "#     midtone, midgrad (grad on midtones), grad_hf (HF of grad),\n",
        "#     tgrad, freq\n",
        "# - FAIR TEST: SAME videos + SAME clip start frames across ALL streams/models\n",
        "# - Saves: raw_manifest, dedup_manifest, normalized_manifest,\n",
        "#          train/test manifests, clip_plan.csv, SUMMARY.csv\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm transformers accelerate\n",
        "\n",
        "import os, cv2, random, hashlib, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models as tvm\n",
        "\n",
        "from transformers import TimesformerForVideoClassification\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "print(\"Torch device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "RUN_NAME = \"BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC\"\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "\n",
        "# âœ… meaningful size\n",
        "N_EACH = 100\n",
        "\n",
        "# oversample factor to survive dedup, then rebalance to exact N_EACH each\n",
        "RAW_MULT = 1.4\n",
        "RAW_EACH = int(np.ceil(N_EACH * RAW_MULT))\n",
        "\n",
        "# Normalization (metadata-neutral)\n",
        "NORM_FPS   = 25\n",
        "NORM_W     = 256\n",
        "NORM_H     = 256\n",
        "NORM_CODEC = \"libx264\"\n",
        "NORM_CRF   = 23\n",
        "NORM_PIX   = \"yuv420p\"\n",
        "NORM_AUDIO = False\n",
        "\n",
        "# Model input sampling\n",
        "T = 8\n",
        "FRAME_STRIDE = 3\n",
        "TARGET_SIZE  = 224\n",
        "BATCH_SIZE   = 6\n",
        "EPOCHS       = 3\n",
        "LR           = 3e-4\n",
        "NUM_WORKERS  = 2\n",
        "\n",
        "# Fixed clip sampling per video (fairness)\n",
        "CLIPS_PER_VIDEO_TRAIN = 3   # more training signal\n",
        "CLIPS_PER_VIDEO_TEST  = 1   # stable evaluation\n",
        "\n",
        "# Midtone thresholds\n",
        "MID_LO = 0.25\n",
        "MID_HI = 0.75\n",
        "\n",
        "# Streams to test (NO freq+high combos)\n",
        "STREAMS_FOR_R2RES = [\"mid\", \"midtone\", \"midgrad\", \"grad_hf\", \"freq\", \"tgrad\"]\n",
        "STREAMS_FOR_TRANSFORMERS = [\"mid\", \"freq\", \"midtone\", \"midgrad\", \"grad_hf\", \"tgrad\"]\n",
        "\n",
        "# Fusion runs that make sense\n",
        "FUSION_RUNS = [\n",
        "    [\"mid\", \"midgrad\"],\n",
        "    [\"midgrad\", \"freq\"],\n",
        "    [\"midgrad\", \"grad_hf\"],\n",
        "    [\"midgrad\", \"tgrad\"],\n",
        "    [\"mid\", \"freq\", \"midgrad\", \"grad_hf\", \"tgrad\"],\n",
        "]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) LOAD VIDEO PATHS (RAW_EACH + RAW_EACH)\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\",\".webm\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "assert len(real) >= RAW_EACH and (len(ai_core)+len(sora)) >= RAW_EACH, \"Not enough videos.\"\n",
        "\n",
        "AI_ALL = ai_core + sora\n",
        "AI = random.sample(AI_ALL, RAW_EACH)\n",
        "REAL = random.sample(real, RAW_EACH)\n",
        "\n",
        "raw_df = pd.DataFrame({\n",
        "    \"path\": AI + REAL,\n",
        "    \"y\":    [1]*len(AI) + [0]*len(REAL),\n",
        "    \"src\":  [\"AI\"]*len(AI) + [\"REAL\"]*len(REAL),\n",
        "}).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "raw_manifest = os.path.join(RUN_DIR, f\"raw_manifest_{RAW_EACH}each.csv\")\n",
        "raw_df.to_csv(raw_manifest, index=False)\n",
        "print(\"\\nRAW pool:\", len(raw_df), \"total | AI=\", (raw_df.y==1).sum(), \"REAL=\", (raw_df.y==0).sum())\n",
        "print(\"Saved:\", raw_manifest)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) HASHING FOR DEDUP\n",
        "# -----------------------------\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path)\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    fr = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return h\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) DEDUP (Exact + Near)\n",
        "# -----------------------------\n",
        "print(\"\\nComputing hashes...\")\n",
        "sha_list, dh_list = [], []\n",
        "for p in tqdm(raw_df.path.tolist(), desc=\"Hashing\"):\n",
        "    sha_list.append(sha1_first_frame(p))\n",
        "    dh_list.append(dhash_first_frame(p))\n",
        "\n",
        "df = raw_df.copy()\n",
        "df[\"sha1\"] = sha_list\n",
        "df[\"dhash\"] = dh_list\n",
        "df = df.dropna(subset=[\"sha1\",\"dhash\"]).reset_index(drop=True)\n",
        "\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"sha1\"]).reset_index(drop=True)\n",
        "print(\"Exact dupes removed:\", before - len(df))\n",
        "\n",
        "def uf_make(n):\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb: return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "    return find, union\n",
        "\n",
        "def bucket_key(h, bits=12):\n",
        "    return int(h) >> (64 - bits)\n",
        "\n",
        "bucket = defaultdict(list)\n",
        "for i, h in enumerate(df.dhash.values):\n",
        "    bucket[bucket_key(h)].append(i)\n",
        "\n",
        "find, union = uf_make(len(df))\n",
        "for _, idxs in bucket.items():\n",
        "    if len(idxs) < 2:\n",
        "        continue\n",
        "    # keep fast if a bucket is huge\n",
        "    if len(idxs) > 250:\n",
        "        idxs = random.sample(idxs, 250)\n",
        "\n",
        "    for ii in range(len(idxs)):\n",
        "        a = idxs[ii]\n",
        "        ha = int(df.loc[a,\"dhash\"])\n",
        "        for jj in range(ii+1, len(idxs)):\n",
        "            b = idxs[jj]\n",
        "            hb = int(df.loc[b,\"dhash\"])\n",
        "            if hamming(ha, hb) <= 3:\n",
        "                union(a, b)\n",
        "\n",
        "df[\"cluster\"] = [find(i) for i in range(len(df))]\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"cluster\"]).reset_index(drop=True)\n",
        "print(\"Near-dupes removed:\", before - len(df))\n",
        "print(\"After dedup:\", len(df))\n",
        "\n",
        "# âœ… rebalance AFTER dedup to exact N_EACH each\n",
        "df_ai = df[df.y==1].sample(n=min(N_EACH, (df.y==1).sum()), random_state=SEED)\n",
        "df_rl = df[df.y==0].sample(n=min(N_EACH, (df.y==0).sum()), random_state=SEED)\n",
        "\n",
        "assert len(df_ai) >= N_EACH and len(df_rl) >= N_EACH, (\n",
        "    f\"After dedup you have AI={len(df_ai)} REAL={len(df_rl)}. \"\n",
        "    f\"Increase RAW_MULT or lower N_EACH.\"\n",
        ")\n",
        "\n",
        "df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "print(\"Rebalanced:\", len(df), \"total | AI=\", (df.y==1).sum(), \"REAL=\", (df.y==0).sum())\n",
        "\n",
        "dedup_manifest = os.path.join(RUN_DIR, \"dedup_manifest.csv\")\n",
        "df.to_csv(dedup_manifest, index=False)\n",
        "print(\"Saved:\", dedup_manifest)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) NORMALIZE\n",
        "# -----------------------------\n",
        "NORM_DIR = os.path.join(RUN_DIR, \"normalized\")\n",
        "os.makedirs(NORM_DIR, exist_ok=True)\n",
        "\n",
        "def norm_path_for(i):\n",
        "    return os.path.join(NORM_DIR, f\"v_{i:05d}.mp4\")\n",
        "\n",
        "def ffmpeg_normalize(in_path, out_path):\n",
        "    aflag = [\"-an\"] if not NORM_AUDIO else []\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-vf\", f\"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,\"\n",
        "               f\"pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,fps={NORM_FPS}\",\n",
        "        \"-vsync\",\"cfr\",\n",
        "        *aflag,\n",
        "        \"-c:v\", NORM_CODEC,\n",
        "        \"-crf\", str(NORM_CRF),\n",
        "        \"-pix_fmt\", NORM_PIX,\n",
        "        out_path\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "\n",
        "print(\"\\nNormalizing...\")\n",
        "norm_paths = []\n",
        "for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=\"Normalize\")):\n",
        "    outp = norm_path_for(i)\n",
        "    if not os.path.exists(outp):\n",
        "        ffmpeg_normalize(row.path, outp)\n",
        "    norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)\n",
        "\n",
        "ndf = df.copy()\n",
        "ndf[\"norm_path\"] = norm_paths\n",
        "ndf = ndf.dropna(subset=[\"norm_path\"]).reset_index(drop=True)\n",
        "\n",
        "norm_manifest = os.path.join(RUN_DIR, \"normalized_manifest.csv\")\n",
        "ndf.to_csv(norm_manifest, index=False)\n",
        "print(\"Normalized:\", len(ndf), \"Saved:\", norm_manifest)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5) CLUSTER-SAFE SPLIT\n",
        "# -----------------------------\n",
        "clusters = ndf.cluster.values\n",
        "cluster_to_rows = defaultdict(list)\n",
        "for i,c in enumerate(clusters):\n",
        "    cluster_to_rows[int(c)].append(i)\n",
        "\n",
        "cluster_ids = list(cluster_to_rows.keys())\n",
        "cluster_labels = []\n",
        "for c in cluster_ids:\n",
        "    ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]\n",
        "    cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)\n",
        "\n",
        "c_train, c_test = train_test_split(\n",
        "    cluster_ids, test_size=0.3, random_state=SEED, stratify=cluster_labels\n",
        ")\n",
        "\n",
        "train_idx, test_idx = [], []\n",
        "for c in c_train: train_idx.extend(cluster_to_rows[c])\n",
        "for c in c_test:  test_idx.extend(cluster_to_rows[c])\n",
        "\n",
        "train_df = ndf.iloc[train_idx].reset_index(drop=True)\n",
        "test_df  = ndf.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCluster-safe split:\")\n",
        "print(\" Train:\", len(train_df), \" Test:\", len(test_df))\n",
        "print(\" Train AI/REAL:\", int((train_df.y==1).sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\" Test  AI/REAL:\", int((test_df.y==1).sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "train_df.to_csv(os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\"), index=False)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) STREAMS (CORRECTED)\n",
        "# -----------------------------\n",
        "def to_rgb(frame_bgr):\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def resize_center(frame, size):\n",
        "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def luma01_from_bgr(fr_bgr):\n",
        "    fr = fr_bgr.astype(np.float32) / 255.0\n",
        "    b,g,r = fr[:,:,0], fr[:,:,1], fr[:,:,2]\n",
        "    return (0.114*b + 0.587*g + 0.299*r).astype(np.float32)\n",
        "\n",
        "def midtone_mask(luma01, lo=MID_LO, hi=MID_HI):\n",
        "    return ((luma01 >= lo) & (luma01 <= hi)).astype(np.float32)\n",
        "\n",
        "def sobel_mag01(gray01):\n",
        "    g8 = (gray01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    gx = cv2.Sobel(g8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(g8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    mag = cv2.magnitude(gx, gy)\n",
        "    mag = mag / (np.max(mag) + 1e-8)\n",
        "    return mag.astype(np.float32)\n",
        "\n",
        "def fft_highpass_map01(img01, frac_hi=0.45):\n",
        "    f = np.fft.fft2(img01.astype(np.float32))\n",
        "    f = np.fft.fftshift(f)\n",
        "    H,W = img01.shape\n",
        "    cy,cx = H//2, W//2\n",
        "    yy,xx = np.ogrid[:H,:W]\n",
        "    r = np.sqrt((yy-cy)**2 + (xx-cx)**2)\n",
        "    rmax = np.max(r) + 1e-8\n",
        "    hp = (r >= frac_hi*rmax).astype(np.float32)\n",
        "    f_hp = f * hp\n",
        "    f_hp = np.fft.ifftshift(f_hp)\n",
        "    out = np.fft.ifft2(f_hp)\n",
        "    out = np.real(out).astype(np.float32)\n",
        "    out = out - np.min(out)\n",
        "    out = out / (np.max(out) + 1e-8)\n",
        "    return out\n",
        "\n",
        "def map01_to_bgr(map01):\n",
        "    x = (map01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    return cv2.cvtColor(x, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def stream_mid(fr):\n",
        "    return fr\n",
        "\n",
        "def stream_midtone(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    return map01_to_bgr(mid)\n",
        "\n",
        "def stream_midgrad(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    return map01_to_bgr(mag)\n",
        "\n",
        "def stream_grad_hf(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    hf = fft_highpass_map01(mag, frac_hi=0.45)\n",
        "    return map01_to_bgr(hf)\n",
        "\n",
        "def stream_freq(fr):\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    mag = np.log1p(np.abs(fshift))\n",
        "    mag = mag / (mag.max() + 1e-6)\n",
        "    return map01_to_bgr(mag.astype(np.float32))\n",
        "\n",
        "def stream_tgrad(fr_prev, fr_cur):\n",
        "    g_prev = cv2.cvtColor(stream_midgrad(fr_prev), cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    g_cur  = cv2.cvtColor(stream_midgrad(fr_cur),  cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    diff = np.abs(g_cur - g_prev)\n",
        "    diff = diff / (diff.max() + 1e-8)\n",
        "    return map01_to_bgr(diff)\n",
        "\n",
        "STREAM_FN = {\n",
        "    \"mid\": stream_mid,\n",
        "    \"midtone\": stream_midtone,\n",
        "    \"midgrad\": stream_midgrad,\n",
        "    \"grad_hf\": stream_grad_hf,\n",
        "    \"freq\": stream_freq,\n",
        "}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7) DATASETS (FIXED CLIPS FOR FAIRNESS)\n",
        "# -----------------------------\n",
        "def open_video(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    return cap if cap.isOpened() else None\n",
        "\n",
        "def get_frame_count(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return 0\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    return n\n",
        "\n",
        "def pick_fixed_start(norm_path, max_start, clip_id, salt=SEED):\n",
        "    # deterministic per (video, clip_id, seed)\n",
        "    h = hashlib.sha1(f\"{norm_path}|{clip_id}|{salt}\".encode(\"utf-8\")).hexdigest()\n",
        "    if max_start <= 0:\n",
        "        return 0\n",
        "    return int(h[:8], 16) % (max_start + 1)\n",
        "\n",
        "def build_clip_plan(df_videos, clips_per_video, split_name):\n",
        "    rows = []\n",
        "    for r in df_videos.itertuples(index=False):\n",
        "        p = str(r.norm_path)\n",
        "        n = get_frame_count(p)\n",
        "        max_start = max(0, n - (T-1)*FRAME_STRIDE - 1)\n",
        "        for clip_id in range(clips_per_video):\n",
        "            st = pick_fixed_start(p, max_start, clip_id, salt=SEED)\n",
        "            rows.append({\n",
        "                \"norm_path\": p,\n",
        "                \"y\": int(r.y),\n",
        "                \"split\": split_name,\n",
        "                \"clip_id\": int(clip_id),\n",
        "                \"start\": int(st),\n",
        "                \"frame_count\": int(n),\n",
        "                # store config so we can auto-regenerate if changed\n",
        "                \"plan_T\": int(T),\n",
        "                \"plan_stride\": int(FRAME_STRIDE),\n",
        "                \"plan_seed\": int(SEED),\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "clip_plan_path = os.path.join(RUN_DIR, \"clip_plan.csv\")\n",
        "\n",
        "need_rebuild = True\n",
        "if os.path.exists(clip_plan_path):\n",
        "    old = pd.read_csv(clip_plan_path)\n",
        "    # rebuild if any mismatch (seed/T/stride) OR split sizes differ\n",
        "    if len(old) > 0:\n",
        "        same_cfg = (\n",
        "            int(old.plan_T.iloc[0]) == int(T) and\n",
        "            int(old.plan_stride.iloc[0]) == int(FRAME_STRIDE) and\n",
        "            int(old.plan_seed.iloc[0]) == int(SEED)\n",
        "        )\n",
        "        # We also want it to match current train/test video sets\n",
        "        old_train_paths = set(old[old.split==\"train\"].norm_path.unique().tolist())\n",
        "        old_test_paths  = set(old[old.split==\"test\"].norm_path.unique().tolist())\n",
        "        new_train_paths = set(train_df.norm_path.unique().tolist())\n",
        "        new_test_paths  = set(test_df.norm_path.unique().tolist())\n",
        "\n",
        "        same_paths = (old_train_paths == new_train_paths) and (old_test_paths == new_test_paths)\n",
        "\n",
        "        if same_cfg and same_paths:\n",
        "            need_rebuild = False\n",
        "            clip_plan = old\n",
        "            print(\"Loaded existing fixed clip plan:\", clip_plan_path)\n",
        "\n",
        "if need_rebuild:\n",
        "    clip_train = build_clip_plan(train_df, CLIPS_PER_VIDEO_TRAIN, \"train\")\n",
        "    clip_test  = build_clip_plan(test_df,  CLIPS_PER_VIDEO_TEST,  \"test\")\n",
        "    clip_plan  = pd.concat([clip_train, clip_test], axis=0).reset_index(drop=True)\n",
        "    clip_plan.to_csv(clip_plan_path, index=False)\n",
        "    print(\"Saved NEW fixed clip plan:\", clip_plan_path)\n",
        "\n",
        "clip_train_df = clip_plan[clip_plan.split==\"train\"].reset_index(drop=True)\n",
        "clip_test_df  = clip_plan[clip_plan.split==\"test\"].reset_index(drop=True)\n",
        "\n",
        "print(\"Fixed clips:\")\n",
        "print(\" Train clips:\", len(clip_train_df), f\"({CLIPS_PER_VIDEO_TRAIN} per video)\")\n",
        "print(\" Test  clips:\", len(clip_test_df),  f\"({CLIPS_PER_VIDEO_TEST} per video)\")\n",
        "\n",
        "def sample_clip_frames_fixed(path, start, T=8, stride=3, size=224):\n",
        "    cap = open_video(path)\n",
        "    if cap is None:\n",
        "        return None\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    if n <= 0:\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        idx = min(int(start) + i*stride, n-1)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = resize_center(fr, size)\n",
        "        frames.append(fr)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "class FixedClipStreamDataset(Dataset):\n",
        "    def __init__(self, clip_df, stream=\"mid\", T=8, stride=3, size=224):\n",
        "        self.df = clip_df.reset_index(drop=True)\n",
        "        self.stream = stream\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.norm_path.iloc[idx]\n",
        "        y = int(self.df.y.iloc[idx])\n",
        "        start = int(self.df.start.iloc[idx])\n",
        "\n",
        "        frames = sample_clip_frames_fixed(path, start, T=self.T, stride=self.stride, size=self.size)\n",
        "        if frames is None:\n",
        "            x = np.zeros((self.T, 3, self.size, self.size), dtype=np.float32)\n",
        "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        if self.stream in STREAM_FN:\n",
        "            proc = [STREAM_FN[self.stream](fr) for fr in frames]\n",
        "        elif self.stream == \"tgrad\":\n",
        "            proc = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "        else:\n",
        "            proc = frames\n",
        "\n",
        "        arr = np.stack([to_rgb(fr) for fr in proc], axis=0).astype(np.float32)/255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))\n",
        "        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class FixedClipMultiStreamDataset(Dataset):\n",
        "    def __init__(self, clip_df, streams, T=8, stride=3, size=224):\n",
        "        self.df = clip_df.reset_index(drop=True)\n",
        "        self.streams = list(streams)\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.norm_path.iloc[idx]\n",
        "        y = int(self.df.y.iloc[idx])\n",
        "        start = int(self.df.start.iloc[idx])\n",
        "\n",
        "        frames = sample_clip_frames_fixed(path, start, T=self.T, stride=self.stride, size=self.size)\n",
        "        z = torch.zeros((self.T,3,self.size,self.size), dtype=torch.float32)\n",
        "\n",
        "        if frames is None:\n",
        "            out = {s: z for s in self.streams}\n",
        "            return out, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        out = {}\n",
        "        for s in self.streams:\n",
        "            if s == \"tgrad\":\n",
        "                seq = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "            else:\n",
        "                seq = [STREAM_FN[s](fr) for fr in frames]\n",
        "\n",
        "            arr = np.stack([to_rgb(fr) for fr in seq], axis=0).astype(np.float32)/255.0\n",
        "            arr = np.transpose(arr, (0,3,1,2))\n",
        "            out[s] = torch.from_numpy(arr)\n",
        "\n",
        "        return out, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8) MODELS\n",
        "# -----------------------------\n",
        "class R2ResNetFusion(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)\n",
        "        self.r2.fc = nn.Identity()\n",
        "\n",
        "        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "        self.res.fc = nn.Identity()\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        xr2 = x.permute(0,2,1,3,4)\n",
        "        f_r2 = self.r2(xr2)\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.res(xf).view(B, T, -1).mean(dim=1)\n",
        "        feat = torch.cat([f_r2, f], dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "class MultiResNetFusion(nn.Module):\n",
        "    def __init__(self, streams, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.streams = list(streams)\n",
        "        self.backbones = nn.ModuleDict()\n",
        "        for s in self.streams:\n",
        "            res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "            res.fc = nn.Identity()\n",
        "            self.backbones[s] = res\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512*len(self.streams), 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_dict):\n",
        "        feats = []\n",
        "        for s in self.streams:\n",
        "            x = batch_dict[s]\n",
        "            B,T,C,H,W = x.shape\n",
        "            xf = x.reshape(B*T, C, H, W)\n",
        "            f = self.backbones[s](xf).view(B, T, -1).mean(dim=1)\n",
        "            feats.append(f)\n",
        "        feat = torch.cat(feats, dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "class ViTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.vit = tvm.vit_b_16(weights=tvm.ViT_B_16_Weights.DEFAULT)\n",
        "        self.vit.heads = nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.vit(xf).view(B, T, -1).mean(dim=1)\n",
        "        return self.head(f)\n",
        "\n",
        "class TimeSformerClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.model = TimesformerForVideoClassification.from_pretrained(\n",
        "            \"facebook/timesformer-base-finetuned-k400\",\n",
        "            num_labels=num_classes,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(pixel_values=x).logits\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 9) TRAIN / EVAL\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, proba):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, pred)),\n",
        "        \"f1\": float(f1_score(y_true, pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"ap\": float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader, is_multi=False):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for batch in loader:\n",
        "        if is_multi:\n",
        "            x_dict, y = batch\n",
        "            x_dict = {k:v.to(DEVICE) for k,v in x_dict.items()}\n",
        "            logits = model(x_dict)\n",
        "        else:\n",
        "            x, y = batch\n",
        "            x = x.to(DEVICE)\n",
        "            logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy())\n",
        "        all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return compute_metrics(y_true, proba)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=3, is_multi=False, name=\"model\"):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses=[]\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            if is_multi:\n",
        "                x_dict, y = batch\n",
        "                x_dict = {k:v.to(DEVICE) for k,v in x_dict.items()}\n",
        "                y = y.to(DEVICE)\n",
        "                logits = model(x_dict)\n",
        "            else:\n",
        "                x, y = batch\n",
        "                x = x.to(DEVICE)\n",
        "                y = y.to(DEVICE)\n",
        "                logits = model(x)\n",
        "\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        m = eval_model(model, test_loader, is_multi=is_multi)\n",
        "        print(f\"{name} | ep {ep}/{epochs} | loss={np.mean(losses):.4f} | \"\n",
        "              f\"test_f1={m['f1']:.3f} auc={m['roc_auc']:.3f} acc={m['acc']:.3f}\")\n",
        "\n",
        "        if m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return eval_model(model, test_loader, is_multi=is_multi)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 10) RUN BENCHMARKS (FAIR: use clip_train_df / clip_test_df)\n",
        "# -----------------------------\n",
        "results = []\n",
        "\n",
        "def run_single_stream(model_kind, stream):\n",
        "    ds_tr = FixedClipStreamDataset(clip_train_df, stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = FixedClipStreamDataset(clip_test_df,  stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    if model_kind == \"r2res\":\n",
        "        model = R2ResNetFusion()\n",
        "        name = f\"R2Res_{stream}\"\n",
        "    elif model_kind == \"vit\":\n",
        "        model = ViTClassifier()\n",
        "        name = f\"ViT_{stream}\"\n",
        "    elif model_kind == \"timesformer\":\n",
        "        model = TimeSformerClassifier()\n",
        "        name = f\"TS_{stream}\"\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    print(\"\\n===================== RUN:\", name, \"=====================\")\n",
        "    met = train_model(model, tr, te, epochs=EPOCHS, is_multi=False, name=name)\n",
        "    met[\"model\"] = name\n",
        "    results.append(met)\n",
        "\n",
        "def run_fusion(streams):\n",
        "    ds_tr = FixedClipMultiStreamDataset(clip_train_df, streams=streams, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = FixedClipMultiStreamDataset(clip_test_df,  streams=streams, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    model = MultiResNetFusion(streams=streams)\n",
        "    name = \"FUSION_\" + \"+\".join(streams)\n",
        "\n",
        "    print(\"\\n===================== RUN:\", name, \"=====================\")\n",
        "    met = train_model(model, tr, te, epochs=EPOCHS, is_multi=True, name=name)\n",
        "    met[\"model\"] = name\n",
        "    results.append(met)\n",
        "\n",
        "# R2Res\n",
        "for s in STREAMS_FOR_R2RES:\n",
        "    run_single_stream(\"r2res\", s)\n",
        "\n",
        "# ViT\n",
        "for s in STREAMS_FOR_TRANSFORMERS:\n",
        "    run_single_stream(\"vit\", s)\n",
        "\n",
        "# TimeSformer\n",
        "for s in STREAMS_FOR_TRANSFORMERS:\n",
        "    run_single_stream(\"timesformer\", s)\n",
        "\n",
        "# Fusion\n",
        "for streams in FUSION_RUNS:\n",
        "    run_fusion(streams)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 11) SAVE SUMMARY\n",
        "# -----------------------------\n",
        "res_df = pd.DataFrame(results).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
        "summary_csv = os.path.join(RUN_DIR, \"SUMMARY.csv\")\n",
        "res_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "print(\"\\n==== FINAL SUMMARY (sorted by F1) ====\")\n",
        "print(res_df[[\"model\",\"acc\",\"f1\",\"roc_auc\",\"ap\"]].head(50))\n",
        "print(\"\\nSaved summary to:\", summary_csv)\n",
        "print(\"Run folder:\", RUN_DIR)\n",
        "print(\"Clip plan:\", clip_plan_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXyOYS0ctU2R",
        "outputId": "9dc4518b-a54a-4131-d72b-268b2a9a3a53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "GPU: NVIDIA L4\n",
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "RAW pool: 280 total | AI= 140 REAL= 140\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/raw_manifest_140each.csv\n",
            "\n",
            "Computing hashes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280/280 [03:41<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact dupes removed: 0\n",
            "Near-dupes removed: 1\n",
            "After dedup: 278\n",
            "Rebalanced: 200 total | AI= 100 REAL= 100\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/dedup_manifest.csv\n",
            "\n",
            "Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:33<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: 200 Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/normalized_manifest.csv\n",
            "\n",
            "Cluster-safe split:\n",
            " Train: 140  Test: 60\n",
            " Train AI/REAL: 70 / 70\n",
            " Test  AI/REAL: 30 / 30\n",
            "Saved NEW fixed clip plan: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/clip_plan.csv\n",
            "Fixed clips:\n",
            " Train clips: 420 (3 per video)\n",
            " Test  clips: 60 (1 per video)\n",
            "\n",
            "===================== RUN: R2Res_mid =====================\n",
            "R2Res_mid | ep 1/3 | loss=0.4192 | test_f1=0.967 auc=0.991 acc=0.967\n",
            "R2Res_mid | ep 2/3 | loss=0.1758 | test_f1=0.784 auc=0.872 acc=0.733\n",
            "R2Res_mid | ep 3/3 | loss=0.2596 | test_f1=0.967 auc=0.999 acc=0.967\n",
            "\n",
            "===================== RUN: R2Res_midtone =====================\n",
            "R2Res_midtone | ep 1/3 | loss=0.4101 | test_f1=0.822 auc=0.943 acc=0.783\n",
            "R2Res_midtone | ep 2/3 | loss=0.2343 | test_f1=0.918 auc=0.979 acc=0.917\n",
            "R2Res_midtone | ep 3/3 | loss=0.2216 | test_f1=0.929 auc=0.987 acc=0.933\n",
            "\n",
            "===================== RUN: R2Res_midgrad =====================\n",
            "R2Res_midgrad | ep 1/3 | loss=0.4034 | test_f1=0.421 auc=0.886 acc=0.633\n",
            "R2Res_midgrad | ep 2/3 | loss=0.3000 | test_f1=0.912 auc=0.976 acc=0.917\n",
            "R2Res_midgrad | ep 3/3 | loss=0.2531 | test_f1=0.915 auc=0.983 acc=0.917\n",
            "\n",
            "===================== RUN: R2Res_grad_hf =====================\n",
            "R2Res_grad_hf | ep 1/3 | loss=0.4864 | test_f1=0.720 auc=0.904 acc=0.650\n",
            "R2Res_grad_hf | ep 2/3 | loss=0.3233 | test_f1=0.741 auc=0.952 acc=0.650\n",
            "R2Res_grad_hf | ep 3/3 | loss=0.2715 | test_f1=0.853 auc=0.958 acc=0.833\n",
            "\n",
            "===================== RUN: R2Res_freq =====================\n",
            "R2Res_freq | ep 1/3 | loss=0.4419 | test_f1=0.378 auc=0.877 acc=0.617\n",
            "R2Res_freq | ep 2/3 | loss=0.3647 | test_f1=0.730 auc=0.847 acc=0.717\n",
            "R2Res_freq | ep 3/3 | loss=0.3060 | test_f1=0.900 auc=0.977 acc=0.900\n",
            "\n",
            "===================== RUN: R2Res_tgrad =====================\n",
            "R2Res_tgrad | ep 1/3 | loss=0.4364 | test_f1=0.862 auc=0.959 acc=0.850\n",
            "R2Res_tgrad | ep 2/3 | loss=0.2893 | test_f1=0.857 auc=0.956 acc=0.833\n",
            "R2Res_tgrad | ep 3/3 | loss=0.2225 | test_f1=0.821 auc=0.950 acc=0.833\n",
            "\n",
            "===================== RUN: ViT_mid =====================\n",
            "ViT_mid | ep 1/3 | loss=0.7129 | test_f1=0.667 auc=0.857 acc=0.500\n",
            "ViT_mid | ep 2/3 | loss=0.4908 | test_f1=0.846 auc=0.829 acc=0.867\n",
            "ViT_mid | ep 3/3 | loss=0.4110 | test_f1=0.846 auc=0.886 acc=0.867\n",
            "\n",
            "===================== RUN: ViT_freq =====================\n",
            "ViT_freq | ep 1/3 | loss=0.7234 | test_f1=0.667 auc=0.497 acc=0.500\n",
            "ViT_freq | ep 2/3 | loss=0.7097 | test_f1=0.000 auc=0.602 acc=0.500\n",
            "ViT_freq | ep 3/3 | loss=0.7135 | test_f1=0.667 auc=0.541 acc=0.500\n",
            "\n",
            "===================== RUN: ViT_midtone =====================\n",
            "ViT_midtone | ep 1/3 | loss=0.7227 | test_f1=0.679 auc=0.717 acc=0.717\n",
            "ViT_midtone | ep 2/3 | loss=0.6098 | test_f1=0.667 auc=0.768 acc=0.500\n",
            "ViT_midtone | ep 3/3 | loss=0.6007 | test_f1=0.723 auc=0.816 acc=0.783\n",
            "\n",
            "===================== RUN: ViT_midgrad =====================\n",
            "ViT_midgrad | ep 1/3 | loss=0.7174 | test_f1=0.000 auc=0.918 acc=0.500\n",
            "ViT_midgrad | ep 2/3 | loss=0.5783 | test_f1=0.846 auc=0.998 acc=0.867\n",
            "ViT_midgrad | ep 3/3 | loss=0.5720 | test_f1=0.966 auc=0.999 acc=0.967\n",
            "\n",
            "===================== RUN: ViT_grad_hf =====================\n",
            "ViT_grad_hf | ep 1/3 | loss=0.7232 | test_f1=0.667 auc=0.228 acc=0.500\n",
            "ViT_grad_hf | ep 2/3 | loss=0.7268 | test_f1=0.000 auc=0.227 acc=0.500\n",
            "ViT_grad_hf | ep 3/3 | loss=0.7133 | test_f1=0.667 auc=0.228 acc=0.500\n",
            "\n",
            "===================== RUN: ViT_tgrad =====================\n",
            "ViT_tgrad | ep 1/3 | loss=0.7339 | test_f1=0.667 auc=0.564 acc=0.500\n",
            "ViT_tgrad | ep 2/3 | loss=0.6995 | test_f1=0.000 auc=0.616 acc=0.500\n",
            "ViT_tgrad | ep 3/3 | loss=0.7052 | test_f1=0.000 auc=0.490 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_mid =====================\n",
            "TS_mid | ep 1/3 | loss=0.6097 | test_f1=0.846 auc=0.987 acc=0.867\n",
            "TS_mid | ep 2/3 | loss=0.3674 | test_f1=0.923 auc=0.996 acc=0.917\n",
            "TS_mid | ep 3/3 | loss=0.3179 | test_f1=0.968 auc=0.981 acc=0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_freq =====================\n",
            "TS_freq | ep 1/3 | loss=0.8200 | test_f1=0.667 auc=0.530 acc=0.500\n",
            "TS_freq | ep 2/3 | loss=0.7034 | test_f1=0.000 auc=0.519 acc=0.500\n",
            "TS_freq | ep 3/3 | loss=0.7096 | test_f1=0.667 auc=0.539 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_midtone =====================\n",
            "TS_midtone | ep 1/3 | loss=0.7834 | test_f1=0.000 auc=0.601 acc=0.500\n",
            "TS_midtone | ep 2/3 | loss=0.6915 | test_f1=0.000 auc=0.724 acc=0.500\n",
            "TS_midtone | ep 3/3 | loss=0.7045 | test_f1=0.533 auc=0.678 acc=0.650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_midgrad =====================\n",
            "TS_midgrad | ep 1/3 | loss=0.8951 | test_f1=0.667 auc=0.923 acc=0.500\n",
            "TS_midgrad | ep 2/3 | loss=0.5953 | test_f1=0.921 auc=0.987 acc=0.917\n",
            "TS_midgrad | ep 3/3 | loss=0.6290 | test_f1=0.667 auc=0.954 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_grad_hf =====================\n",
            "TS_grad_hf | ep 1/3 | loss=0.7954 | test_f1=0.000 auc=0.221 acc=0.500\n",
            "TS_grad_hf | ep 2/3 | loss=0.7043 | test_f1=0.000 auc=0.227 acc=0.500\n",
            "TS_grad_hf | ep 3/3 | loss=0.7042 | test_f1=0.667 auc=0.227 acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_tgrad =====================\n",
            "TS_tgrad | ep 1/3 | loss=0.7240 | test_f1=0.667 auc=0.777 acc=0.667\n",
            "TS_tgrad | ep 2/3 | loss=0.5860 | test_f1=0.652 auc=0.919 acc=0.733\n",
            "TS_tgrad | ep 3/3 | loss=0.5972 | test_f1=0.667 auc=0.538 acc=0.500\n",
            "\n",
            "===================== RUN: FUSION_mid+midgrad =====================\n",
            "FUSION_mid+midgrad | ep 1/3 | loss=0.3614 | test_f1=0.808 auc=0.902 acc=0.833\n",
            "FUSION_mid+midgrad | ep 2/3 | loss=0.2551 | test_f1=0.938 auc=1.000 acc=0.933\n",
            "FUSION_mid+midgrad | ep 3/3 | loss=0.1655 | test_f1=0.952 auc=0.992 acc=0.950\n",
            "\n",
            "===================== RUN: FUSION_midgrad+freq =====================\n",
            "FUSION_midgrad+freq | ep 1/3 | loss=0.4193 | test_f1=0.852 auc=0.984 acc=0.867\n",
            "FUSION_midgrad+freq | ep 2/3 | loss=0.2570 | test_f1=0.912 auc=0.992 acc=0.917\n",
            "FUSION_midgrad+freq | ep 3/3 | loss=0.1314 | test_f1=0.918 auc=0.986 acc=0.917\n",
            "\n",
            "===================== RUN: FUSION_midgrad+grad_hf =====================\n",
            "FUSION_midgrad+grad_hf | ep 1/3 | loss=0.3814 | test_f1=0.727 auc=0.854 acc=0.700\n",
            "FUSION_midgrad+grad_hf | ep 2/3 | loss=0.2501 | test_f1=0.800 auc=0.903 acc=0.800\n",
            "FUSION_midgrad+grad_hf | ep 3/3 | loss=0.2111 | test_f1=0.824 auc=0.920 acc=0.850\n",
            "\n",
            "===================== RUN: FUSION_midgrad+tgrad =====================\n",
            "FUSION_midgrad+tgrad | ep 1/3 | loss=0.3775 | test_f1=0.732 auc=0.834 acc=0.633\n",
            "FUSION_midgrad+tgrad | ep 2/3 | loss=0.2943 | test_f1=0.951 auc=0.990 acc=0.950\n",
            "FUSION_midgrad+tgrad | ep 3/3 | loss=0.1955 | test_f1=0.811 auc=0.964 acc=0.767\n",
            "\n",
            "===================== RUN: FUSION_mid+freq+midgrad+grad_hf+tgrad =====================\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 1/3 | loss=0.3611 | test_f1=0.882 auc=0.969 acc=0.867\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 2/3 | loss=0.2944 | test_f1=0.951 auc=0.978 acc=0.950\n",
            "FUSION_mid+freq+midgrad+grad_hf+tgrad | ep 3/3 | loss=0.2129 | test_f1=0.933 auc=0.993 acc=0.933\n",
            "\n",
            "==== FINAL SUMMARY (sorted by F1) ====\n",
            "                                    model       acc        f1   roc_auc  \\\n",
            "0                                  TS_mid  0.966667  0.967742  0.981111   \n",
            "1                               R2Res_mid  0.966667  0.966667  0.991111   \n",
            "2                             ViT_midgrad  0.966667  0.965517  0.998889   \n",
            "3                      FUSION_mid+midgrad  0.950000  0.952381  0.992222   \n",
            "4   FUSION_mid+freq+midgrad+grad_hf+tgrad  0.950000  0.950820  0.977778   \n",
            "5                    FUSION_midgrad+tgrad  0.950000  0.950820  0.990000   \n",
            "6                           R2Res_midtone  0.933333  0.928571  0.986667   \n",
            "7                              TS_midgrad  0.916667  0.920635  0.986667   \n",
            "8                     FUSION_midgrad+freq  0.916667  0.918033  0.985556   \n",
            "9                           R2Res_midgrad  0.916667  0.915254  0.983333   \n",
            "10                             R2Res_freq  0.900000  0.900000  0.976667   \n",
            "11                            R2Res_tgrad  0.850000  0.861538  0.958889   \n",
            "12                          R2Res_grad_hf  0.833333  0.852941  0.957778   \n",
            "13                                ViT_mid  0.866667  0.846154  0.828889   \n",
            "14                 FUSION_midgrad+grad_hf  0.850000  0.823529  0.920000   \n",
            "15                            ViT_midtone  0.783333  0.723404  0.815556   \n",
            "16                               ViT_freq  0.500000  0.666667  0.497222   \n",
            "17                              ViT_tgrad  0.500000  0.666667  0.564444   \n",
            "18                            ViT_grad_hf  0.500000  0.666667  0.227778   \n",
            "19                               TS_tgrad  0.666667  0.666667  0.776667   \n",
            "20                                TS_freq  0.500000  0.666667  0.530000   \n",
            "21                             TS_grad_hf  0.500000  0.666667  0.226667   \n",
            "22                             TS_midtone  0.650000  0.533333  0.677778   \n",
            "\n",
            "          ap  \n",
            "0   0.979478  \n",
            "1   0.992184  \n",
            "2   0.998925  \n",
            "3   0.992192  \n",
            "4   0.978476  \n",
            "5   0.990865  \n",
            "6   0.987592  \n",
            "7   0.987158  \n",
            "8   0.985375  \n",
            "9   0.985006  \n",
            "10  0.981019  \n",
            "11  0.957438  \n",
            "12  0.965250  \n",
            "13  0.888437  \n",
            "14  0.940784  \n",
            "15  0.873726  \n",
            "16  0.606651  \n",
            "17  0.555869  \n",
            "18  0.397458  \n",
            "19  0.729714  \n",
            "20  0.558121  \n",
            "21  0.397172  \n",
            "22  0.735464  \n",
            "\n",
            "Saved summary to: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/SUMMARY.csv\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC\n",
            "Clip plan: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_ALLMETHODS_MEANINGFUL_100EACH_FIXEDCLIPS_NOPCA_NOGINC/clip_plan.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BENCHMARK_NEUTRAL_ALLMETHODS_FIXEDCLIPS + PCA + tgrad + Fusion(mid+tgrad)\n",
        "# - Fixed clip plan: same clips used across ALL streams/models\n",
        "# - Streams: mid, midtone, midgrad, grad_hf, freq, tgrad\n",
        "# - Adds: PCA projection (optional) + GINC (optional hook)\n",
        "# - Adds: mid+tgrad fusion for ViT, R2Res, TimeSformer\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm transformers accelerate\n",
        "\n",
        "import os, cv2, json, random, hashlib, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models as tvm\n",
        "from transformers import TimesformerForVideoClassification\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "print(\"Torch device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "RUN_NAME = \"BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS\"\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "\n",
        "# Dataset size\n",
        "N_EACH = 100\n",
        "RAW_MULT = 1.4\n",
        "RAW_EACH = int(np.ceil(N_EACH * RAW_MULT))\n",
        "\n",
        "# Normalization\n",
        "NORM_FPS   = 25\n",
        "NORM_W     = 256\n",
        "NORM_H     = 256\n",
        "NORM_CODEC = \"libx264\"\n",
        "NORM_CRF   = 23\n",
        "NORM_PIX   = \"yuv420p\"\n",
        "NORM_AUDIO = False\n",
        "\n",
        "# Model input sampling\n",
        "T = 8\n",
        "FRAME_STRIDE = 3\n",
        "TARGET_SIZE  = 224\n",
        "\n",
        "# Fixed clip plan counts\n",
        "TRAIN_CLIPS_PER_VIDEO = 3\n",
        "TEST_CLIPS_PER_VIDEO  = 1\n",
        "\n",
        "BATCH_SIZE   = 6\n",
        "EPOCHS       = 3\n",
        "LR           = 3e-4\n",
        "NUM_WORKERS  = 2\n",
        "\n",
        "# Midtone thresholds\n",
        "MID_LO = 0.25\n",
        "MID_HI = 0.75\n",
        "\n",
        "# PCA controls (set these)\n",
        "USE_PCA = True\n",
        "PCA_DIMS = 128  # 64/128/256 typical\n",
        "\n",
        "# Optional GINC hook (kept simple: off by default)\n",
        "USE_GINC = False\n",
        "\n",
        "# Streams\n",
        "STREAMS = [\"mid\", \"midtone\", \"midgrad\", \"grad_hf\", \"freq\", \"tgrad\"]\n",
        "\n",
        "# -----------------------------\n",
        "# 1) LOAD VIDEO PATHS (RAW_EACH + RAW_EACH)\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\",\".webm\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "assert len(real) >= RAW_EACH and (len(ai_core)+len(sora)) >= RAW_EACH, \"Not enough videos.\"\n",
        "\n",
        "AI_ALL = ai_core + sora\n",
        "AI = random.sample(AI_ALL, RAW_EACH)\n",
        "REAL = random.sample(real, RAW_EACH)\n",
        "\n",
        "raw_df = pd.DataFrame({\n",
        "    \"path\": AI + REAL,\n",
        "    \"y\":    [1]*len(AI) + [0]*len(REAL),\n",
        "    \"src\":  [\"AI\"]*len(AI) + [\"REAL\"]*len(REAL),\n",
        "}).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "raw_manifest = os.path.join(RUN_DIR, f\"raw_manifest_{RAW_EACH}each.csv\")\n",
        "raw_df.to_csv(raw_manifest, index=False)\n",
        "print(\"\\nRAW pool:\", len(raw_df), \"total | AI=\", (raw_df.y==1).sum(), \"REAL=\", (raw_df.y==0).sum())\n",
        "print(\"Saved:\", raw_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) HASHING FOR DEDUP\n",
        "# -----------------------------\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path)\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    fr = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return h\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "# -----------------------------\n",
        "# 3) DEDUP\n",
        "# -----------------------------\n",
        "print(\"\\nComputing hashes...\")\n",
        "sha_list, dh_list = [], []\n",
        "for p in tqdm(raw_df.path.tolist(), desc=\"Hashing\"):\n",
        "    sha_list.append(sha1_first_frame(p))\n",
        "    dh_list.append(dhash_first_frame(p))\n",
        "\n",
        "df = raw_df.copy()\n",
        "df[\"sha1\"] = sha_list\n",
        "df[\"dhash\"] = dh_list\n",
        "df = df.dropna(subset=[\"sha1\",\"dhash\"]).reset_index(drop=True)\n",
        "\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"sha1\"]).reset_index(drop=True)\n",
        "print(\"Exact dupes removed:\", before - len(df))\n",
        "\n",
        "def uf_make(n):\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb: return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "    return find, union\n",
        "\n",
        "def bucket_key(h, bits=12):\n",
        "    return int(h) >> (64 - bits)\n",
        "\n",
        "bucket = defaultdict(list)\n",
        "for i, h in enumerate(df.dhash.values):\n",
        "    bucket[bucket_key(int(h))].append(i)\n",
        "\n",
        "find, union = uf_make(len(df))\n",
        "for _, idxs in bucket.items():\n",
        "    if len(idxs) < 2:\n",
        "        continue\n",
        "    if len(idxs) > 250:\n",
        "        idxs = random.sample(idxs, 250)\n",
        "    for ii in range(len(idxs)):\n",
        "        a = idxs[ii]\n",
        "        ha = int(df.loc[a,\"dhash\"])\n",
        "        for jj in range(ii+1, len(idxs)):\n",
        "            b = idxs[jj]\n",
        "            hb = int(df.loc[b,\"dhash\"])\n",
        "            if hamming(ha, hb) <= 3:\n",
        "                union(a, b)\n",
        "\n",
        "df[\"cluster\"] = [find(i) for i in range(len(df))]\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"cluster\"]).reset_index(drop=True)\n",
        "print(\"Near-dupes removed:\", before - len(df))\n",
        "print(\"After dedup:\", len(df))\n",
        "\n",
        "df_ai = df[df.y==1].sample(n=min(N_EACH, (df.y==1).sum()), random_state=SEED)\n",
        "df_rl = df[df.y==0].sample(n=min(N_EACH, (df.y==0).sum()), random_state=SEED)\n",
        "\n",
        "assert len(df_ai) >= N_EACH and len(df_rl) >= N_EACH, (\n",
        "    f\"After dedup you have AI={len(df_ai)} REAL={len(df_rl)}. Increase RAW_MULT or lower N_EACH.\"\n",
        ")\n",
        "\n",
        "df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "print(\"Rebalanced:\", len(df), \"total | AI=\", (df.y==1).sum(), \"REAL=\", (df.y==0).sum())\n",
        "\n",
        "dedup_manifest = os.path.join(RUN_DIR, \"dedup_manifest.csv\")\n",
        "df.to_csv(dedup_manifest, index=False)\n",
        "print(\"Saved:\", dedup_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) NORMALIZE\n",
        "# -----------------------------\n",
        "NORM_DIR = os.path.join(RUN_DIR, \"normalized\")\n",
        "os.makedirs(NORM_DIR, exist_ok=True)\n",
        "\n",
        "def norm_path_for(i):\n",
        "    return os.path.join(NORM_DIR, f\"v_{i:05d}.mp4\")\n",
        "\n",
        "def ffmpeg_normalize(in_path, out_path):\n",
        "    aflag = [\"-an\"] if not NORM_AUDIO else []\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-vf\", f\"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,fps={NORM_FPS}\",\n",
        "        \"-vsync\",\"cfr\",\n",
        "        *aflag,\n",
        "        \"-c:v\", NORM_CODEC,\n",
        "        \"-crf\", str(NORM_CRF),\n",
        "        \"-pix_fmt\", NORM_PIX,\n",
        "        out_path\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "\n",
        "print(\"\\nNormalizing...\")\n",
        "norm_paths = []\n",
        "for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=\"Normalize\")):\n",
        "    outp = norm_path_for(i)\n",
        "    if not os.path.exists(outp):\n",
        "        ffmpeg_normalize(row.path, outp)\n",
        "    norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)\n",
        "\n",
        "ndf = df.copy()\n",
        "ndf[\"norm_path\"] = norm_paths\n",
        "ndf = ndf.dropna(subset=[\"norm_path\"]).reset_index(drop=True)\n",
        "\n",
        "norm_manifest = os.path.join(RUN_DIR, \"normalized_manifest.csv\")\n",
        "ndf.to_csv(norm_manifest, index=False)\n",
        "print(\"Normalized:\", len(ndf), \"Saved:\", norm_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) CLUSTER-SAFE SPLIT\n",
        "# -----------------------------\n",
        "clusters = ndf.cluster.values\n",
        "cluster_to_rows = defaultdict(list)\n",
        "for i,c in enumerate(clusters):\n",
        "    cluster_to_rows[int(c)].append(i)\n",
        "\n",
        "cluster_ids = list(cluster_to_rows.keys())\n",
        "cluster_labels = []\n",
        "for c in cluster_ids:\n",
        "    ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]\n",
        "    cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)\n",
        "\n",
        "c_train, c_test = train_test_split(\n",
        "    cluster_ids, test_size=0.3, random_state=SEED, stratify=cluster_labels\n",
        ")\n",
        "\n",
        "train_idx, test_idx = [], []\n",
        "for c in c_train: train_idx.extend(cluster_to_rows[c])\n",
        "for c in c_test:  test_idx.extend(cluster_to_rows[c])\n",
        "\n",
        "train_df = ndf.iloc[train_idx].reset_index(drop=True)\n",
        "test_df  = ndf.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCluster-safe split:\")\n",
        "print(\" Train:\", len(train_df), \" Test:\", len(test_df))\n",
        "print(\" Train AI/REAL:\", int((train_df.y==1).sum()), \"/\", int((train_df.y==0).sum()))\n",
        "print(\" Test  AI/REAL:\", int((test_df.y==1).sum()), \"/\", int((test_df.y==0).sum()))\n",
        "\n",
        "train_df.to_csv(os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) FIXED CLIP PLAN (critical for \"no bias\" clip selection)\n",
        "# -----------------------------\n",
        "clip_plan_path = os.path.join(RUN_DIR, \"clip_plan.csv\")\n",
        "\n",
        "def get_frame_count(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return 0\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    return n\n",
        "\n",
        "def make_clip_plan(df, split_name, clips_per_video):\n",
        "    rows = []\n",
        "    for i, row in enumerate(df.itertuples(index=False)):\n",
        "        n = get_frame_count(row.norm_path)\n",
        "        if n <= 0:\n",
        "            continue\n",
        "        max_start = max(0, n - (T-1)*FRAME_STRIDE - 1)\n",
        "        rng = random.Random(SEED + (999 if split_name==\"test\" else 111) + i)\n",
        "        for k in range(clips_per_video):\n",
        "            start = rng.randint(0, max_start) if max_start > 0 else 0\n",
        "            rows.append({\n",
        "                \"split\": split_name,\n",
        "                \"video_idx\": i,\n",
        "                \"clip_k\": k,\n",
        "                \"norm_path\": row.norm_path,\n",
        "                \"y\": int(row.y),\n",
        "                \"start\": int(start),\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "if os.path.exists(clip_plan_path):\n",
        "    clip_plan = pd.read_csv(clip_plan_path)\n",
        "    print(\"Loaded existing clip plan:\", clip_plan_path)\n",
        "else:\n",
        "    clip_plan_tr = make_clip_plan(train_df, \"train\", TRAIN_CLIPS_PER_VIDEO)\n",
        "    clip_plan_te = make_clip_plan(test_df,  \"test\",  TEST_CLIPS_PER_VIDEO)\n",
        "    clip_plan = pd.concat([clip_plan_tr, clip_plan_te], axis=0).reset_index(drop=True)\n",
        "    clip_plan.to_csv(clip_plan_path, index=False)\n",
        "    print(\"Saved NEW fixed clip plan:\", clip_plan_path)\n",
        "\n",
        "print(\"Fixed clips:\")\n",
        "print(\" Train clips:\", (clip_plan.split==\"train\").sum(), f\"({TRAIN_CLIPS_PER_VIDEO} per video)\")\n",
        "print(\" Test  clips:\", (clip_plan.split==\"test\").sum(),  f\"({TEST_CLIPS_PER_VIDEO} per video)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) STREAMS\n",
        "# -----------------------------\n",
        "def to_rgb(frame_bgr):\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def resize_center(frame, size):\n",
        "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def luma01_from_bgr(fr_bgr):\n",
        "    fr = fr_bgr.astype(np.float32) / 255.0\n",
        "    b,g,r = fr[:,:,0], fr[:,:,1], fr[:,:,2]\n",
        "    return (0.114*b + 0.587*g + 0.299*r).astype(np.float32)\n",
        "\n",
        "def midtone_mask(luma01, lo=MID_LO, hi=MID_HI):\n",
        "    return ((luma01 >= lo) & (luma01 <= hi)).astype(np.float32)\n",
        "\n",
        "def sobel_mag01(gray01):\n",
        "    g8 = (gray01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    gx = cv2.Sobel(g8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(g8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    mag = cv2.magnitude(gx, gy)\n",
        "    mag = mag / (np.max(mag) + 1e-8)\n",
        "    return mag.astype(np.float32)\n",
        "\n",
        "def fft_highpass_map01(img01, frac_hi=0.45):\n",
        "    f = np.fft.fft2(img01.astype(np.float32))\n",
        "    f = np.fft.fftshift(f)\n",
        "    H,W = img01.shape\n",
        "    cy,cx = H//2, W//2\n",
        "    yy,xx = np.ogrid[:H,:W]\n",
        "    r = np.sqrt((yy-cy)**2 + (xx-cx)**2)\n",
        "    rmax = np.max(r) + 1e-8\n",
        "    hp = (r >= frac_hi*rmax).astype(np.float32)\n",
        "    f_hp = f * hp\n",
        "    f_hp = np.fft.ifftshift(f_hp)\n",
        "    out = np.fft.ifft2(f_hp)\n",
        "    out = np.real(out).astype(np.float32)\n",
        "    out = out - np.min(out)\n",
        "    out = out / (np.max(out) + 1e-8)\n",
        "    return out\n",
        "\n",
        "def map01_to_bgr(map01):\n",
        "    x = (map01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    return cv2.cvtColor(x, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def stream_mid(fr):\n",
        "    return fr\n",
        "\n",
        "def stream_midtone(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    return map01_to_bgr(mid)\n",
        "\n",
        "def stream_midgrad(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    return map01_to_bgr(mag)\n",
        "\n",
        "def stream_grad_hf(fr):\n",
        "    l = luma01_from_bgr(fr)\n",
        "    m = midtone_mask(l)\n",
        "    mid = l * m\n",
        "    mag = sobel_mag01(mid)\n",
        "    hf = fft_highpass_map01(mag, frac_hi=0.45)\n",
        "    return map01_to_bgr(hf)\n",
        "\n",
        "def stream_freq(fr):\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    mag = np.log1p(np.abs(fshift))\n",
        "    mag = mag / (mag.max() + 1e-6)\n",
        "    return map01_to_bgr(mag.astype(np.float32))\n",
        "\n",
        "# Temporal gradient built on midgrad difference (stable)\n",
        "def stream_tgrad(prev_fr, cur_fr):\n",
        "    g_prev = cv2.cvtColor(stream_midgrad(prev_fr), cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    g_cur  = cv2.cvtColor(stream_midgrad(cur_fr),  cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    diff = np.abs(g_cur - g_prev)\n",
        "    diff = diff / (diff.max() + 1e-8)\n",
        "    return map01_to_bgr(diff)\n",
        "\n",
        "STREAM_FN = {\n",
        "    \"mid\": stream_mid,\n",
        "    \"midtone\": stream_midtone,\n",
        "    \"midgrad\": stream_midgrad,\n",
        "    \"grad_hf\": stream_grad_hf,\n",
        "    \"freq\": stream_freq,\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 8) FIXED-CLIP DATASETS (uses clip_plan.csv)\n",
        "# -----------------------------\n",
        "def open_video(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    return cap if cap.isOpened() else None\n",
        "\n",
        "def read_clip_frames_fixed(path, start, T=8, stride=3, size=224):\n",
        "    cap = open_video(path)\n",
        "    if cap is None:\n",
        "        return None\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        idx = int(start + i*stride)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = resize_center(fr, size)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "class ClipPlanSingleStream(Dataset):\n",
        "    def __init__(self, clip_plan_df, stream=\"mid\", T=8, stride=3, size=224):\n",
        "        self.df = clip_plan_df.reset_index(drop=True)\n",
        "        self.stream = stream\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        y = int(row.y)\n",
        "        frames = read_clip_frames_fixed(row.norm_path, row.start, T=self.T, stride=self.stride, size=self.size)\n",
        "\n",
        "        if frames is None:\n",
        "            x = np.zeros((self.T, 3, self.size, self.size), dtype=np.float32)\n",
        "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        if self.stream == \"tgrad\":\n",
        "            proc = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "        else:\n",
        "            proc = [STREAM_FN[self.stream](fr) for fr in frames]\n",
        "\n",
        "        arr = np.stack([to_rgb(fr) for fr in proc], axis=0).astype(np.float32)/255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))\n",
        "        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class ClipPlanTwoStream(Dataset):\n",
        "    \"\"\"Returns (xA, xB, y) using SAME fixed clip frames.\"\"\"\n",
        "    def __init__(self, clip_plan_df, streamA=\"mid\", streamB=\"tgrad\", T=8, stride=3, size=224):\n",
        "        self.df = clip_plan_df.reset_index(drop=True)\n",
        "        self.streamA = streamA\n",
        "        self.streamB = streamB\n",
        "        self.T = T\n",
        "        self.stride = stride\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def _proc_stream(self, frames, stream):\n",
        "        if stream == \"tgrad\":\n",
        "            proc = [stream_tgrad(frames[max(i-1,0)], frames[i]) for i in range(len(frames))]\n",
        "        else:\n",
        "            proc = [STREAM_FN[stream](fr) for fr in frames]\n",
        "        arr = np.stack([to_rgb(fr) for fr in proc], axis=0).astype(np.float32)/255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))\n",
        "        return torch.from_numpy(arr)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        y = int(row.y)\n",
        "        frames = read_clip_frames_fixed(row.norm_path, row.start, T=self.T, stride=self.stride, size=self.size)\n",
        "\n",
        "        z = torch.zeros((self.T,3,self.size,self.size), dtype=torch.float32)\n",
        "        if frames is None:\n",
        "            return z, z, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        xA = self._proc_stream(frames, self.streamA)\n",
        "        xB = self._proc_stream(frames, self.streamB)\n",
        "        return xA, xB, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
        "\n",
        "train_clips = clip_plan[clip_plan.split==\"train\"].reset_index(drop=True)\n",
        "test_clips  = clip_plan[clip_plan.split==\"test\"].reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 9) PCA + GINC HELPERS\n",
        "# -----------------------------\n",
        "class PCALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies: (x - mean) @ components.T   where components shape [k, d]\n",
        "    Stored as buffers so it moves with .to(device).\n",
        "    \"\"\"\n",
        "    def __init__(self, mean_vec, components):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"mean\", torch.tensor(mean_vec, dtype=torch.float32))\n",
        "        self.register_buffer(\"components\", torch.tensor(components, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, d]\n",
        "        x0 = x - self.mean\n",
        "        return x0 @ self.components.t()\n",
        "\n",
        "def ginc_transform(x, strength=0.15):\n",
        "    \"\"\"\n",
        "    Placeholder for a simple \"GINC-like\" perturbation in feature space.\n",
        "    Real GINC is more specific; this is an optional hook.\n",
        "    \"\"\"\n",
        "    if not USE_GINC:\n",
        "        return x\n",
        "    noise = torch.randn_like(x) * strength\n",
        "    return x + noise\n",
        "\n",
        "def fit_pca_on_features(train_feats, n_components=128):\n",
        "    pca = PCA(n_components=n_components, random_state=SEED)\n",
        "    pca.fit(train_feats)\n",
        "    return pca.mean_, pca.components_\n",
        "\n",
        "# -----------------------------\n",
        "# 10) BACKBONES (return pooled features)\n",
        "# -----------------------------\n",
        "class R2ResBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)\n",
        "        self.r2.fc = nn.Identity()\n",
        "        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "        self.res.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,T,C,H,W]\n",
        "        B,T,C,H,W = x.shape\n",
        "        xr2 = x.permute(0,2,1,3,4)           # [B,C,T,H,W]\n",
        "        f_r2 = self.r2(xr2)                  # [B,512]\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.res(xf).view(B, T, -1).mean(dim=1)  # [B,512]\n",
        "        return torch.cat([f_r2, f], dim=1)   # [B,1024]\n",
        "\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vit = tvm.vit_b_16(weights=tvm.ViT_B_16_Weights.DEFAULT)\n",
        "        self.vit.heads = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,T,C,H,W]\n",
        "        B,T,C,H,W = x.shape\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.vit(xf).view(B, T, -1).mean(dim=1)  # [B,768]\n",
        "        return f\n",
        "\n",
        "class TSBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ts = TimesformerForVideoClassification.from_pretrained(\n",
        "            \"facebook/timesformer-base-finetuned-k400\",\n",
        "            num_labels=2,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        # x: [B,T,C,H,W] expected by HF\n",
        "        out = self.ts.timesformer(pixel_values=x, output_hidden_states=False, return_dict=True)\n",
        "        # out.last_hidden_state: [B, 1+T*patches, D]\n",
        "        # Use CLS token as pooled feature:\n",
        "        cls = out.last_hidden_state[:, 0, :]  # [B,768]\n",
        "        return cls\n",
        "\n",
        "# -----------------------------\n",
        "# 11) CLASSIFIERS WITH OPTIONAL PCA\n",
        "# -----------------------------\n",
        "class LinearHead(nn.Module):\n",
        "    def __init__(self, d_in, num_classes=2, pca_layer=None):\n",
        "        super().__init__()\n",
        "        self.pca = pca_layer\n",
        "        d = d_in if pca_layer is None else pca_layer.components.shape[0]\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feat):\n",
        "        feat = ginc_transform(feat)\n",
        "        if self.pca is not None:\n",
        "            feat = self.pca(feat)\n",
        "        return self.head(feat)\n",
        "\n",
        "class SingleStreamModel(nn.Module):\n",
        "    def __init__(self, backbone, d_in, pca_layer=None):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = LinearHead(d_in, pca_layer=pca_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(self.backbone, TSBackbone):\n",
        "            feat = self.backbone.forward_features(x)\n",
        "        else:\n",
        "            feat = self.backbone(x)\n",
        "        return self.head(feat)\n",
        "\n",
        "class TwoStreamFusionModel(nn.Module):\n",
        "    \"\"\"mid + tgrad fusion: concat pooled features, optional PCA, classifier.\"\"\"\n",
        "    def __init__(self, backboneA, backboneB, dA, dB, pca_layer=None):\n",
        "        super().__init__()\n",
        "        self.A = backboneA\n",
        "        self.B = backboneB\n",
        "        self.head = LinearHead(dA + dB, pca_layer=pca_layer)\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        if isinstance(self.A, TSBackbone):\n",
        "            fA = self.A.forward_features(xA)\n",
        "        else:\n",
        "            fA = self.A(xA)\n",
        "\n",
        "        if isinstance(self.B, TSBackbone):\n",
        "            fB = self.B.forward_features(xB)\n",
        "        else:\n",
        "            fB = self.B(xB)\n",
        "\n",
        "        feat = torch.cat([fA, fB], dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "# -----------------------------\n",
        "# 12) TRAIN / EVAL\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, proba):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, pred)),\n",
        "        \"f1\": float(f1_score(y_true, pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"ap\": float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_single(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy()); all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return compute_metrics(y_true, proba)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_two(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for xA, xB, y in loader:\n",
        "        xA = xA.to(DEVICE); xB = xB.to(DEVICE)\n",
        "        logits = model(xA, xB)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy()); all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return compute_metrics(y_true, proba)\n",
        "\n",
        "def train_loop(model, train_loader, test_loader, epochs=3, two_stream=False, name=\"model\"):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses=[]\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if two_stream:\n",
        "                xA, xB, y = batch\n",
        "                xA = xA.to(DEVICE); xB = xB.to(DEVICE); y = y.to(DEVICE)\n",
        "                logits = model(xA, xB)\n",
        "            else:\n",
        "                x, y = batch\n",
        "                x = x.to(DEVICE); y = y.to(DEVICE)\n",
        "                logits = model(x)\n",
        "\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        m = eval_model_two(model, test_loader) if two_stream else eval_model_single(model, test_loader)\n",
        "        print(f\"{name} | ep {ep}/{epochs} | loss={np.mean(losses):.4f} | test_f1={m['f1']:.3f} auc={m['roc_auc']:.3f} acc={m['acc']:.3f}\")\n",
        "\n",
        "        if m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return eval_model_two(model, test_loader) if two_stream else eval_model_single(model, test_loader)\n",
        "\n",
        "# -----------------------------\n",
        "# 13) PCA FITTING UTILITIES (per model/stream)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def collect_features_single(backbone, loader):\n",
        "    backbone = backbone.to(DEVICE)\n",
        "    backbone.eval()\n",
        "    feats = []\n",
        "    for x, _ in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        if isinstance(backbone, TSBackbone):\n",
        "            f = backbone.forward_features(x)\n",
        "        else:\n",
        "            f = backbone(x)\n",
        "        feats.append(f.detach().cpu().numpy())\n",
        "    return np.concatenate(feats, axis=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_features_two(backboneA, backboneB, loader):\n",
        "    backboneA = backboneA.to(DEVICE); backboneB = backboneB.to(DEVICE)\n",
        "    backboneA.eval(); backboneB.eval()\n",
        "    feats = []\n",
        "    for xA, xB, _ in loader:\n",
        "        xA = xA.to(DEVICE); xB = xB.to(DEVICE)\n",
        "        if isinstance(backboneA, TSBackbone):\n",
        "            fA = backboneA.forward_features(xA)\n",
        "        else:\n",
        "            fA = backboneA(xA)\n",
        "        if isinstance(backboneB, TSBackbone):\n",
        "            fB = backboneB.forward_features(xB)\n",
        "        else:\n",
        "            fB = backboneB(xB)\n",
        "        feats.append(torch.cat([fA, fB], dim=1).detach().cpu().numpy())\n",
        "    return np.concatenate(feats, axis=0)\n",
        "\n",
        "# -----------------------------\n",
        "# 14) RUNS: mid PCA for ALL, plus tgrad and mid+tgrad fusion for all backbones\n",
        "# -----------------------------\n",
        "results = []\n",
        "\n",
        "def run_single(kind, stream, use_pca=True):\n",
        "    ds_tr = ClipPlanSingleStream(train_clips, stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = ClipPlanSingleStream(test_clips,  stream=stream, T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    if kind == \"r2res\":\n",
        "        backbone = R2ResBackbone()\n",
        "        d_in = 1024\n",
        "        name = f\"R2Res_{stream}\"\n",
        "    elif kind == \"vit\":\n",
        "        backbone = ViTBackbone()\n",
        "        d_in = 768\n",
        "        name = f\"ViT_{stream}\"\n",
        "    elif kind == \"timesformer\":\n",
        "        backbone = TSBackbone()\n",
        "        d_in = 768\n",
        "        name = f\"TS_{stream}\"\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    pca_layer = None\n",
        "    if USE_PCA and use_pca:\n",
        "        # Fit PCA on TRAIN features\n",
        "        feat_loader = make_loader(ds_tr, shuffle=False)\n",
        "        train_feats = collect_features_single(backbone, feat_loader)\n",
        "        mean, comps = fit_pca_on_features(train_feats, n_components=min(PCA_DIMS, train_feats.shape[1]))\n",
        "        pca_layer = PCALayer(mean, comps)\n",
        "\n",
        "    model = SingleStreamModel(backbone, d_in=d_in, pca_layer=pca_layer)\n",
        "    print(\"\\n===================== RUN:\", name, \"(PCA)\" if pca_layer else \"\", \"=====================\")\n",
        "    met = train_loop(model, tr, te, epochs=EPOCHS, two_stream=False, name=name + (\"_PCA\" if pca_layer else \"\"))\n",
        "    met[\"model\"] = name + (\"_PCA\" if pca_layer else \"\")\n",
        "    results.append(met)\n",
        "\n",
        "def run_mid_tgrad_fusion(kind, use_pca=True):\n",
        "    ds_tr = ClipPlanTwoStream(train_clips, streamA=\"mid\", streamB=\"tgrad\", T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    ds_te = ClipPlanTwoStream(test_clips,  streamA=\"mid\", streamB=\"tgrad\", T=T, stride=FRAME_STRIDE, size=TARGET_SIZE)\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    if kind == \"r2res\":\n",
        "        A = R2ResBackbone(); B = R2ResBackbone()\n",
        "        dA, dB = 1024, 1024\n",
        "        name = \"R2Res_FUSION_mid+tgrad\"\n",
        "    elif kind == \"vit\":\n",
        "        A = ViTBackbone(); B = ViTBackbone()\n",
        "        dA, dB = 768, 768\n",
        "        name = \"ViT_FUSION_mid+tgrad\"\n",
        "    elif kind == \"timesformer\":\n",
        "        A = TSBackbone(); B = TSBackbone()\n",
        "        dA, dB = 768, 768\n",
        "        name = \"TS_FUSION_mid+tgrad\"\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    pca_layer = None\n",
        "    if USE_PCA and use_pca:\n",
        "        feat_loader = make_loader(ds_tr, shuffle=False)\n",
        "        train_feats = collect_features_two(A, B, feat_loader)\n",
        "        mean, comps = fit_pca_on_features(train_feats, n_components=min(PCA_DIMS, train_feats.shape[1]))\n",
        "        pca_layer = PCALayer(mean, comps)\n",
        "\n",
        "    model = TwoStreamFusionModel(A, B, dA=dA, dB=dB, pca_layer=pca_layer)\n",
        "    print(\"\\n===================== RUN:\", name, \"(PCA)\" if pca_layer else \"\", \"=====================\")\n",
        "    met = train_loop(model, tr, te, epochs=EPOCHS, two_stream=True, name=name + (\"_PCA\" if pca_layer else \"\"))\n",
        "    met[\"model\"] = name + (\"_PCA\" if pca_layer else \"\")\n",
        "    results.append(met)\n",
        "\n",
        "# Run: mid stream WITH PCA for all\n",
        "for k in [\"r2res\", \"vit\", \"timesformer\"]:\n",
        "    run_single(k, \"mid\", use_pca=True)\n",
        "\n",
        "# Run: tgrad stream WITH PCA for all\n",
        "for k in [\"r2res\", \"vit\", \"timesformer\"]:\n",
        "    run_single(k, \"tgrad\", use_pca=True)\n",
        "\n",
        "# Run: fusion mid+tgrad WITH PCA for all\n",
        "for k in [\"r2res\", \"vit\", \"timesformer\"]:\n",
        "    run_mid_tgrad_fusion(k, use_pca=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 15) SAVE SUMMARY\n",
        "# -----------------------------\n",
        "res_df = pd.DataFrame(results).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
        "summary_csv = os.path.join(RUN_DIR, \"SUMMARY_PCA_TGRAD_FUSION.csv\")\n",
        "res_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "print(\"\\n==== FINAL SUMMARY (sorted by F1) ====\")\n",
        "print(res_df[[\"model\",\"acc\",\"f1\",\"roc_auc\",\"ap\"]])\n",
        "print(\"\\nSaved summary to:\", summary_csv)\n",
        "print(\"Run folder:\", RUN_DIR)\n",
        "print(\"Clip plan:\", clip_plan_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nft_rvZLs4e",
        "outputId": "9fa36da4-36a5-451f-e676-2a69653131e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "GPU: NVIDIA L4\n",
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "RAW pool: 280 total | AI= 140 REAL= 140\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/raw_manifest_140each.csv\n",
            "\n",
            "Computing hashes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280/280 [01:11<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact dupes removed: 0\n",
            "Near-dupes removed: 1\n",
            "After dedup: 278\n",
            "Rebalanced: 200 total | AI= 100 REAL= 100\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/dedup_manifest.csv\n",
            "\n",
            "Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:43<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: 200 Saved: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/normalized_manifest.csv\n",
            "\n",
            "Cluster-safe split:\n",
            " Train: 140  Test: 60\n",
            " Train AI/REAL: 70 / 70\n",
            " Test  AI/REAL: 30 / 30\n",
            "Saved NEW fixed clip plan: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/clip_plan.csv\n",
            "Fixed clips:\n",
            " Train clips: 420 (3 per video)\n",
            " Test  clips: 60 (1 per video)\n",
            "\n",
            "===================== RUN: R2Res_mid (PCA) =====================\n",
            "R2Res_mid_PCA | ep 1/3 | loss=0.2508 | test_f1=0.984 auc=0.999 acc=0.983\n",
            "R2Res_mid_PCA | ep 2/3 | loss=0.1943 | test_f1=0.967 auc=0.999 acc=0.967\n",
            "R2Res_mid_PCA | ep 3/3 | loss=0.1139 | test_f1=0.952 auc=0.999 acc=0.950\n",
            "\n",
            "===================== RUN: ViT_mid (PCA) =====================\n",
            "ViT_mid_PCA | ep 1/3 | loss=0.6317 | test_f1=0.286 auc=0.887 acc=0.583\n",
            "ViT_mid_PCA | ep 2/3 | loss=0.4774 | test_f1=0.889 auc=0.898 acc=0.900\n",
            "ViT_mid_PCA | ep 3/3 | loss=0.3697 | test_f1=0.909 auc=0.919 acc=0.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_mid (PCA) =====================\n",
            "TS_mid_PCA | ep 1/3 | loss=0.4338 | test_f1=0.915 auc=0.974 acc=0.917\n",
            "TS_mid_PCA | ep 2/3 | loss=0.3446 | test_f1=0.918 auc=0.961 acc=0.917\n",
            "TS_mid_PCA | ep 3/3 | loss=0.6013 | test_f1=0.815 auc=0.934 acc=0.833\n",
            "\n",
            "===================== RUN: R2Res_tgrad (PCA) =====================\n",
            "R2Res_tgrad_PCA | ep 1/3 | loss=0.4108 | test_f1=0.868 auc=0.972 acc=0.883\n",
            "R2Res_tgrad_PCA | ep 2/3 | loss=0.1974 | test_f1=0.881 auc=0.924 acc=0.883\n",
            "R2Res_tgrad_PCA | ep 3/3 | loss=0.2039 | test_f1=0.889 auc=0.966 acc=0.900\n",
            "\n",
            "===================== RUN: ViT_tgrad (PCA) =====================\n",
            "ViT_tgrad_PCA | ep 1/3 | loss=0.7186 | test_f1=0.667 auc=0.696 acc=0.500\n",
            "ViT_tgrad_PCA | ep 2/3 | loss=0.6511 | test_f1=0.368 auc=0.753 acc=0.600\n",
            "ViT_tgrad_PCA | ep 3/3 | loss=0.5891 | test_f1=0.800 auc=0.842 acc=0.767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_tgrad (PCA) =====================\n",
            "TS_tgrad_PCA | ep 1/3 | loss=0.6691 | test_f1=0.667 auc=0.903 acc=0.500\n",
            "TS_tgrad_PCA | ep 2/3 | loss=0.6599 | test_f1=0.643 auc=0.764 acc=0.667\n",
            "TS_tgrad_PCA | ep 3/3 | loss=0.6443 | test_f1=0.674 auc=0.697 acc=0.517\n",
            "\n",
            "===================== RUN: R2Res_FUSION_mid+tgrad (PCA) =====================\n",
            "R2Res_FUSION_mid+tgrad_PCA | ep 1/3 | loss=0.2445 | test_f1=0.909 auc=0.977 acc=0.917\n",
            "R2Res_FUSION_mid+tgrad_PCA | ep 2/3 | loss=0.2111 | test_f1=0.951 auc=0.996 acc=0.950\n",
            "R2Res_FUSION_mid+tgrad_PCA | ep 3/3 | loss=0.0879 | test_f1=0.983 auc=1.000 acc=0.983\n",
            "\n",
            "===================== RUN: ViT_FUSION_mid+tgrad (PCA) =====================\n",
            "ViT_FUSION_mid+tgrad_PCA | ep 1/3 | loss=0.6329 | test_f1=0.333 auc=0.874 acc=0.600\n",
            "ViT_FUSION_mid+tgrad_PCA | ep 2/3 | loss=0.5075 | test_f1=0.868 auc=0.928 acc=0.883\n",
            "ViT_FUSION_mid+tgrad_PCA | ep 3/3 | loss=0.3901 | test_f1=0.868 auc=0.914 acc=0.883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== RUN: TS_FUSION_mid+tgrad (PCA) =====================\n",
            "TS_FUSION_mid+tgrad_PCA | ep 1/3 | loss=0.4413 | test_f1=0.909 auc=0.997 acc=0.917\n",
            "TS_FUSION_mid+tgrad_PCA | ep 2/3 | loss=0.2263 | test_f1=0.921 auc=0.991 acc=0.917\n",
            "TS_FUSION_mid+tgrad_PCA | ep 3/3 | loss=0.3100 | test_f1=0.667 auc=0.838 acc=0.500\n",
            "\n",
            "==== FINAL SUMMARY (sorted by F1) ====\n",
            "                        model       acc        f1   roc_auc        ap\n",
            "0               R2Res_mid_PCA  0.983333  0.983607  0.998889  0.998925\n",
            "1  R2Res_FUSION_mid+tgrad_PCA  0.983333  0.983051  1.000000  1.000000\n",
            "2     TS_FUSION_mid+tgrad_PCA  0.916667  0.920635  0.991111  0.992038\n",
            "3                  TS_mid_PCA  0.916667  0.918033  0.961111  0.940741\n",
            "4                 ViT_mid_PCA  0.916667  0.909091  0.918889  0.943458\n",
            "5             R2Res_tgrad_PCA  0.900000  0.888889  0.965556  0.975424\n",
            "6    ViT_FUSION_mid+tgrad_PCA  0.883333  0.867925  0.927778  0.945805\n",
            "7               ViT_tgrad_PCA  0.766667  0.800000  0.842222  0.846317\n",
            "8                TS_tgrad_PCA  0.516667  0.674157  0.696667  0.731301\n",
            "\n",
            "Saved summary to: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/SUMMARY_PCA_TGRAD_FUSION.csv\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS\n",
            "Clip plan: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_NEUTRAL_FIXEDCLIPS_PCA_TGRAD_FUSIONS/clip_plan.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SCALETEST_R2RES_MID_PCA_GINC  (COMPLETE + FIXED)\n",
        "# - FixedClip plan (same clips for every stream/method)\n",
        "# - R2Res_mid baseline (no PCA)\n",
        "# - R2Res_mid + PCA (feature-space bottleneck) + MLP head\n",
        "# - Optional: GINC (simple \"global intensity noise control\") augmentation\n",
        "# - FIX: pandas .T bug (never use r.T; use r[\"clip_T\"])\n",
        "#\n",
        "# Works in Colab. Uses your Drive video folders.\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm\n",
        "\n",
        "import os, cv2, random, hashlib, subprocess, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from torchvision import models as tvm\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "print(\"Torch device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---- Your data roots ----\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "RUN_NAME = \"SCALETEST_R2RES_MID_PCA_GINC_FIXED\"\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "\n",
        "# ---- Scale test sizes ----\n",
        "# You can run multiple sizes in one go (fast to compare scaling).\n",
        "SIZES = [200]   # means N_EACH = 200 (AI=200, REAL=200). Add 400, 600, etc.\n",
        "\n",
        "# oversample pool BEFORE dedup to survive duplicate removal\n",
        "RAW_MULT = 1.4\n",
        "\n",
        "# ---- Normalize settings (metadata-neutral) ----\n",
        "NORM_FPS   = 25\n",
        "NORM_W     = 256\n",
        "NORM_H     = 256\n",
        "NORM_CODEC = \"libx264\"\n",
        "NORM_CRF   = 23\n",
        "NORM_PIX   = \"yuv420p\"\n",
        "NORM_AUDIO = False\n",
        "\n",
        "# ---- Clip sampling plan ----\n",
        "CLIP_T = 8\n",
        "FRAME_STRIDE = 3\n",
        "TARGET_SIZE  = 224\n",
        "\n",
        "# Fixed clips per video:\n",
        "TRAIN_CLIPS_PER_VIDEO = 3\n",
        "TEST_CLIPS_PER_VIDEO  = 1\n",
        "\n",
        "# ---- Training ----\n",
        "BATCH_SIZE  = 6\n",
        "EPOCHS      = 3\n",
        "LR          = 3e-4\n",
        "NUM_WORKERS = 2   # set 0 if debugging\n",
        "\n",
        "# ---- PCA config ----\n",
        "USE_PCA = True\n",
        "PCA_DIM = 128  # bottleneck dimension\n",
        "# We'll fit PCA on TRAIN features only, then transform train/test.\n",
        "\n",
        "# ---- GINC config ----\n",
        "# \"GINC\" here is a controlled brightness/contrast + mild noise augmentation\n",
        "# applied consistently, to test robustness (you can turn it off).\n",
        "USE_GINC = True\n",
        "GINC_P = 0.50        # probability apply per clip\n",
        "GINC_NOISE_STD = 0.02  # noise in [0..1] scale\n",
        "GINC_BRIGHT = 0.10      # +/- brightness\n",
        "GINC_CONTRAST = 0.10    # +/- contrast\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Helpers: listing, hashing, dedup\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\",\".webm\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path)\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    fr = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return int(h)\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "def uf_make(n):\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb: return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "    return find, union\n",
        "\n",
        "def bucket_key(h, bits=12):\n",
        "    # dhash is 64-bit-ish in practice; shift to bucket\n",
        "    return int(h) >> max(0, (64 - bits))\n",
        "\n",
        "# -----------------------------\n",
        "# 2) ffmpeg normalization\n",
        "# -----------------------------\n",
        "def ffmpeg_normalize(in_path, out_path):\n",
        "    aflag = [\"-an\"] if not NORM_AUDIO else []\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-vf\", f\"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,\"\n",
        "               f\"pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,fps={NORM_FPS}\",\n",
        "        \"-vsync\",\"cfr\",\n",
        "        *aflag,\n",
        "        \"-c:v\", NORM_CODEC,\n",
        "        \"-crf\", str(NORM_CRF),\n",
        "        \"-pix_fmt\", NORM_PIX,\n",
        "        out_path\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Fixed clip plan builder\n",
        "# -----------------------------\n",
        "def get_frame_count(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return 0\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    return n\n",
        "\n",
        "def build_fixed_clip_plan(ndf, train_df, test_df,\n",
        "                          T=CLIP_T, stride=FRAME_STRIDE,\n",
        "                          train_k=TRAIN_CLIPS_PER_VIDEO, test_k=TEST_CLIPS_PER_VIDEO):\n",
        "    \"\"\"\n",
        "    Returns clip_plan dataframe with columns:\n",
        "    norm_path, y, split, start, clip_T, stride\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    rng = random.Random(SEED)\n",
        "\n",
        "    def add_for_df(df_split, split_name, k_per_video):\n",
        "        for r in df_split.itertuples(index=False):\n",
        "            n = get_frame_count(r.norm_path)\n",
        "            if n <= 0:\n",
        "                continue\n",
        "            max_start = max(0, n - (T-1)*stride - 1)\n",
        "            for _ in range(k_per_video):\n",
        "                start = rng.randint(0, max_start) if max_start > 0 else 0\n",
        "                rows.append({\n",
        "                    \"norm_path\": r.norm_path,\n",
        "                    \"y\": int(r.y),\n",
        "                    \"split\": split_name,\n",
        "                    \"start\": int(start),\n",
        "                    \"clip_T\": int(T),\n",
        "                    \"stride\": int(stride),\n",
        "                })\n",
        "\n",
        "    add_for_df(train_df, \"train\", train_k)\n",
        "    add_for_df(test_df,  \"test\",  test_k)\n",
        "\n",
        "    clip_plan = pd.DataFrame(rows)\n",
        "    return clip_plan\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Streams / preprocessing\n",
        "# -----------------------------\n",
        "def to_rgb(frame_bgr):\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def resize_center(frame, size):\n",
        "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def stream_mid(fr_bgr):\n",
        "    return fr_bgr\n",
        "\n",
        "STREAMS = {\"mid\": stream_mid}\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Read fixed frames\n",
        "# -----------------------------\n",
        "def read_frames_fixed(path, start, T, stride, size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        idx = int(start + i*stride)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = resize_center(fr, size)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# -----------------------------\n",
        "# 6) GINC augmentation (optional)\n",
        "# -----------------------------\n",
        "def apply_ginc_numpy(arr_tchw):\n",
        "    \"\"\"\n",
        "    arr_tchw: numpy float32 in [0,1], shape (T,C,H,W)\n",
        "    Applies mild brightness/contrast + gaussian noise.\n",
        "    \"\"\"\n",
        "    # contrast\n",
        "    c = 1.0 + random.uniform(-GINC_CONTRAST, GINC_CONTRAST)\n",
        "    # brightness\n",
        "    b = random.uniform(-GINC_BRIGHT, GINC_BRIGHT)\n",
        "    out = arr_tchw * c + b\n",
        "    # noise\n",
        "    noise = np.random.normal(0.0, GINC_NOISE_STD, size=out.shape).astype(np.float32)\n",
        "    out = out + noise\n",
        "    out = np.clip(out, 0.0, 1.0)\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Dataset (FIXED .T bug)\n",
        "# -----------------------------\n",
        "class FixedClipDataset(Dataset):\n",
        "    def __init__(self, clip_plan_df, split=\"train\", stream=\"mid\", size=224, ginc=False):\n",
        "        self.df = clip_plan_df[clip_plan_df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.stream = stream\n",
        "        self.size = size\n",
        "        self.ginc = ginc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.df.iloc[i]\n",
        "\n",
        "        norm_path = r[\"norm_path\"]\n",
        "        start     = int(r[\"start\"])\n",
        "        stride    = int(r[\"stride\"])\n",
        "        T_        = int(r[\"clip_T\"])      # âœ… FIX: never use r.T\n",
        "        y         = int(r[\"y\"])\n",
        "\n",
        "        frames = read_frames_fixed(norm_path, start, T=T_, stride=stride, size=self.size)\n",
        "\n",
        "        if frames is None:\n",
        "            x = np.zeros((T_, 3, self.size, self.size), dtype=np.float32)\n",
        "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        fn = STREAMS[self.stream]\n",
        "        proc = [fn(fr) for fr in frames]\n",
        "\n",
        "        arr = np.stack([to_rgb(fr) for fr in proc], axis=0).astype(np.float32) / 255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))  # (T,C,H,W)\n",
        "\n",
        "        if self.ginc and (random.random() < GINC_P):\n",
        "            arr = apply_ginc_numpy(arr)\n",
        "\n",
        "        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Models\n",
        "# -----------------------------\n",
        "class R2ResNetFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Base: R2Plus1D-18 for spatiotemporal + ResNet18 for per-frame\n",
        "    Concatenate features -> classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)\n",
        "        self.r2.fc = nn.Identity()  # 512\n",
        "\n",
        "        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "        self.res.fc = nn.Identity() # 512\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,C,H,W)\n",
        "        B,T,C,H,W = x.shape\n",
        "        xr2 = x.permute(0,2,1,3,4)         # (B,C,T,H,W)\n",
        "        f_r2 = self.r2(xr2)                # (B,512)\n",
        "\n",
        "        xf = x.reshape(B*T, C, H, W)       # (B*T,C,H,W)\n",
        "        f = self.res(xf).view(B, T, -1).mean(dim=1)  # (B,512)\n",
        "\n",
        "        feat = torch.cat([f_r2, f], dim=1) # (B,1024)\n",
        "        return self.head(feat)\n",
        "\n",
        "class R2Res_PCA_MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses SAME backbone as R2ResNetFusion, but:\n",
        "    backbone features -> PCA projection (fixed, not learned) -> MLP head.\n",
        "    PCA is applied outside torch (sklearn) for stability. This module\n",
        "    just expects already-projected features if you want.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=PCA_DIM, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, PCA_DIM)\n",
        "        return self.head(z)\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Metrics / Train helpers\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, proba):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    out = {\n",
        "        \"acc\": float(accuracy_score(y_true, pred)),\n",
        "        \"f1\": float(f1_score(y_true, pred)),\n",
        "    }\n",
        "    out[\"roc_auc\"] = float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
        "    out[\"ap\"]      = float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_logits(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy())\n",
        "        all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return compute_metrics(y_true, proba)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=3, name=\"model\"):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        m = eval_logits(model, test_loader)\n",
        "        print(f\"{name} | ep {ep}/{epochs} | loss={np.mean(losses):.4f} | test_f1={m['f1']:.3f} auc={m['roc_auc']:.3f} acc={m['acc']:.3f}\")\n",
        "\n",
        "        if m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return eval_logits(model, test_loader)\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Feature extraction for PCA\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def extract_r2res_features(model_backbone, loader):\n",
        "    \"\"\"\n",
        "    Extracts the 1024-d pre-head feature used by R2ResNetFusion.\n",
        "    We'll reproduce the backbone computations, then return features.\n",
        "    \"\"\"\n",
        "    model_backbone.eval()\n",
        "    feats = []\n",
        "    ys = []\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        B,T,C,H,W = x.shape\n",
        "\n",
        "        # r2plus1d\n",
        "        xr2 = x.permute(0,2,1,3,4)\n",
        "        f_r2 = model_backbone.r2(xr2)  # (B,512)\n",
        "\n",
        "        # resnet per-frame\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = model_backbone.res(xf).view(B, T, -1).mean(dim=1)  # (B,512)\n",
        "\n",
        "        feat = torch.cat([f_r2, f], dim=1)  # (B,1024)\n",
        "        feats.append(feat.detach().cpu().numpy())\n",
        "        ys.append(y.numpy())\n",
        "\n",
        "    X = np.concatenate(feats, axis=0)\n",
        "    y = np.concatenate(ys, axis=0)\n",
        "    return X, y\n",
        "\n",
        "def train_eval_pca_mlp(train_loader, test_loader, pca_dim=128, name=\"R2Res_mid_PCA_MLP\"):\n",
        "    \"\"\"\n",
        "    Pipeline:\n",
        "      1) build backbone (same as R2ResNetFusion) and train it a bit? (optional)\n",
        "      2) extract features from backbone (frozen)\n",
        "      3) fit PCA on train features\n",
        "      4) train an MLP head on PCA features\n",
        "    Practical: we DO NOT train backbone here (for speed/clarity).\n",
        "    If you want backbone finetune, do baseline training first and reuse weights.\n",
        "    \"\"\"\n",
        "    # Create backbone with pretrained weights\n",
        "    backbone = R2ResNetFusion(num_classes=2).to(DEVICE)\n",
        "    # Replace head with identity so forward isn't used; we call .r2 and .res directly.\n",
        "    backbone.head = nn.Identity()\n",
        "\n",
        "    # Extract features\n",
        "    Xtr, ytr = extract_r2res_features(backbone, train_loader)\n",
        "    Xte, yte = extract_r2res_features(backbone, test_loader)\n",
        "\n",
        "    # Fit PCA on train only\n",
        "    pca = PCA(n_components=pca_dim, random_state=SEED)\n",
        "    Ztr = pca.fit_transform(Xtr)\n",
        "    Zte = pca.transform(Xte)\n",
        "\n",
        "    # MLP head\n",
        "    head = R2Res_PCA_MLP(in_dim=pca_dim, num_classes=2).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(head.parameters(), lr=LR)\n",
        "\n",
        "    # Make tensors\n",
        "    Ztr_t = torch.from_numpy(Ztr).float()\n",
        "    ytr_t = torch.from_numpy(ytr).long()\n",
        "    Zte_t = torch.from_numpy(Zte).float()\n",
        "    yte_t = torch.from_numpy(yte).long()\n",
        "\n",
        "    # simple minibatching\n",
        "    def iterate_batches(Z, y, bs):\n",
        "        idx = np.arange(len(Z))\n",
        "        np.random.shuffle(idx)\n",
        "        for s in range(0, len(Z), bs):\n",
        "            j = idx[s:s+bs]\n",
        "            yield Z[j], y[j]\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        head.train()\n",
        "        losses = []\n",
        "        for zb, yb in iterate_batches(Ztr_t, ytr_t, BATCH_SIZE*4):\n",
        "            zb = zb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = head(zb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # eval\n",
        "        head.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = head(Zte_t.to(DEVICE))\n",
        "            p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        m = compute_metrics(yte, p)\n",
        "\n",
        "        print(f\"{name} | ep {ep}/{EPOCHS} | loss={np.mean(losses):.4f} | test_f1={m['f1']:.3f} auc={m['roc_auc']:.3f} acc={m['acc']:.3f}\")\n",
        "\n",
        "        if m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in head.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        head.load_state_dict(best_state)\n",
        "\n",
        "    # final metrics\n",
        "    head.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = head(Zte_t.to(DEVICE))\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "    m = compute_metrics(yte, p)\n",
        "\n",
        "    return m, pca\n",
        "\n",
        "# -----------------------------\n",
        "# 11) Main loop: build dataset, normalize, fixed clips, run models\n",
        "# -----------------------------\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "\n",
        "AI_ALL = ai_core + sora\n",
        "assert len(AI_ALL) > 0 and len(real) > 0, \"Video folders empty or wrong paths.\"\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for N_EACH in SIZES:\n",
        "    tag = f\"N{N_EACH}each\"\n",
        "    print(\"\\n\" + \"=\"*24, \"DATASET\", tag, \"=\"*24)\n",
        "\n",
        "    RAW_EACH = int(math.ceil(N_EACH * RAW_MULT))\n",
        "    assert len(real) >= RAW_EACH and len(AI_ALL) >= RAW_EACH, f\"Not enough videos for {tag}\"\n",
        "\n",
        "    AI = random.sample(AI_ALL, RAW_EACH)\n",
        "    RL = random.sample(real, RAW_EACH)\n",
        "\n",
        "    raw_df = pd.DataFrame({\n",
        "        \"path\": AI + RL,\n",
        "        \"y\":    [1]*len(AI) + [0]*len(RL),\n",
        "        \"src\":  [\"AI\"]*len(AI) + [\"REAL\"]*len(RL),\n",
        "    }).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "    raw_df.to_csv(os.path.join(RUN_DIR, f\"raw_manifest_{tag}.csv\"), index=False)\n",
        "\n",
        "    # ---- Hash & dedup ----\n",
        "    sha_list, dh_list = [], []\n",
        "    for p in tqdm(raw_df.path.tolist(), desc=f\"[{tag}] hashing\"):\n",
        "        sha_list.append(sha1_first_frame(p))\n",
        "        dh_list.append(dhash_first_frame(p))\n",
        "\n",
        "    df = raw_df.copy()\n",
        "    df[\"sha1\"] = sha_list\n",
        "    df[\"dhash\"] = dh_list\n",
        "    df = df.dropna(subset=[\"sha1\",\"dhash\"]).reset_index(drop=True)\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"sha1\"]).reset_index(drop=True)\n",
        "\n",
        "    bucket = defaultdict(list)\n",
        "    for i, h in enumerate(df.dhash.values):\n",
        "        bucket[bucket_key(int(h))].append(i)\n",
        "\n",
        "    find, union = uf_make(len(df))\n",
        "    for _, idxs in bucket.items():\n",
        "        if len(idxs) < 2:\n",
        "            continue\n",
        "        if len(idxs) > 250:\n",
        "            idxs = random.sample(idxs, 250)\n",
        "        for ii in range(len(idxs)):\n",
        "            a = idxs[ii]\n",
        "            ha = int(df.loc[a,\"dhash\"])\n",
        "            for jj in range(ii+1, len(idxs)):\n",
        "                b = idxs[jj]\n",
        "                hb = int(df.loc[b,\"dhash\"])\n",
        "                if hamming(ha, hb) <= 3:\n",
        "                    union(a, b)\n",
        "\n",
        "    df[\"cluster\"] = [find(i) for i in range(len(df))]\n",
        "    before2 = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"cluster\"]).reset_index(drop=True)\n",
        "\n",
        "    # ---- Rebalance after dedup ----\n",
        "    df_ai = df[df.y==1].sample(n=min(N_EACH, int((df.y==1).sum())), random_state=SEED)\n",
        "    df_rl = df[df.y==0].sample(n=min(N_EACH, int((df.y==0).sum())), random_state=SEED)\n",
        "    assert len(df_ai) >= N_EACH and len(df_rl) >= N_EACH, (\n",
        "        f\"After dedup: AI={len(df_ai)} REAL={len(df_rl)}. Increase RAW_MULT or lower N_EACH.\"\n",
        "    )\n",
        "    df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "    df.to_csv(os.path.join(RUN_DIR, f\"dedup_manifest_{tag}.csv\"), index=False)\n",
        "\n",
        "    # ---- Normalize ----\n",
        "    NORM_DIR = os.path.join(RUN_DIR, f\"normalized_{tag}\")\n",
        "    os.makedirs(NORM_DIR, exist_ok=True)\n",
        "\n",
        "    def norm_path_for(i):\n",
        "        return os.path.join(NORM_DIR, f\"v_{i:05d}.mp4\")\n",
        "\n",
        "    norm_paths = []\n",
        "    for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=f\"[{tag}] normalize\")):\n",
        "        outp = norm_path_for(i)\n",
        "        if not os.path.exists(outp):\n",
        "            ffmpeg_normalize(row.path, outp)\n",
        "        norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)\n",
        "\n",
        "    ndf = df.copy()\n",
        "    ndf[\"norm_path\"] = norm_paths\n",
        "    ndf = ndf.dropna(subset=[\"norm_path\"]).reset_index(drop=True)\n",
        "\n",
        "    ndf.to_csv(os.path.join(RUN_DIR, f\"normalized_manifest_{tag}.csv\"), index=False)\n",
        "\n",
        "    # ---- Cluster-safe split ----\n",
        "    clusters = ndf.cluster.values\n",
        "    cluster_to_rows = defaultdict(list)\n",
        "    for i,c in enumerate(clusters):\n",
        "        cluster_to_rows[int(c)].append(i)\n",
        "\n",
        "    cluster_ids = list(cluster_to_rows.keys())\n",
        "    cluster_labels = []\n",
        "    for c in cluster_ids:\n",
        "        ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]\n",
        "        cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)\n",
        "\n",
        "    c_train, c_test = train_test_split(\n",
        "        cluster_ids, test_size=0.3, random_state=SEED, stratify=cluster_labels\n",
        "    )\n",
        "\n",
        "    train_idx, test_idx = [], []\n",
        "    for c in c_train: train_idx.extend(cluster_to_rows[c])\n",
        "    for c in c_test:  test_idx.extend(cluster_to_rows[c])\n",
        "\n",
        "    train_df = ndf.iloc[train_idx].reset_index(drop=True)\n",
        "    test_df  = ndf.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "    # ---- Build fixed clip plan (SAME clips for all methods) ----\n",
        "    clip_plan = build_fixed_clip_plan(ndf, train_df, test_df)\n",
        "    clip_plan_path = os.path.join(RUN_DIR, f\"clip_plan_{tag}.csv\")\n",
        "    clip_plan.to_csv(clip_plan_path, index=False)\n",
        "\n",
        "    print(f\"\\n[{tag}] Fixed clip plan saved:\", clip_plan_path)\n",
        "    print(f\"[{tag}] Train clips:\", (clip_plan.split==\"train\").sum(), f\"({TRAIN_CLIPS_PER_VIDEO} per video)\")\n",
        "    print(f\"[{tag}] Test  clips:\", (clip_plan.split==\"test\").sum(),  f\"({TEST_CLIPS_PER_VIDEO} per video)\")\n",
        "\n",
        "    # ---- Loaders (mid stream) ----\n",
        "    ds_tr = FixedClipDataset(clip_plan, split=\"train\", stream=\"mid\", size=TARGET_SIZE, ginc=USE_GINC)\n",
        "    ds_te = FixedClipDataset(clip_plan, split=\"test\",  stream=\"mid\", size=TARGET_SIZE, ginc=False) # no aug at test\n",
        "\n",
        "    tr = make_loader(ds_tr, shuffle=True)\n",
        "    te = make_loader(ds_te, shuffle=False)\n",
        "\n",
        "    # ---- Baseline: R2Res_mid (no PCA) ----\n",
        "    print(f\"\\n--- R2Res_mid (no PCA) [{tag}] ---\")\n",
        "    m_base = train_model(R2ResNetFusion(), tr, te, epochs=EPOCHS, name=f\"R2Res_mid_{tag}\")\n",
        "    m_base[\"model\"] = \"R2Res_mid\"\n",
        "    m_base[\"tag\"] = tag\n",
        "    m_base[\"use_pca\"] = False\n",
        "    m_base[\"use_ginc\"] = bool(USE_GINC)\n",
        "    all_results.append(m_base)\n",
        "\n",
        "    # ---- PCA+MLP head on frozen pretrained backbone features ----\n",
        "    if USE_PCA:\n",
        "        print(f\"\\n--- R2Res_mid PCA+MLP [{tag}] ---\")\n",
        "        m_pca, pca_obj = train_eval_pca_mlp(tr, te, pca_dim=PCA_DIM, name=f\"R2Res_mid_PCA_MLP_{tag}\")\n",
        "        m_pca[\"model\"] = \"R2Res_mid_PCA_MLP\"\n",
        "        m_pca[\"tag\"] = tag\n",
        "        m_pca[\"use_pca\"] = True\n",
        "        m_pca[\"pca_dim\"] = PCA_DIM\n",
        "        m_pca[\"use_ginc\"] = bool(USE_GINC)\n",
        "        all_results.append(m_pca)\n",
        "\n",
        "        # Save PCA object\n",
        "        import joblib\n",
        "        pca_path = os.path.join(RUN_DIR, f\"pca_{tag}_dim{PCA_DIM}.joblib\")\n",
        "        joblib.dump(pca_obj, pca_path)\n",
        "        print(f\"[{tag}] Saved PCA:\", pca_path)\n",
        "\n",
        "# -----------------------------\n",
        "# 12) Save summary\n",
        "# -----------------------------\n",
        "res_df = pd.DataFrame(all_results).sort_values([\"tag\",\"f1\"], ascending=[True, False]).reset_index(drop=True)\n",
        "summary_csv = os.path.join(RUN_DIR, \"SUMMARY_SCALETEST_FIXED.csv\")\n",
        "res_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "print(\"\\n==== SUMMARY (sorted by tag then F1) ====\")\n",
        "print(res_df[[\"tag\",\"model\",\"acc\",\"f1\",\"roc_auc\",\"ap\",\"use_pca\",\"use_ginc\"]])\n",
        "print(\"\\nSaved summary to:\", summary_csv)\n",
        "print(\"Run folder:\", RUN_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0flpiPk8hpXb",
        "outputId": "0c85dd64-2786-4287-b019-1c22a7fccbf2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "GPU: Tesla T4\n",
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/SCALETEST_R2RES_MID_PCA_GINC_FIXED\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "======================== DATASET N200each ========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[N200each] hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 560/560 [01:34<00:00,  5.94it/s]\n",
            "[N200each] normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [03:57<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[N200each] Fixed clip plan saved: /content/drive/MyDrive/ai_detection_eval/SCALETEST_R2RES_MID_PCA_GINC_FIXED/clip_plan_N200each.csv\n",
            "[N200each] Train clips: 840 (3 per video)\n",
            "[N200each] Test  clips: 120 (1 per video)\n",
            "\n",
            "--- R2Res_mid (no PCA) [N200each] ---\n",
            "R2Res_mid_N200each | ep 1/3 | loss=0.3173 | test_f1=0.951 auc=0.995 acc=0.950\n",
            "R2Res_mid_N200each | ep 2/3 | loss=0.2059 | test_f1=0.950 auc=0.976 acc=0.950\n",
            "R2Res_mid_N200each | ep 3/3 | loss=0.1384 | test_f1=0.952 auc=0.983 acc=0.950\n",
            "\n",
            "--- R2Res_mid PCA+MLP [N200each] ---\n",
            "R2Res_mid_PCA_MLP_N200each | ep 1/3 | loss=0.5747 | test_f1=0.874 auc=0.954 acc=0.875\n",
            "R2Res_mid_PCA_MLP_N200each | ep 2/3 | loss=0.3698 | test_f1=0.915 auc=0.968 acc=0.917\n",
            "R2Res_mid_PCA_MLP_N200each | ep 3/3 | loss=0.2586 | test_f1=0.923 auc=0.973 acc=0.925\n",
            "[N200each] Saved PCA: /content/drive/MyDrive/ai_detection_eval/SCALETEST_R2RES_MID_PCA_GINC_FIXED/pca_N200each_dim128.joblib\n",
            "\n",
            "==== SUMMARY (sorted by tag then F1) ====\n",
            "        tag              model    acc        f1   roc_auc        ap  use_pca  \\\n",
            "0  N200each          R2Res_mid  0.950  0.951613  0.983333  0.982446    False   \n",
            "1  N200each  R2Res_mid_PCA_MLP  0.925  0.923077  0.973333  0.973958     True   \n",
            "\n",
            "   use_ginc  \n",
            "0      True  \n",
            "1      True  \n",
            "\n",
            "Saved summary to: /content/drive/MyDrive/ai_detection_eval/SCALETEST_R2RES_MID_PCA_GINC_FIXED/SUMMARY_SCALETEST_FIXED.csv\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/SCALETEST_R2RES_MID_PCA_GINC_FIXED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PAPER FINAL RUN (MID-ONLY) â€” R2Res (R2+1D + ResNet18) + MLP\n",
        "# Full dataset target: 1200 AI (AI_CORE+SORA) + 1200 REAL\n",
        "# FIXED: no oversample requirement on REAL (you only have 1200)\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install opencv-python-headless scikit-learn pandas tqdm matplotlib joblib\n",
        "\n",
        "import os, cv2, random, hashlib, subprocess, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import models as tvm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
        "    roc_curve, precision_recall_curve, confusion_matrix, brier_score_loss\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "print(\"Torch device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Input directories (your setup)\n",
        "AI_CORE_DIR = \"/content/drive/MyDrive/AudioModel/AI\"\n",
        "SORA_DIR    = \"/content/drive/MyDrive/soravideo/sora2aivideos\"\n",
        "REAL_DIR    = \"/content/drive/MyDrive/genvid_ucf_1200/real_1200\"\n",
        "\n",
        "# Output\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ai_detection_eval\"\n",
        "RUN_NAME = \"PAPER_FINAL_R2RES_MID_ONLY_FULLDATA\"\n",
        "RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "\n",
        "# Targets (paper run)\n",
        "N_EACH_TARGET = 1200  # AI=1200, REAL=1200\n",
        "\n",
        "# Oversample settings:\n",
        "# - REAL cannot be oversampled beyond what exists (you have 1200)\n",
        "# - AI can be oversampled because you have 2450 total\n",
        "AI_RAW_MULT = 1.10     # oversample AI only\n",
        "REAL_RAW_MULT = 1.00   # DO NOT oversample REAL\n",
        "\n",
        "# Metadata-neutral normalization\n",
        "NORM_FPS   = 25\n",
        "NORM_W     = 256\n",
        "NORM_H     = 256\n",
        "NORM_CODEC = \"libx264\"\n",
        "NORM_CRF   = 23\n",
        "NORM_PIX   = \"yuv420p\"\n",
        "NORM_AUDIO = False\n",
        "\n",
        "# Clip sampling\n",
        "T = 8\n",
        "FRAME_STRIDE = 3\n",
        "TARGET_SIZE  = 224\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE   = 6        # lower if OOM\n",
        "EPOCHS       = 12\n",
        "LR           = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_WORKERS  = 2\n",
        "\n",
        "TRAIN_CLIPS_PER_VIDEO = 3\n",
        "TEST_CLIPS_PER_VIDEO  = 1\n",
        "\n",
        "# Dedup controls\n",
        "NEAR_DUP_HAMMING_MAX = 3\n",
        "\n",
        "SAVE_EVERY_EPOCH = True\n",
        "BEST_ON = \"f1\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1) HELPERS: list videos\n",
        "# -----------------------------\n",
        "def list_videos(root):\n",
        "    vids = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\",\".webm\")):\n",
        "                vids.append(os.path.join(r, f))\n",
        "    return vids\n",
        "\n",
        "ai_core = list_videos(AI_CORE_DIR)\n",
        "sora    = list_videos(SORA_DIR)\n",
        "real    = list_videos(REAL_DIR)\n",
        "\n",
        "print(\"AI_CORE videos:\", len(ai_core))\n",
        "print(\"SORA videos   :\", len(sora))\n",
        "print(\"REAL videos   :\", len(real))\n",
        "\n",
        "AI_ALL = ai_core + sora\n",
        "\n",
        "# Determine how many we can actually target safely\n",
        "N_EACH = min(N_EACH_TARGET, len(real), len(AI_ALL))\n",
        "if N_EACH < N_EACH_TARGET:\n",
        "    print(f\"[WARN] Not enough videos to hit 1200/1200. Using N_EACH={N_EACH} instead.\")\n",
        "\n",
        "AI_RAW_EACH   = min(int(np.ceil(N_EACH * AI_RAW_MULT)), len(AI_ALL))\n",
        "REAL_RAW_EACH = min(int(np.ceil(N_EACH * REAL_RAW_MULT)), len(real))\n",
        "\n",
        "# -----------------------------\n",
        "# 2) SAMPLE RAW POOL\n",
        "# -----------------------------\n",
        "AI_raw   = random.sample(AI_ALL, AI_RAW_EACH)\n",
        "REAL_raw = random.sample(real,   REAL_RAW_EACH)\n",
        "\n",
        "raw_df = pd.DataFrame({\n",
        "    \"path\": AI_raw + REAL_raw,\n",
        "    \"y\":    [1]*len(AI_raw) + [0]*len(REAL_raw),\n",
        "    \"src\":  [\"AI\"]*len(AI_raw) + [\"REAL\"]*len(REAL_raw),\n",
        "}).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "raw_manifest = os.path.join(RUN_DIR, f\"raw_manifest_AI{AI_RAW_EACH}_REAL{REAL_RAW_EACH}.csv\")\n",
        "raw_df.to_csv(raw_manifest, index=False)\n",
        "print(\"\\nRAW pool:\", len(raw_df), \"total | AI=\", (raw_df.y==1).sum(), \"REAL=\", (raw_df.y==0).sum())\n",
        "print(\"Saved:\", raw_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) HASHING + DEDUP\n",
        "# -----------------------------\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path)\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def dhash_first_frame(path, hash_size=8):\n",
        "    fr = get_first_frame(path, size=(hash_size+1, hash_size))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "    diff = gray[:,1:] > gray[:,:-1]\n",
        "    h = 0\n",
        "    for b in diff.flatten():\n",
        "        h = (h << 1) | int(b)\n",
        "    return h\n",
        "\n",
        "def hamming(a, b):\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "def uf_make(n):\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb: return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "    return find, union\n",
        "\n",
        "def bucket_key(h, bits=12):\n",
        "    return int(h) >> (64 - bits)\n",
        "\n",
        "print(\"\\nComputing hashes for dedup...\")\n",
        "sha_list, dh_list = [], []\n",
        "for p in tqdm(raw_df.path.tolist(), desc=\"Hashing\"):\n",
        "    sha_list.append(sha1_first_frame(p))\n",
        "    dh_list.append(dhash_first_frame(p))\n",
        "\n",
        "df = raw_df.copy()\n",
        "df[\"sha1\"] = sha_list\n",
        "df[\"dhash\"] = dh_list\n",
        "df = df.dropna(subset=[\"sha1\",\"dhash\"]).reset_index(drop=True)\n",
        "\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"sha1\"]).reset_index(drop=True)\n",
        "print(\"Exact dupes removed:\", before - len(df))\n",
        "\n",
        "bucket = defaultdict(list)\n",
        "for i, h in enumerate(df.dhash.values):\n",
        "    bucket[bucket_key(h)].append(i)\n",
        "\n",
        "find, union = uf_make(len(df))\n",
        "for _, idxs in bucket.items():\n",
        "    if len(idxs) < 2:\n",
        "        continue\n",
        "    if len(idxs) > 350:\n",
        "        idxs = random.sample(idxs, 350)\n",
        "    for ii in range(len(idxs)):\n",
        "        a = idxs[ii]\n",
        "        ha = int(df.loc[a,\"dhash\"])\n",
        "        for jj in range(ii+1, len(idxs)):\n",
        "            b = idxs[jj]\n",
        "            hb = int(df.loc[b,\"dhash\"])\n",
        "            if hamming(ha, hb) <= NEAR_DUP_HAMMING_MAX:\n",
        "                union(a, b)\n",
        "\n",
        "df[\"cluster\"] = [find(i) for i in range(len(df))]\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"cluster\"]).reset_index(drop=True)\n",
        "print(\"Near-dupes removed:\", before - len(df))\n",
        "print(\"After dedup:\", len(df))\n",
        "\n",
        "# Rebalance to exact N_EACH, but if dedup removed too many REAL/AI,\n",
        "# fall back to the maximum possible common size.\n",
        "df_ai = df[df.y==1]\n",
        "df_rl = df[df.y==0]\n",
        "N_EACH_FINAL = min(N_EACH, len(df_ai), len(df_rl))\n",
        "if N_EACH_FINAL < N_EACH:\n",
        "    print(f\"[WARN] After dedup, cannot keep N_EACH={N_EACH}. Using N_EACH_FINAL={N_EACH_FINAL}.\")\n",
        "\n",
        "df_ai = df_ai.sample(n=N_EACH_FINAL, random_state=SEED)\n",
        "df_rl = df_rl.sample(n=N_EACH_FINAL, random_state=SEED)\n",
        "df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "dedup_manifest = os.path.join(RUN_DIR, \"dedup_manifest.csv\")\n",
        "df.to_csv(dedup_manifest, index=False)\n",
        "print(\"Rebalanced:\", len(df), \"total | AI=\", (df.y==1).sum(), \"REAL=\", (df.y==0).sum())\n",
        "print(\"Saved:\", dedup_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) NORMALIZE VIDEOS (metadata-neutral)\n",
        "# -----------------------------\n",
        "NORM_DIR = os.path.join(RUN_DIR, \"normalized\")\n",
        "os.makedirs(NORM_DIR, exist_ok=True)\n",
        "\n",
        "def norm_path_for(i):\n",
        "    return os.path.join(NORM_DIR, f\"v_{i:05d}.mp4\")\n",
        "\n",
        "def ffmpeg_normalize(in_path, out_path):\n",
        "    aflag = [\"-an\"] if not NORM_AUDIO else []\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "        \"-i\", in_path,\n",
        "        \"-vf\", f\"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,\"\n",
        "               f\"pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,\"\n",
        "               f\"fps={NORM_FPS}\",\n",
        "        \"-vsync\",\"cfr\",\n",
        "        *aflag,\n",
        "        \"-c:v\", NORM_CODEC,\n",
        "        \"-crf\", str(NORM_CRF),\n",
        "        \"-pix_fmt\", NORM_PIX,\n",
        "        out_path\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "\n",
        "print(\"\\nNormalizing videos...\")\n",
        "norm_paths = []\n",
        "for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=\"Normalize\")):\n",
        "    outp = norm_path_for(i)\n",
        "    if not os.path.exists(outp):\n",
        "        ffmpeg_normalize(row.path, outp)\n",
        "    norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)\n",
        "\n",
        "ndf = df.copy()\n",
        "ndf[\"norm_path\"] = norm_paths\n",
        "ndf = ndf.dropna(subset=[\"norm_path\"]).reset_index(drop=True)\n",
        "\n",
        "norm_manifest = os.path.join(RUN_DIR, \"normalized_manifest.csv\")\n",
        "ndf.to_csv(norm_manifest, index=False)\n",
        "print(\"Normalized:\", len(ndf), \"Saved:\", norm_manifest)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) CLUSTER-SAFE SPLIT (video-level)\n",
        "# -----------------------------\n",
        "clusters = ndf.cluster.values\n",
        "cluster_to_rows = defaultdict(list)\n",
        "for i,c in enumerate(clusters):\n",
        "    cluster_to_rows[int(c)].append(i)\n",
        "\n",
        "cluster_ids = list(cluster_to_rows.keys())\n",
        "cluster_labels = []\n",
        "for c in cluster_ids:\n",
        "    ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]\n",
        "    cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)\n",
        "\n",
        "c_train, c_test = train_test_split(\n",
        "    cluster_ids, test_size=0.30, random_state=SEED, stratify=cluster_labels\n",
        ")\n",
        "\n",
        "train_vid_idx, test_vid_idx = [], []\n",
        "for c in c_train: train_vid_idx.extend(cluster_to_rows[c])\n",
        "for c in c_test:  test_vid_idx.extend(cluster_to_rows[c])\n",
        "\n",
        "train_videos = ndf.iloc[train_vid_idx].reset_index(drop=True)\n",
        "test_videos  = ndf.iloc[test_vid_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCluster-safe split (video-level):\")\n",
        "print(\" Train videos:\", len(train_videos), \" Test videos:\", len(test_videos))\n",
        "print(\" Train AI/REAL:\", int((train_videos.y==1).sum()), \"/\", int((train_videos.y==0).sum()))\n",
        "print(\" Test  AI/REAL:\", int((test_videos.y==1).sum()), \"/\", int((test_videos.y==0).sum()))\n",
        "\n",
        "train_videos.to_csv(os.path.join(RUN_DIR, \"train_videos.csv\"), index=False)\n",
        "test_videos.to_csv(os.path.join(RUN_DIR, \"test_videos.csv\"), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) FIXED CLIP PLAN\n",
        "# -----------------------------\n",
        "def get_frame_count(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return 0\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    return n\n",
        "\n",
        "def make_clip_plan(videos_df, clips_per_video, T, stride, seed):\n",
        "    rng = random.Random(seed)\n",
        "    rows = []\n",
        "    for r in videos_df.itertuples(index=False):\n",
        "        path = r.norm_path\n",
        "        n = get_frame_count(path)\n",
        "        max_start = max(0, n - (T-1)*stride - 1)\n",
        "        for _ in range(clips_per_video):\n",
        "            start = rng.randint(0, max_start) if max_start > 0 else 0\n",
        "            rows.append({\n",
        "                \"norm_path\": path,\n",
        "                \"y\": int(r.y),\n",
        "                \"src\": r.src,\n",
        "                \"cluster\": int(r.cluster),\n",
        "                \"start\": int(start),\n",
        "                \"T\": int(T),\n",
        "                \"stride\": int(stride),\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "clip_train = make_clip_plan(train_videos, TRAIN_CLIPS_PER_VIDEO, T, FRAME_STRIDE, seed=SEED+11)\n",
        "clip_test  = make_clip_plan(test_videos,  TEST_CLIPS_PER_VIDEO,  T, FRAME_STRIDE, seed=SEED+99)\n",
        "\n",
        "clip_train_path = os.path.join(RUN_DIR, \"clip_plan_train.csv\")\n",
        "clip_test_path  = os.path.join(RUN_DIR, \"clip_plan_test.csv\")\n",
        "clip_train.to_csv(clip_train_path, index=False)\n",
        "clip_test.to_csv(clip_test_path, index=False)\n",
        "\n",
        "print(\"\\nSaved fixed clip plans:\")\n",
        "print(\" Train:\", clip_train_path, \"| clips:\", len(clip_train))\n",
        "print(\" Test :\", clip_test_path,  \"| clips:\", len(clip_test))\n",
        "\n",
        "# -----------------------------\n",
        "# 7) DATASET (mid stream only)\n",
        "# -----------------------------\n",
        "def resize_center_bgr(frame_bgr, size):\n",
        "    return cv2.resize(frame_bgr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def to_rgb(frame_bgr):\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def read_frames_fixed(path, start, T, stride, size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        idx = start + i*stride\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            cap.release()\n",
        "            return None\n",
        "        fr = resize_center_bgr(fr, size)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "class FixedClipDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, clip_df, size=224):\n",
        "        self.df = clip_df.reset_index(drop=True)\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        frames = read_frames_fixed(\n",
        "            r[\"norm_path\"],\n",
        "            int(r[\"start\"]),\n",
        "            T=int(r[\"T\"]),\n",
        "            stride=int(r[\"stride\"]),\n",
        "            size=self.size\n",
        "        )\n",
        "        y = int(r[\"y\"])\n",
        "        if frames is None:\n",
        "            x = np.zeros((int(r[\"T\"]), 3, self.size, self.size), dtype=np.float32)\n",
        "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        arr = np.stack([to_rgb(fr) for fr in frames], axis=0).astype(np.float32) / 255.0\n",
        "        arr = np.transpose(arr, (0,3,1,2))  # T,C,H,W\n",
        "        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "train_loader = make_loader(FixedClipDataset(clip_train, TARGET_SIZE), shuffle=True)\n",
        "test_loader  = make_loader(FixedClipDataset(clip_test,  TARGET_SIZE), shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) MODEL (R2+1D + ResNet18 fusion + MLP)\n",
        "# -----------------------------\n",
        "class R2ResNetFusion_MLP(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)\n",
        "        self.r2.fc = nn.Identity()\n",
        "\n",
        "        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "        self.res.fc = nn.Identity()\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        xr2 = x.permute(0,2,1,3,4)\n",
        "        f_r2 = self.r2(xr2)  # B,512\n",
        "\n",
        "        xf = x.reshape(B*T, C, H, W)\n",
        "        f = self.res(xf).view(B, T, -1).mean(dim=1)  # B,512\n",
        "\n",
        "        feat = torch.cat([f_r2, f], dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "# -----------------------------\n",
        "# 9) METRICS + PLOTS\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, proba):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, pred)),\n",
        "        \"f1\": float(f1_score(y_true, pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"ap\": float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_y.append(y.numpy()); all_p.append(p)\n",
        "    y_true = np.concatenate(all_y, axis=0)\n",
        "    proba  = np.concatenate(all_p, axis=0)\n",
        "    return y_true, proba, compute_metrics(y_true, proba)\n",
        "\n",
        "def save_pr_roc_plots(y_true, proba, out_dir, prefix=\"best_test\"):\n",
        "    fpr, tpr, _ = roc_curve(y_true, proba)\n",
        "    plt.figure(); plt.plot(fpr, tpr)\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC ({prefix})\")\n",
        "    roc_path = os.path.join(out_dir, f\"{prefix}_roc.png\")\n",
        "    plt.savefig(roc_path, dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    prec, rec, _ = precision_recall_curve(y_true, proba)\n",
        "    plt.figure(); plt.plot(rec, prec)\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR ({prefix})\")\n",
        "    pr_path = os.path.join(out_dir, f\"{prefix}_pr.png\")\n",
        "    plt.savefig(pr_path, dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "    return roc_path, pr_path\n",
        "\n",
        "def save_confusion(y_true, proba, out_dir, prefix=\"best_test\"):\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "    cm = confusion_matrix(y_true, pred)\n",
        "    plt.figure(); plt.imshow(cm, interpolation=\"nearest\"); plt.title(f\"CM ({prefix})\")\n",
        "    plt.colorbar()\n",
        "    plt.xticks([0,1], [\"REAL\",\"AI\"]); plt.yticks([0,1], [\"REAL\",\"AI\"])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\")\n",
        "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
        "    cm_path = os.path.join(out_dir, f\"{prefix}_confusion.png\")\n",
        "    plt.savefig(cm_path, dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "    return cm_path\n",
        "\n",
        "def save_calibration_plot(y_true, proba, out_dir, prefix=\"best_test\", bins=10):\n",
        "    proba = np.asarray(proba); y_true = np.asarray(y_true)\n",
        "    edges = np.linspace(0,1,bins+1)\n",
        "    bin_ids = np.clip(np.digitize(proba, edges)-1, 0, bins-1)\n",
        "    conf, acc = [], []\n",
        "    for b in range(bins):\n",
        "        m = (bin_ids==b)\n",
        "        if m.sum()==0:\n",
        "            conf.append(np.nan); acc.append(np.nan)\n",
        "        else:\n",
        "            conf.append(proba[m].mean()); acc.append(y_true[m].mean())\n",
        "    plt.figure()\n",
        "    plt.plot([0,1],[0,1])\n",
        "    plt.scatter(conf, acc)\n",
        "    plt.xlabel(\"Mean predicted prob\"); plt.ylabel(\"Empirical positives\")\n",
        "    plt.title(f\"Calibration ({prefix})\")\n",
        "    cal_path = os.path.join(out_dir, f\"{prefix}_calibration.png\")\n",
        "    plt.savefig(cal_path, dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "    return cal_path, float(brier_score_loss(y_true, proba))\n",
        "\n",
        "# -----------------------------\n",
        "# 10) TRAIN LOOP\n",
        "# -----------------------------\n",
        "def train_main(model, train_loader, test_loader, epochs, name):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    ckpt_dir = os.path.join(RUN_DIR, \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    plot_dir = os.path.join(RUN_DIR, \"plots\")\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    history = []\n",
        "    best_score = -1e9\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE); y = y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        y_true, proba, m = eval_model(model, test_loader)\n",
        "        row = {\"epoch\": ep, \"train_loss\": float(np.mean(losses)), **m}\n",
        "        history.append(row)\n",
        "\n",
        "        print(f\"{name} | ep {ep}/{epochs} | loss={row['train_loss']:.4f} | \"\n",
        "              f\"test_f1={row['f1']:.3f} auc={row['roc_auc']:.3f} acc={row['acc']:.3f} ap={row['ap']:.3f}\")\n",
        "\n",
        "        if SAVE_EVERY_EPOCH:\n",
        "            torch.save({\n",
        "                \"epoch\": ep,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"opt_state\": opt.state_dict(),\n",
        "                \"metrics\": row,\n",
        "            }, os.path.join(ckpt_dir, f\"{name}_epoch{ep:03d}.pt\"))\n",
        "\n",
        "        score = row[BEST_ON]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_epoch = ep\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    y_true, proba, final_m = eval_model(model, test_loader)\n",
        "\n",
        "    best_path = os.path.join(RUN_DIR, f\"{name}_BEST.pt\")\n",
        "    torch.save({\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"final_metrics\": final_m,\n",
        "        \"history\": history,\n",
        "        \"manifests\": {\n",
        "            \"normalized_manifest\": norm_manifest,\n",
        "            \"clip_train\": clip_train_path,\n",
        "            \"clip_test\": clip_test_path\n",
        "        }\n",
        "    }, best_path)\n",
        "\n",
        "    hist_df = pd.DataFrame(history)\n",
        "    hist_csv = os.path.join(RUN_DIR, f\"{name}_history.csv\")\n",
        "    hist_df.to_csv(hist_csv, index=False)\n",
        "\n",
        "    # plots\n",
        "    plt.figure(); plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Train loss\"); plt.title(\"Train loss\")\n",
        "    plt.savefig(os.path.join(plot_dir, \"train_loss.png\"), dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    for k in [\"acc\",\"f1\",\"roc_auc\",\"ap\"]:\n",
        "        plt.figure(); plt.plot(hist_df[\"epoch\"], hist_df[k])\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(k); plt.title(f\"Test {k}\")\n",
        "        plt.savefig(os.path.join(plot_dir, f\"test_{k}.png\"), dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    roc_path, pr_path = save_pr_roc_plots(y_true, proba, plot_dir, \"best_test\")\n",
        "    cm_path = save_confusion(y_true, proba, plot_dir, \"best_test\")\n",
        "    cal_path, brier = save_calibration_plot(y_true, proba, plot_dir, \"best_test\")\n",
        "\n",
        "    final_metrics_path = os.path.join(RUN_DIR, f\"{name}_final_metrics.json\")\n",
        "    with open(final_metrics_path, \"w\") as f:\n",
        "        json.dump({**final_m, \"best_epoch\": best_epoch, \"brier\": brier}, f, indent=2)\n",
        "\n",
        "    print(\"\\nSaved BEST:\", best_path)\n",
        "    print(\"Saved history:\", hist_csv)\n",
        "    print(\"Saved final metrics:\", final_metrics_path)\n",
        "    print(\"Saved plots in:\", plot_dir)\n",
        "    print(\" -\", roc_path)\n",
        "    print(\" -\", pr_path)\n",
        "    print(\" -\", cm_path)\n",
        "    print(\" -\", cal_path)\n",
        "\n",
        "    return model, final_m, best_epoch\n",
        "\n",
        "# -----------------------------\n",
        "# 11) RUN PAPER TRAINING\n",
        "# -----------------------------\n",
        "model = R2ResNetFusion_MLP()\n",
        "trained_model, final_metrics, best_epoch = train_main(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    epochs=EPOCHS,\n",
        "    name=\"R2Res_mid_MLP_FULLDATA\"\n",
        ")\n",
        "\n",
        "print(\"\\n==== FINAL (BEST) TEST METRICS ====\")\n",
        "print(\"Best epoch:\", best_epoch)\n",
        "print(final_metrics)\n",
        "print(\"\\nRun folder:\", RUN_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZ455j83gc7",
        "outputId": "01dbe29b-4190-4327-f885-7dd106551915"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch device: cuda\n",
            "GPU: Tesla T4\n",
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA\n",
            "AI_CORE videos: 1200\n",
            "SORA videos   : 1250\n",
            "REAL videos   : 1200\n",
            "\n",
            "RAW pool: 2520 total | AI= 1320 REAL= 1200\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/raw_manifest_AI1320_REAL1200.csv\n",
            "\n",
            "Computing hashes for dedup...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2520/2520 [30:08<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact dupes removed: 0\n",
            "Near-dupes removed: 34\n",
            "After dedup: 2485\n",
            "[WARN] After dedup, cannot keep N_EACH=1200. Using N_EACH_FINAL=1175.\n",
            "Rebalanced: 2350 total | AI= 1175 REAL= 1175\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/dedup_manifest.csv\n",
            "\n",
            "Normalizing videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2350/2350 [36:38<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: 2350 Saved: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/normalized_manifest.csv\n",
            "\n",
            "Cluster-safe split (video-level):\n",
            " Train videos: 1645  Test videos: 705\n",
            " Train AI/REAL: 823 / 822\n",
            " Test  AI/REAL: 352 / 353\n",
            "\n",
            "Saved fixed clip plans:\n",
            " Train: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/clip_plan_train.csv | clips: 4935\n",
            " Test : /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/clip_plan_test.csv | clips: 705\n",
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:00<00:00, 239MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 231MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2Res_mid_MLP_FULLDATA | ep 1/12 | loss=0.1764 | test_f1=0.950 auc=0.983 acc=0.950 ap=0.987\n",
            "R2Res_mid_MLP_FULLDATA | ep 2/12 | loss=0.1167 | test_f1=0.936 auc=0.984 acc=0.935 ap=0.988\n",
            "R2Res_mid_MLP_FULLDATA | ep 3/12 | loss=0.0945 | test_f1=0.956 auc=0.988 acc=0.956 ap=0.991\n",
            "R2Res_mid_MLP_FULLDATA | ep 4/12 | loss=0.0675 | test_f1=0.940 auc=0.984 acc=0.939 ap=0.984\n",
            "R2Res_mid_MLP_FULLDATA | ep 5/12 | loss=0.0675 | test_f1=0.960 auc=0.989 acc=0.960 ap=0.991\n",
            "R2Res_mid_MLP_FULLDATA | ep 6/12 | loss=0.0469 | test_f1=0.955 auc=0.984 acc=0.956 ap=0.989\n",
            "R2Res_mid_MLP_FULLDATA | ep 7/12 | loss=0.0292 | test_f1=0.958 auc=0.989 acc=0.957 ap=0.991\n",
            "R2Res_mid_MLP_FULLDATA | ep 8/12 | loss=0.0351 | test_f1=0.952 auc=0.987 acc=0.952 ap=0.981\n",
            "R2Res_mid_MLP_FULLDATA | ep 9/12 | loss=0.0283 | test_f1=0.958 auc=0.988 acc=0.957 ap=0.989\n",
            "R2Res_mid_MLP_FULLDATA | ep 10/12 | loss=0.0237 | test_f1=0.963 auc=0.991 acc=0.963 ap=0.993\n",
            "R2Res_mid_MLP_FULLDATA | ep 11/12 | loss=0.0273 | test_f1=0.960 auc=0.990 acc=0.960 ap=0.991\n",
            "R2Res_mid_MLP_FULLDATA | ep 12/12 | loss=0.0234 | test_f1=0.804 auc=0.958 acc=0.818 ap=0.961\n",
            "\n",
            "Saved BEST: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/R2Res_mid_MLP_FULLDATA_BEST.pt\n",
            "Saved history: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/R2Res_mid_MLP_FULLDATA_history.csv\n",
            "Saved final metrics: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/R2Res_mid_MLP_FULLDATA_final_metrics.json\n",
            "Saved plots in: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/plots\n",
            " - /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/plots/best_test_roc.png\n",
            " - /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/plots/best_test_pr.png\n",
            " - /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/plots/best_test_confusion.png\n",
            " - /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA/plots/best_test_calibration.png\n",
            "\n",
            "==== FINAL (BEST) TEST METRICS ====\n",
            "Best epoch: 10\n",
            "{'acc': 0.9631205673758865, 'f1': 0.9629629629629629, 'roc_auc': 0.9910587818696883, 'ap': 0.992605673142243}\n",
            "\n",
            "Run folder: /content/drive/MyDrive/ai_detection_eval/PAPER_FINAL_R2RES_MID_ONLY_FULLDATA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "telLSz07u2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbRrIy0h3gZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "81fU44BnTkIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FAST SANITY BENCHMARK (Bias-reduced manifest + 10 AI / 10 REAL)\n",
        "# FULL WORKING SCRIPT (beginning -> end)\n",
        "#\n",
        "# What this does (NO TIME WASTE):\n",
        "#  1) Loads bias-reduced manifest: dedup_manifest.csv (columns include: path, y, source, ...)\n",
        "#     - Automatically renames y -> label if needed\n",
        "#  2) Samples EXACTLY 10 AI + 10 REAL (seeded)\n",
        "#  3) Builds the EXACT modalities you described:\n",
        "#       - midtone+grad  : gradient magnitude computed ONLY on midtone-masked luma\n",
        "#       - high+grad     : HIGH-PASS (FFT) of that gradient magnitude map (spatial HF map)\n",
        "#       - tgrad         : temporal diff of midtone-grad magnitude (midtone masked)\n",
        "#     NO \"freq+high\" fusion (not allowed)\n",
        "#     NO multi-stream modality combos (single tensor input = channel concatenation)\n",
        "#\n",
        "#  4) Benchmarks:\n",
        "#       A) PCA baseline (using midtone-grad / high+grad / tgrad stats + downsample maps)\n",
        "#       B) GINC baseline (GLCM texture features on midtone-grad + high+grad + tgrad + stats)\n",
        "#       C) Deep models (quick epochs because this is method selection):\n",
        "#           - ResNet (2D on mid-frame of fused tensor)\n",
        "#           - ViT (2D on mid-frame)\n",
        "#           - R2+1D (3D)\n",
        "#           - TimeSformerLite (single model)\n",
        "#           - r2res ensemble (logit avg: r2+1d + resnet)\n",
        "#           - vitres ensemble (logit avg: vit + resnet)\n",
        "#\n",
        "# Output:\n",
        "#   - SUMMARY.csv + SUMMARY.json in run_dir\n",
        "#\n",
        "# NOTE:\n",
        "#   This is a \"sanity pick the main method\" run.\n",
        "#   With only 20 videos, metrics will be noisyâ€”use it to pick top candidates fast.\n",
        "# ============================================================\n",
        "\n",
        "import os, json, time, random, math\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional, List, Dict\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
        "\n",
        "# For GINC-like texture features\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # bias-reduced manifest\n",
        "    manifest_csv: str = \"/content/drive/MyDrive/ai_detection_eval/BENCHMARK_TEST_STAGE_20251229_201313/dedup_manifest.csv\"\n",
        "\n",
        "    # output\n",
        "    run_dir: str = \"/content/drive/MyDrive/ai_detection_eval/BENCHMARK_SANITY_10EACH_DEDUP\"\n",
        "\n",
        "    # sample size\n",
        "    n_ai: int = 10\n",
        "    n_real: int = 10\n",
        "\n",
        "    # reproducibility\n",
        "    seed: int = 1337\n",
        "\n",
        "    # device\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # video settings\n",
        "    num_frames: int = 16\n",
        "    resize_hw: Tuple[int, int] = (224, 224)\n",
        "\n",
        "    # midtone mask in [0..1] luma\n",
        "    mid_lo: float = 0.25\n",
        "    mid_hi: float = 0.75\n",
        "\n",
        "    # modalities (single tensor input by channel concat)\n",
        "    use_rgb: bool = True\n",
        "    use_midgrad_mag: bool = True     # midtone gradient magnitude map (1 ch)\n",
        "    use_grad_hf_map: bool = True     # high-pass of gradient magnitude (1 ch)\n",
        "    use_tgrad_map: bool = True       # temporal diff of midtone-grad mag (1 ch)\n",
        "\n",
        "    # train/eval split (small set => keep test not too tiny)\n",
        "    test_size: float = 0.30\n",
        "\n",
        "    # dataloader\n",
        "    batch_size: int = 4\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # training (keep smallâ€”this is just selection)\n",
        "    epochs: int = 2\n",
        "    lr: float = 2e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    amp: bool = True\n",
        "\n",
        "    # run suites\n",
        "    run_pca: bool = True\n",
        "    run_ginc: bool = True\n",
        "\n",
        "    run_resnet: bool = True\n",
        "    run_vit: bool = True\n",
        "    run_r2plus1d: bool = True\n",
        "    run_timesformer: bool = True\n",
        "\n",
        "    run_r2res: bool = True\n",
        "    run_vitres: bool = True\n",
        "\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.run_dir, exist_ok=True)\n",
        "\n",
        "random.seed(cfg.seed)\n",
        "np.random.seed(cfg.seed)\n",
        "torch.manual_seed(cfg.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Manifest loading (handles y vs label)\n",
        "# -----------------------------\n",
        "def load_manifest_any_schema(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # path column\n",
        "    if \"path\" not in df.columns:\n",
        "        for c in [\"video_path\", \"filepath\", \"file\", \"fp\"]:\n",
        "            if c in df.columns:\n",
        "                df = df.rename(columns={c: \"path\"})\n",
        "                break\n",
        "    if \"path\" not in df.columns:\n",
        "        raise ValueError(f\"No path column found. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # label column (dedup_manifest uses 'y')\n",
        "    if \"label\" not in df.columns:\n",
        "        if \"y\" in df.columns:\n",
        "            df = df.rename(columns={\"y\": \"label\"})\n",
        "        else:\n",
        "            raise ValueError(f\"No label column found (expected label or y). Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # normalize label type\n",
        "    def to01(x):\n",
        "        if isinstance(x, str):\n",
        "            s = x.strip().lower()\n",
        "            if s in [\"ai\", \"fake\", \"gen\", \"generated\", \"1\", \"true\", \"yes\"]:\n",
        "                return 1\n",
        "            if s in [\"real\", \"0\", \"false\", \"no\"]:\n",
        "                return 0\n",
        "        return int(x)\n",
        "\n",
        "    df = df[[\"path\", \"label\"]].copy()\n",
        "    df[\"label\"] = df[\"label\"].apply(to01).astype(int)\n",
        "\n",
        "    df = df[df[\"path\"].astype(str).str.len() > 0].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def sample_10_each(df: pd.DataFrame, n_ai: int, n_real: int, seed: int) -> pd.DataFrame:\n",
        "    ai = df[df[\"label\"] == 1]\n",
        "    re = df[df[\"label\"] == 0]\n",
        "\n",
        "    if len(ai) < n_ai or len(re) < n_real:\n",
        "        raise ValueError(f\"Not enough samples. Have AI={len(ai)} REAL={len(re)}; need {n_ai}/{n_real}.\")\n",
        "\n",
        "    ai_s = ai.sample(n=n_ai, random_state=seed)\n",
        "    re_s = re.sample(n=n_real, random_state=seed)\n",
        "    out = pd.concat([ai_s, re_s], axis=0).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Video read + sampling\n",
        "# -----------------------------\n",
        "def safe_cap(path: str):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "    return cap\n",
        "\n",
        "def get_frame_count(cap) -> int:\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return n if n > 0 else 0\n",
        "\n",
        "def sample_indices(n: int, k: int) -> List[int]:\n",
        "    if n <= 0:\n",
        "        return []\n",
        "    if n <= k:\n",
        "        idx = list(range(n))\n",
        "        while len(idx) < k:\n",
        "            idx.append(n - 1)\n",
        "        return idx\n",
        "    return [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "\n",
        "def read_clip(path: str, k: int, hw: Tuple[int,int]) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns RGB float32 [0,1], shape (T,H,W,3)\n",
        "    \"\"\"\n",
        "    cap = safe_cap(path)\n",
        "    if cap is None:\n",
        "        return None\n",
        "\n",
        "    n = get_frame_count(cap)\n",
        "    idxs = sample_indices(n, k)\n",
        "    if not idxs:\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    frames = []\n",
        "    for t in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            continue\n",
        "        fr = cv2.resize(fr, (hw[1], hw[0]), interpolation=cv2.INTER_AREA)\n",
        "        fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "        fr = fr.astype(np.float32) / 255.0\n",
        "        frames.append(fr)\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "    while len(frames) < k:\n",
        "        frames.append(frames[-1].copy())\n",
        "\n",
        "    return np.stack(frames[:k], axis=0)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Correct modalities (your definitions)\n",
        "# -----------------------------\n",
        "def luma01_from_rgb(fr: np.ndarray) -> np.ndarray:\n",
        "    r, g, b = fr[:,:,0], fr[:,:,1], fr[:,:,2]\n",
        "    return (0.299*r + 0.587*g + 0.114*b).astype(np.float32)\n",
        "\n",
        "def midtone_mask(luma01: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
        "    return ((luma01 >= lo) & (luma01 <= hi)).astype(np.float32)\n",
        "\n",
        "def sobel_mag(gray01: np.ndarray) -> np.ndarray:\n",
        "    g8 = (gray01 * 255.0).clip(0,255).astype(np.uint8)\n",
        "    gx = cv2.Sobel(g8, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(g8, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    mag = cv2.magnitude(gx, gy)\n",
        "    mag = mag / (np.max(mag) + 1e-8)\n",
        "    return mag.astype(np.float32)\n",
        "\n",
        "def fft_highpass_map(img01: np.ndarray, frac_hi: float = 0.45) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    High-pass filtered spatial map from input (here: gradient magnitude).\n",
        "    \"\"\"\n",
        "    f = np.fft.fft2(img01.astype(np.float32))\n",
        "    f = np.fft.fftshift(f)\n",
        "\n",
        "    H, W = img01.shape\n",
        "    cy, cx = H//2, W//2\n",
        "    yy, xx = np.ogrid[:H,:W]\n",
        "    r = np.sqrt((yy-cy)**2 + (xx-cx)**2)\n",
        "    rmax = np.max(r) + 1e-8\n",
        "    hp = (r >= frac_hi * rmax).astype(np.float32)\n",
        "\n",
        "    f_hp = f * hp\n",
        "    f_hp = np.fft.ifftshift(f_hp)\n",
        "    out = np.fft.ifft2(f_hp)\n",
        "    out = np.real(out).astype(np.float32)\n",
        "\n",
        "    out = out - np.min(out)\n",
        "    out = out / (np.max(out) + 1e-8)\n",
        "    return out\n",
        "\n",
        "def in_channels(cfg: CFG) -> int:\n",
        "    c = 0\n",
        "    if cfg.use_rgb: c += 3\n",
        "    if cfg.use_midgrad_mag: c += 1\n",
        "    if cfg.use_grad_hf_map: c += 1\n",
        "    if cfg.use_tgrad_map: c += 1\n",
        "    return c\n",
        "\n",
        "def build_modal_tensor_and_maps(clip_rgb: np.ndarray, cfg: CFG):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      fused: (C,T,H,W) float32\n",
        "      mags:  (T,H,W)   midtone-grad magnitude maps\n",
        "      hf:    (T,H,W)   high-pass maps of mags\n",
        "      tgr:   (T,H,W)   temporal diff maps of mags\n",
        "    \"\"\"\n",
        "    T,H,W,_ = clip_rgb.shape\n",
        "\n",
        "    # precompute midtone-grad mags and masks\n",
        "    mags = []\n",
        "    masks = []\n",
        "    for t in range(T):\n",
        "        l = luma01_from_rgb(clip_rgb[t])\n",
        "        m = midtone_mask(l, cfg.mid_lo, cfg.mid_hi)\n",
        "        mid = l * m\n",
        "        mag = sobel_mag(mid)         # midtone-only gradient magnitude\n",
        "        mags.append(mag)\n",
        "        masks.append(m)\n",
        "\n",
        "    mags = np.stack(mags, axis=0).astype(np.float32)      # (T,H,W)\n",
        "    masks = np.stack(masks, axis=0).astype(np.float32)    # (T,H,W)\n",
        "\n",
        "    # high-pass of gradient magnitude (per frame)\n",
        "    hf = np.stack([fft_highpass_map(mags[t], frac_hi=0.45) for t in range(T)], axis=0).astype(np.float32)\n",
        "\n",
        "    # temporal grad (diff of mags), midtone-masked\n",
        "    tgr = np.zeros_like(mags, dtype=np.float32)\n",
        "    for t in range(1, T):\n",
        "        diff = np.abs(mags[t] - mags[t-1])\n",
        "        diff = diff * (masks[t] > 0).astype(np.float32)\n",
        "        tgr[t] = diff\n",
        "    tgr = tgr / (np.max(tgr) + 1e-8)\n",
        "\n",
        "    chans = []\n",
        "    if cfg.use_rgb:\n",
        "        rgb = np.transpose(clip_rgb, (3,0,1,2)).astype(np.float32)  # (3,T,H,W)\n",
        "        chans.append(rgb)\n",
        "    if cfg.use_midgrad_mag:\n",
        "        chans.append(mags[None, ...])   # (1,T,H,W)\n",
        "    if cfg.use_grad_hf_map:\n",
        "        chans.append(hf[None, ...])     # (1,T,H,W)\n",
        "    if cfg.use_tgrad_map:\n",
        "        chans.append(tgr[None, ...])    # (1,T,H,W)\n",
        "\n",
        "    fused = np.concatenate(chans, axis=0).astype(np.float32)        # (C,T,H,W)\n",
        "    return fused, mags, hf, tgr\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# FAST DATASET WITH CACHING (only 20 videos)\n",
        "# -----------------------------\n",
        "class CachedVideoDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, cfg: CFG):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.cfg = cfg\n",
        "        self.C = in_channels(cfg)\n",
        "\n",
        "        # cache everything ONCE to avoid slow repeated decoding\n",
        "        self.cache = []\n",
        "        self.baseline_feats = []  # for PCA/GINC baselines\n",
        "        self.ok_flags = []\n",
        "\n",
        "        for i in range(len(self.df)):\n",
        "            path = self.df.loc[i, \"path\"]\n",
        "            y = int(self.df.loc[i, \"label\"])\n",
        "\n",
        "            clip = read_clip(path, cfg.num_frames, cfg.resize_hw)\n",
        "            if clip is None:\n",
        "                fused = np.zeros((self.C, cfg.num_frames, cfg.resize_hw[0], cfg.resize_hw[1]), dtype=np.float32)\n",
        "                mags = np.zeros((cfg.num_frames, cfg.resize_hw[0], cfg.resize_hw[1]), dtype=np.float32)\n",
        "                hf   = np.zeros_like(mags)\n",
        "                tgr  = np.zeros_like(mags)\n",
        "                ok = 0\n",
        "            else:\n",
        "                fused, mags, hf, tgr = build_modal_tensor_and_maps(clip, cfg)\n",
        "                ok = 1\n",
        "\n",
        "            self.cache.append((fused, y, path, ok))\n",
        "\n",
        "            # baseline features from mags/hf/tgr (midtone-grad-centric)\n",
        "            feat = extract_baseline_features(mags, hf, tgr)\n",
        "            self.baseline_feats.append(feat)\n",
        "            self.ok_flags.append(ok)\n",
        "\n",
        "        self.baseline_feats = np.stack(self.baseline_feats, axis=0).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fused, y, path, ok = self.cache[idx]\n",
        "        return torch.from_numpy(fused), torch.tensor(y, dtype=torch.long), path, torch.tensor(ok, dtype=torch.long)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Baseline features (PCA + GINC)\n",
        "# -----------------------------\n",
        "def downsample_map(x: np.ndarray, out_hw=(32,32)) -> np.ndarray:\n",
        "    return cv2.resize(x, (out_hw[1], out_hw[0]), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
        "\n",
        "def safe_stats(x: np.ndarray) -> List[float]:\n",
        "    x = x.astype(np.float32)\n",
        "    return [\n",
        "        float(np.mean(x)),\n",
        "        float(np.std(x)),\n",
        "        float(np.percentile(x, 25)),\n",
        "        float(np.percentile(x, 50)),\n",
        "        float(np.percentile(x, 75)),\n",
        "        float(np.max(x)),\n",
        "    ]\n",
        "\n",
        "def glcm_props(img01: np.ndarray, levels: int = 16) -> List[float]:\n",
        "    \"\"\"\n",
        "    GLCM texture features on a single map (midtone-grad / HF / TGR),\n",
        "    quantized to 'levels'.\n",
        "    \"\"\"\n",
        "    x = (img01 * (levels - 1)).clip(0, levels - 1).astype(np.uint8)\n",
        "    # distances + angles\n",
        "    glcm = graycomatrix(x, distances=[1, 2], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                        levels=levels, symmetric=True, normed=True)\n",
        "    feats = []\n",
        "    for prop in [\"contrast\", \"homogeneity\", \"energy\", \"correlation\"]:\n",
        "        v = graycoprops(glcm, prop)  # shape (len(dist), len(angle))\n",
        "        feats.append(float(np.mean(v)))\n",
        "        feats.append(float(np.std(v)))\n",
        "    return feats\n",
        "\n",
        "def extract_baseline_features(mags: np.ndarray, hf: np.ndarray, tgr: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Features intended to reflect exactly your modalities:\n",
        "      - midtone-grad magnitude stats\n",
        "      - high+grad (HF of mags) stats\n",
        "      - tgrad stats\n",
        "      - plus downsampled mid-frame maps for PCA to chew on\n",
        "    \"\"\"\n",
        "    T = mags.shape[0]\n",
        "    mid = T // 2\n",
        "\n",
        "    feats = []\n",
        "    # stats across time (global)\n",
        "    feats += safe_stats(mags)\n",
        "    feats += safe_stats(hf)\n",
        "    feats += safe_stats(tgr)\n",
        "\n",
        "    # frame-level stats (mid frame)\n",
        "    feats += safe_stats(mags[mid])\n",
        "    feats += safe_stats(hf[mid])\n",
        "    feats += safe_stats(tgr[mid])\n",
        "\n",
        "    # add small downsampled maps (flatten) for PCA expressivity\n",
        "    ds_mag = downsample_map(mags[mid], (32,32)).flatten()\n",
        "    ds_hf  = downsample_map(hf[mid], (32,32)).flatten()\n",
        "    ds_tg  = downsample_map(tgr[mid], (32,32)).flatten()\n",
        "    feats += ds_mag.tolist()\n",
        "    feats += ds_hf.tolist()\n",
        "    feats += ds_tg.tolist()\n",
        "\n",
        "    # GINC-like texture features (GLCM) for each map type\n",
        "    feats += glcm_props(mags[mid])\n",
        "    feats += glcm_props(hf[mid])\n",
        "    feats += glcm_props(tgr[mid])\n",
        "\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "\n",
        "def eval_probs(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    out = {\n",
        "        \"acc\": float(accuracy_score(y_true, preds)),\n",
        "        \"f1\": float(f1_score(y_true, preds)),\n",
        "        \"precision\": float(precision_score(y_true, preds, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, preds, zero_division=0)),\n",
        "    }\n",
        "    # AUC can fail if test has one class (small sets). Guard it.\n",
        "    try:\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            out[\"roc_auc\"] = float(roc_auc_score(y_true, probs))\n",
        "        else:\n",
        "            out[\"roc_auc\"] = float(\"nan\")\n",
        "    except Exception:\n",
        "        out[\"roc_auc\"] = float(\"nan\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def run_pca_baseline(Xtr: np.ndarray, ytr: np.ndarray, Xte: np.ndarray, yte: np.ndarray) -> Dict[str, float]:\n",
        "    scaler = StandardScaler()\n",
        "    Xtr_s = scaler.fit_transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "\n",
        "    pca = PCA(n_components=min(64, Xtr_s.shape[1]), random_state=cfg.seed)\n",
        "    Xtr_p = pca.fit_transform(Xtr_s)\n",
        "    Xte_p = pca.transform(Xte_s)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=cfg.seed)\n",
        "    clf.fit(Xtr_p, ytr)\n",
        "\n",
        "    probs = clf.predict_proba(Xte_p)[:,1]\n",
        "    return eval_probs(yte, probs)\n",
        "\n",
        "\n",
        "def run_ginc_baseline(Xtr: np.ndarray, ytr: np.ndarray, Xte: np.ndarray, yte: np.ndarray) -> Dict[str, float]:\n",
        "    # \"GINC\" here = texture+gradient feature LR (your sanity-check baseline)\n",
        "    scaler = StandardScaler()\n",
        "    Xtr_s = scaler.fit_transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=cfg.seed)\n",
        "    clf.fit(Xtr_s, ytr)\n",
        "\n",
        "    probs = clf.predict_proba(Xte_s)[:,1]\n",
        "    return eval_probs(yte, probs)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model helpers: adapt first layer for arbitrary channels\n",
        "# -----------------------------\n",
        "def adapt_conv2d_in(conv: nn.Conv2d, new_in: int) -> nn.Conv2d:\n",
        "    new = nn.Conv2d(new_in, conv.out_channels, kernel_size=conv.kernel_size,\n",
        "                    stride=conv.stride, padding=conv.padding, bias=(conv.bias is not None))\n",
        "    with torch.no_grad():\n",
        "        if conv.weight.shape[1] == 3 and new_in >= 3:\n",
        "            new.weight[:, :3] = conv.weight\n",
        "            if new_in > 3:\n",
        "                avg = conv.weight.mean(dim=1, keepdim=True)\n",
        "                for c in range(3, new_in):\n",
        "                    new.weight[:, c:c+1] = avg\n",
        "        else:\n",
        "            nn.init.kaiming_normal_(new.weight, nonlinearity=\"relu\")\n",
        "        if conv.bias is not None:\n",
        "            new.bias.copy_(conv.bias)\n",
        "    return new\n",
        "\n",
        "def adapt_conv3d_in(conv: nn.Conv3d, new_in: int) -> nn.Conv3d:\n",
        "    new = nn.Conv3d(new_in, conv.out_channels, kernel_size=conv.kernel_size,\n",
        "                    stride=conv.stride, padding=conv.padding, bias=(conv.bias is not None))\n",
        "    with torch.no_grad():\n",
        "        if conv.weight.shape[1] == 3 and new_in >= 3:\n",
        "            new.weight[:, :3] = conv.weight\n",
        "            if new_in > 3:\n",
        "                avg = conv.weight.mean(dim=1, keepdim=True)\n",
        "                for c in range(3, new_in):\n",
        "                    new.weight[:, c:c+1] = avg\n",
        "        else:\n",
        "            nn.init.kaiming_normal_(new.weight, nonlinearity=\"relu\")\n",
        "        if conv.bias is not None:\n",
        "            new.bias.copy_(conv.bias)\n",
        "    return new\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Backbones\n",
        "# -----------------------------\n",
        "def build_resnet2d(num_classes: int, in_ch: int):\n",
        "    import torchvision\n",
        "    m = torchvision.models.resnet18(weights=None)\n",
        "    m.conv1 = adapt_conv2d_in(m.conv1, in_ch)\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "def build_vit2d(num_classes: int, in_ch: int):\n",
        "    import torchvision\n",
        "    m = torchvision.models.vit_b_16(weights=None)\n",
        "    m.conv_proj = adapt_conv2d_in(m.conv_proj, in_ch)\n",
        "    m.heads.head = nn.Linear(m.heads.head.in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "def build_r2plus1d(num_classes: int, in_ch: int):\n",
        "    import torchvision\n",
        "    m = torchvision.models.video.r2plus1d_18(weights=None)\n",
        "    m.stem[0] = adapt_conv3d_in(m.stem[0], in_ch)\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# TimeSformerLite (single model)\n",
        "# -----------------------------\n",
        "class TimeSformerLite(nn.Module):\n",
        "    def __init__(self, in_ch: int, num_classes: int, img_size=224, patch=16, T=16,\n",
        "                 dim=384, depth=4, heads=6, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert img_size % patch == 0\n",
        "        self.T = T\n",
        "        self.patch = patch\n",
        "        self.grid = img_size // patch\n",
        "        self.Np = self.grid * self.grid\n",
        "        self.dim = dim\n",
        "\n",
        "        self.patch_embed = nn.Conv2d(in_ch, dim, kernel_size=patch, stride=patch, bias=True)\n",
        "        self.cls = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        self.pos_spatial = nn.Parameter(torch.zeros(1, self.Np, dim))\n",
        "        self.pos_temporal = nn.Parameter(torch.zeros(1, T, dim))\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=dim, nhead=heads, dim_feedforward=int(dim*mlp_ratio),\n",
        "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=depth)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
        "        nn.init.trunc_normal_(self.pos_spatial, std=0.02)\n",
        "        nn.init.trunc_normal_(self.pos_temporal, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,C,T,H,W)\n",
        "        B,C,T,H,W = x.shape\n",
        "        assert T == self.T, f\"Expected T={self.T}, got {T}\"\n",
        "\n",
        "        tokens = []\n",
        "        for t in range(T):\n",
        "            ft = x[:,:,t]               # (B,C,H,W)\n",
        "            pt = self.patch_embed(ft)   # (B,dim,grid,grid)\n",
        "            pt = pt.flatten(2).transpose(1,2)   # (B,Np,dim)\n",
        "            pt = pt + self.pos_spatial\n",
        "            pt = pt + self.pos_temporal[:, t:t+1, :]\n",
        "            tokens.append(pt)\n",
        "\n",
        "        tok = torch.cat(tokens, dim=1)          # (B,T*Np,dim)\n",
        "        cls = self.cls.expand(B, -1, -1)\n",
        "        tok = torch.cat([cls, tok], dim=1)      # (B,1+T*Np,dim)\n",
        "\n",
        "        tok = self.enc(tok)\n",
        "        tok = self.norm(tok)\n",
        "        return self.head(tok[:,0])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2D wrapper: run on mid-frame of the fused tensor\n",
        "# -----------------------------\n",
        "class FrameModelWrapper(nn.Module):\n",
        "    def __init__(self, base2d: nn.Module, use_mid_frame: bool = True):\n",
        "        super().__init__()\n",
        "        self.base2d = base2d\n",
        "        self.use_mid = use_mid_frame\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,C,T,H,W)\n",
        "        T = x.shape[2]\n",
        "        t = T//2 if self.use_mid else 0\n",
        "        fr = x[:,:,t]  # (B,C,H,W)\n",
        "        return self.base2d(fr)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Logit ensemble (model-level fusion, NOT modality streams)\n",
        "# -----------------------------\n",
        "class LogitEnsemble(nn.Module):\n",
        "    def __init__(self, models: List[nn.Module]):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = None\n",
        "        for m in self.models:\n",
        "            out = m(x)\n",
        "            logits = out if logits is None else (logits + out)\n",
        "        return logits / len(self.models)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval helpers\n",
        "# -----------------------------\n",
        "def split_indices(y: np.ndarray, test_size: float, seed: int):\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    idx = np.arange(len(y))\n",
        "    tr, te = next(splitter.split(idx, y))\n",
        "    return tr, te\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_torch(model, loader, device):\n",
        "    model.eval()\n",
        "    probs, ys = [], []\n",
        "    for x,y,_,_ in loader:\n",
        "        x = x.to(device, non_blocking=True).float()\n",
        "        y = y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1]\n",
        "        probs.append(p.detach().cpu().numpy())\n",
        "        ys.append(y.detach().cpu().numpy())\n",
        "\n",
        "    probs = np.concatenate(probs)\n",
        "    ys = np.concatenate(ys)\n",
        "    return eval_probs(ys, probs)\n",
        "\n",
        "def train_model(name: str, model: nn.Module, train_loader, test_loader, cfg: CFG):\n",
        "    device = cfg.device\n",
        "    model = model.to(device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=(cfg.amp and device.startswith(\"cuda\")))\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    best = None\n",
        "    for ep in range(1, cfg.epochs+1):\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x,y,_,_ in train_loader:\n",
        "            x = x.to(device, non_blocking=True).float()\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda', enabled=(cfg.amp and device.startswith(\"cuda\"))):\n",
        "                logits = model(x)\n",
        "                loss = loss_fn(logits, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            total_loss += float(loss.detach().cpu().item())\n",
        "\n",
        "        metrics = eval_model_torch(model, test_loader, device)\n",
        "        dt = time.time() - t0\n",
        "        print(f\"{name} | ep {ep}/{cfg.epochs} | loss={total_loss/max(1,len(train_loader)):.4f} \"\n",
        "              f\"| f1={metrics['f1']:.3f} auc={metrics['roc_auc']:.3f} acc={metrics['acc']:.3f} | {dt:.1f}s\")\n",
        "\n",
        "        if best is None or metrics[\"f1\"] > best[\"f1\"]:\n",
        "            best = metrics.copy()\n",
        "            best[\"epoch\"] = ep\n",
        "\n",
        "    return best\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN\n",
        "# -----------------------------\n",
        "def main(cfg: CFG):\n",
        "    # 1) Load bias-reduced manifest\n",
        "    df = load_manifest_any_schema(cfg.manifest_csv)\n",
        "\n",
        "    # 2) Sample 10 AI + 10 REAL\n",
        "    df_small = sample_10_each(df, cfg.n_ai, cfg.n_real, cfg.seed)\n",
        "\n",
        "    print(\"Using manifest:\", cfg.manifest_csv)\n",
        "    print(\"Sampled set size:\", len(df_small))\n",
        "    print(\"AI =\", int(df_small[\"label\"].sum()), \"REAL =\", int((df_small[\"label\"]==0).sum()))\n",
        "\n",
        "    # 3) Build cached dataset (fast after first decode)\n",
        "    ds = CachedVideoDataset(df_small, cfg)\n",
        "    y_all = df_small[\"label\"].values.astype(int)\n",
        "\n",
        "    tr_idx, te_idx = split_indices(y_all, cfg.test_size, cfg.seed)\n",
        "    df_tr = df_small.iloc[tr_idx].reset_index(drop=True)\n",
        "    df_te = df_small.iloc[te_idx].reset_index(drop=True)\n",
        "\n",
        "    # build train/test datasets from cached dataset by indexing\n",
        "    class SubsetWrap(Dataset):\n",
        "        def __init__(self, base: CachedVideoDataset, idxs: np.ndarray):\n",
        "            self.base = base\n",
        "            self.idxs = idxs.tolist()\n",
        "        def __len__(self): return len(self.idxs)\n",
        "        def __getitem__(self, i): return self.base[self.idxs[i]]\n",
        "\n",
        "    train_ds = SubsetWrap(ds, tr_idx)\n",
        "    test_ds  = SubsetWrap(ds, te_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              num_workers=cfg.num_workers, pin_memory=True)\n",
        "\n",
        "    # baseline features for PCA/GINC\n",
        "    X = ds.baseline_feats\n",
        "    y = y_all\n",
        "    Xtr, ytr = X[tr_idx], y[tr_idx]\n",
        "    Xte, yte = X[te_idx], y[te_idx]\n",
        "\n",
        "    results = []\n",
        "    num_classes = 2\n",
        "    C_in = in_channels(cfg)\n",
        "\n",
        "    # 4A) PCA baseline\n",
        "    if cfg.run_pca:\n",
        "        m = run_pca_baseline(Xtr, ytr, Xte, yte)\n",
        "        results.append({\"model\":\"PCA_MIDGRAD_HF_TGR\", **m})\n",
        "        print(\"PCA baseline:\", m)\n",
        "\n",
        "    # 4B) GINC baseline (GLCM + stats LR)\n",
        "    if cfg.run_ginc:\n",
        "        m = run_ginc_baseline(Xtr, ytr, Xte, yte)\n",
        "        results.append({\"model\":\"GINC_MIDGRAD_HF_TGR_LR\", **m})\n",
        "        print(\"GINC baseline:\", m)\n",
        "\n",
        "    # 4C) Deep models\n",
        "    if cfg.run_resnet:\n",
        "        res = build_resnet2d(num_classes, C_in)\n",
        "        model = FrameModelWrapper(res, use_mid_frame=True)\n",
        "        best = train_model(\"resnet\", model, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"resnet\", **best})\n",
        "\n",
        "    if cfg.run_vit:\n",
        "        vit = build_vit2d(num_classes, C_in)\n",
        "        model = FrameModelWrapper(vit, use_mid_frame=True)\n",
        "        best = train_model(\"vit\", model, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"vit\", **best})\n",
        "\n",
        "    if cfg.run_r2plus1d:\n",
        "        r2 = build_r2plus1d(num_classes, C_in)\n",
        "        best = train_model(\"r2plus1d\", r2, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"r2plus1d\", **best})\n",
        "\n",
        "    if cfg.run_timesformer:\n",
        "        ts = TimeSformerLite(in_ch=C_in, num_classes=num_classes,\n",
        "                             img_size=cfg.resize_hw[0], patch=16, T=cfg.num_frames,\n",
        "                             dim=384, depth=4, heads=6, mlp_ratio=4.0, dropout=0.1)\n",
        "        best = train_model(\"timesformer\", ts, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"timesformer\", **best})\n",
        "\n",
        "    # 4D) Ensembles\n",
        "    if cfg.run_r2res:\n",
        "        r2 = build_r2plus1d(num_classes, C_in)\n",
        "        res = FrameModelWrapper(build_resnet2d(num_classes, C_in), use_mid_frame=True)\n",
        "        ens = LogitEnsemble([r2, res])\n",
        "        best = train_model(\"r2res_ensemble\", ens, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"r2res_ensemble\", **best})\n",
        "\n",
        "    if cfg.run_vitres:\n",
        "        vit = FrameModelWrapper(build_vit2d(num_classes, C_in), use_mid_frame=True)\n",
        "        res = FrameModelWrapper(build_resnet2d(num_classes, C_in), use_mid_frame=True)\n",
        "        ens = LogitEnsemble([vit, res])\n",
        "        best = train_model(\"vitres_ensemble\", ens, train_loader, test_loader, cfg)\n",
        "        results.append({\"model\":\"vitres_ensemble\", **best})\n",
        "\n",
        "    # 5) Save summary\n",
        "    out_csv = os.path.join(cfg.run_dir, \"SUMMARY.csv\")\n",
        "    out_json = os.path.join(cfg.run_dir, \"SUMMARY.json\")\n",
        "\n",
        "    res_df = pd.DataFrame(results)\n",
        "    # some methods may not have \"epoch\"\n",
        "    if \"epoch\" not in res_df.columns:\n",
        "        res_df[\"epoch\"] = np.nan\n",
        "\n",
        "    # sort by f1 descending\n",
        "    res_df = res_df.sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
        "    res_df.to_csv(out_csv, index=False)\n",
        "\n",
        "    with open(out_json, \"w\") as f:\n",
        "        json.dump({\"cfg\": cfg.__dict__, \"results\": results, \"used_manifest\": cfg.manifest_csv}, f, indent=2)\n",
        "\n",
        "    print(\"\\n==== FINAL SUMMARY (sorted by F1) ====\")\n",
        "    print(res_df)\n",
        "\n",
        "    print(\"\\nSaved:\", out_csv)\n",
        "    print(\"Run folder:\", cfg.run_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "LXOEJVkSUQMd",
        "outputId": "cb4c4811-d4e3-47a4-ae9f-af9511aba61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using manifest: /content/drive/MyDrive/ai_detection_eval/BENCHMARK_TEST_STAGE_20251229_201313/dedup_manifest.csv\n",
            "Sampled set size: 20\n",
            "AI = 10 REAL = 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_components=64 must be between 0 and min(n_samples, n_features)=14 with svd_solver='full'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3109245940.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3109245940.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;31m# 4A) PCA baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pca\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pca_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"PCA_MIDGRAD_HF_TGR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PCA baseline:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3109245940.py\u001b[0m in \u001b[0;36mrun_pca_baseline\u001b[0;34m(Xtr, ytr, Xte, yte)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0mXtr_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0mXte_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \"\"\"\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_is_centered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mU\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"covariance_eigh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_array_api_compliant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[1;32m    554\u001b[0m                 )\n\u001b[1;32m    555\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0;34mf\"n_components={n_components} must be between 0 and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;34mf\"min(n_samples, n_features)={min(n_samples, n_features)} with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_components=64 must be between 0 and min(n_samples, n_features)=14 with svd_solver='full'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLlg1JsUWmeY"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8p4kFiCWnoi",
        "outputId": "03d35824-991c-4364-ce81-cae33d5eb172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 10 .mp4 in /content/drive/MyDrive/soravideo\n",
            "âœ… Copied 10 AI videos into /content/drive/MyDrive/ai_video_project_test/ai_videos_10\n",
            "Found AI videos:   10 in /content/drive/MyDrive/ai_video_project_test/ai_videos_10\n",
            "Found Real videos: 10 in /content/drive/MyDrive/ai_video_project_test/real_videos_10\n",
            "Using 10 AI and 10 Real videos (total 20).\n",
            "Using device: cpu\n",
            "Train videos: 14, Test videos: 6\n",
            "\n",
            "=== Training main R(2+1)D (toy) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - loss: 0.6588 - acc: 0.643\n",
            "Epoch 2/5 - loss: 0.7027 - acc: 0.643\n",
            "Epoch 3/5 - loss: 0.4538 - acc: 0.857\n",
            "Epoch 4/5 - loss: 0.4500 - acc: 0.786\n",
            "Epoch 5/5 - loss: 0.1879 - acc: 0.929\n",
            "\n",
            "=== TEST RESULTS ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real      1.000     0.667     0.800         3\n",
            "          AI      0.750     1.000     0.857         3\n",
            "\n",
            "    accuracy                          0.833         6\n",
            "   macro avg      0.875     0.833     0.829         6\n",
            "weighted avg      0.875     0.833     0.829         6\n",
            "\n",
            "Accuracy: 0.8333333333333334\n",
            "Test labels: Counter({np.int64(1): 3, np.int64(0): 3})\n",
            "Test preds : Counter({np.int64(1): 4, np.int64(0): 2})\n",
            "\n",
            "=== CHECK 1: Tiny subset overfit (2 real + 2 AI) ===\n",
            "ep01: loss=0.7514, acc=0.500\n",
            "ep02: loss=0.7348, acc=0.500\n",
            "ep03: loss=0.6444, acc=0.750\n",
            "ep04: loss=0.1084, acc=1.000\n",
            "ep05: loss=0.0377, acc=1.000\n",
            "ep06: loss=0.0223, acc=1.000\n",
            "ep07: loss=0.0486, acc=1.000\n",
            "ep08: loss=0.0095, acc=1.000\n",
            "ep09: loss=0.6594, acc=0.500\n",
            "ep10: loss=0.0136, acc=1.000\n",
            "ep11: loss=0.0133, acc=1.000\n",
            "ep12: loss=0.0064, acc=1.000\n",
            "If this does NOT climb near 1.0 acc â†’ something off in pipeline.\n",
            "\n",
            "=== CHECK 2: Linear probe on frozen features ===\n",
            "Linear probe TRAIN accuracy: 1.000\n",
            "If reasonably high (>0.8), features capture Real vs AI signal.\n",
            "\n",
            "=== CHECK 3: Bias summary ===\n",
            "Test labels: Counter({np.int64(1): 3, np.int64(0): 3})\n",
            "Test preds : Counter({np.int64(1): 4, np.int64(0): 2})\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless torch torchvision scikit-learn tqdm --quiet\n",
        "\n",
        "import os, random, shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r2plus1d_18\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# ========= 0. MOUNT DRIVE =========\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Base project paths\n",
        "BASE_DIR      = Path(\"/content/drive/MyDrive/ai_video_project_test\")\n",
        "AI_DIR        = BASE_DIR / \"ai_videos_10\"\n",
        "REAL_DIR      = BASE_DIR / \"real_videos_10\"\n",
        "SORA_RAW_DIR  = Path(\"/content/drive/MyDrive/soravideo\")  # where your 10 Sora AI videos are\n",
        "\n",
        "AI_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ========= 1. SYNC AI VIDEOS FROM SORAVIDEO IF NEEDED =========\n",
        "ai_existing = list(AI_DIR.glob(\"*.mp4\"))\n",
        "if len(ai_existing) == 0:\n",
        "    sora_videos = sorted([p for p in SORA_RAW_DIR.glob(\"*.mp4\")])\n",
        "    print(f\"Found {len(sora_videos)} .mp4 in {SORA_RAW_DIR}\")\n",
        "    if len(sora_videos) == 0:\n",
        "        raise SystemExit(\n",
        "            f\"âŒ No AI videos found. Put your Sora/AI clips in: {SORA_RAW_DIR}\\n\"\n",
        "            \"or directly in: {AI_DIR}\"\n",
        "        )\n",
        "    # copy up to 10 into AI_DIR\n",
        "    for v in sora_videos[:10]:\n",
        "        shutil.copy(v, AI_DIR / v.name)\n",
        "    print(f\"âœ… Copied {min(10,len(sora_videos))} AI videos into {AI_DIR}\")\n",
        "\n",
        "# ========= 2. COLLECT PATHS =========\n",
        "ai_paths   = sorted([str(p) for p in AI_DIR.glob(\"*.mp4\")])\n",
        "real_paths = sorted([str(p) for p in REAL_DIR.glob(\"*\") if p.suffix.lower() in [\".mp4\", \".avi\", \".mov\", \".mkv\"]])\n",
        "\n",
        "print(f\"Found AI videos:   {len(ai_paths)} in {AI_DIR}\")\n",
        "print(f\"Found Real videos: {len(real_paths)} in {REAL_DIR}\")\n",
        "\n",
        "if len(ai_paths) == 0 or len(real_paths) == 0:\n",
        "    raise SystemExit(\n",
        "        \"âŒ Need at least 1 AI and 1 Real video.\\n\"\n",
        "        f\"AI found: {len(ai_paths)}, Real found: {len(real_paths)}.\\n\"\n",
        "        f\"Check:\\n  AI -> {AI_DIR}\\n  Real -> {REAL_DIR}\"\n",
        "    )\n",
        "\n",
        "# Keep balanced: same count each side\n",
        "N = min(len(ai_paths), len(real_paths))\n",
        "ai_paths   = ai_paths[:N]\n",
        "real_paths = real_paths[:N]\n",
        "\n",
        "video_paths = ai_paths + real_paths\n",
        "labels      = [1]*len(ai_paths) + [0]*len(real_paths)   # 1=AI, 0=Real\n",
        "\n",
        "print(f\"Using {len(ai_paths)} AI and {len(real_paths)} Real videos (total {len(video_paths)}).\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ========= 3. DATASET & LOADER =========\n",
        "\n",
        "NUM_FRAMES = 16\n",
        "\n",
        "frame_transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((112, 112)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
        "                std=[0.22803, 0.22145, 0.216989]),\n",
        "])\n",
        "\n",
        "def load_clip(path, num_frames=NUM_FRAMES):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        return torch.randn(3, num_frames, 112, 112)\n",
        "\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return torch.randn(3, num_frames, 112, 112)\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        reps = (num_frames + len(frames) - 1) // len(frames)\n",
        "        frames = (frames * reps)[:num_frames]\n",
        "\n",
        "    idxs = np.linspace(0, len(frames)-1, num_frames).astype(int)\n",
        "    clip_frames = []\n",
        "    for i in idxs:\n",
        "        f = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)\n",
        "        clip_frames.append(frame_transform(f))\n",
        "    clip = torch.stack(clip_frames, dim=1)  # [3, T, H, W]\n",
        "    return clip\n",
        "\n",
        "class VideoBinaryDataset(Dataset):\n",
        "    def __init__(self, paths, labels):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        y = self.labels[idx]\n",
        "        clip = load_clip(p, NUM_FRAMES)\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# ========= 4. TRAIN / TEST SPLIT =========\n",
        "\n",
        "indices = list(range(len(video_paths)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "\n",
        "ai_idx   = [i for i in indices if labels[i] == 1]\n",
        "real_idx = [i for i in indices if labels[i] == 0]\n",
        "\n",
        "def split_class(idxs, train_ratio=0.7):\n",
        "    k = max(1, int(len(idxs)*train_ratio))\n",
        "    return idxs[:k], idxs[k:]\n",
        "\n",
        "ai_train, ai_test     = split_class(ai_idx)\n",
        "real_train, real_test = split_class(real_idx)\n",
        "\n",
        "train_idx = ai_train + real_train\n",
        "test_idx  = ai_test + real_test\n",
        "\n",
        "print(f\"Train videos: {len(train_idx)}, Test videos: {len(test_idx)}\")\n",
        "\n",
        "train_paths  = [video_paths[i] for i in train_idx]\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "test_paths   = [video_paths[i] for i in test_idx]\n",
        "test_labels  = [labels[i] for i in test_idx]\n",
        "\n",
        "train_dataset = VideoBinaryDataset(train_paths, train_labels)\n",
        "test_dataset  = VideoBinaryDataset(test_paths, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "# ========= 5. R(2+1)D MODEL =========\n",
        "\n",
        "def build_r2plus1d_binary():\n",
        "    m = r2plus1d_18(weights=\"KINETICS400_V1\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "    return m\n",
        "\n",
        "model_r2 = build_r2plus1d_binary().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_r2.parameters(), lr=1e-4)\n",
        "\n",
        "# ========= 6. TRAIN MAIN MODEL =========\n",
        "EPOCHS = 5\n",
        "print(\"\\n=== Training main R(2+1)D (toy) ===\")\n",
        "for ep in range(EPOCHS):\n",
        "    model_r2.train()\n",
        "    run_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for clips, labs in train_loader:\n",
        "        clips = clips.to(device)\n",
        "        labs = labs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model_r2(clips)\n",
        "        loss = criterion(out, labs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        run_loss += loss.item()*labs.size(0)\n",
        "        preds = out.argmax(1)\n",
        "        correct += (preds == labs).sum().item()\n",
        "        total += labs.size(0)\n",
        "\n",
        "    print(f\"Epoch {ep+1}/{EPOCHS} - loss: {run_loss/max(total,1):.4f} - acc: {correct/max(total,1):.3f}\")\n",
        "\n",
        "# ========= 7. TEST EVAL =========\n",
        "model_r2.eval()\n",
        "all_preds, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for clips, labs in test_loader:\n",
        "        clips = clips.to(device)\n",
        "        out = model_r2(clips)\n",
        "        preds = out.argmax(1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_true.extend(labs.numpy())\n",
        "\n",
        "if len(all_true) > 0:\n",
        "    print(\"\\n=== TEST RESULTS ===\")\n",
        "    print(classification_report(all_true, all_preds,\n",
        "                                target_names=[\"Real\",\"AI\"],\n",
        "                                digits=3))\n",
        "    print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n",
        "    print(\"Test labels:\", Counter(all_true))\n",
        "    print(\"Test preds :\", Counter(all_preds))\n",
        "else:\n",
        "    print(\"\\n(No test videos; extremely tiny split.)\")\n",
        "\n",
        "# ========= 8. CHECK 1: OVERFIT 2R+2AI =========\n",
        "print(\"\\n=== CHECK 1: Tiny subset overfit (2 real + 2 AI) ===\")\n",
        "tiny_idx = []\n",
        "r_c = a_c = 0\n",
        "for i,(p,y) in enumerate(zip(train_paths, train_labels)):\n",
        "    if y==0 and r_c<2:\n",
        "        tiny_idx.append(i); r_c+=1\n",
        "    elif y==1 and a_c<2:\n",
        "        tiny_idx.append(i); a_c+=1\n",
        "    if r_c==2 and a_c==2:\n",
        "        break\n",
        "\n",
        "if len(tiny_idx)<4:\n",
        "    print(\"âš ï¸ Not enough mixed samples for 2R+2AI tiny subset.\")\n",
        "else:\n",
        "    tiny_paths  = [train_paths[i] for i in tiny_idx]\n",
        "    tiny_labels = [train_labels[i] for i in tiny_idx]\n",
        "    tiny_ds = VideoBinaryDataset(tiny_paths, tiny_labels)\n",
        "    tiny_dl = DataLoader(tiny_ds, batch_size=2, shuffle=True)\n",
        "\n",
        "    small = build_r2plus1d_binary().to(device)\n",
        "    opt_s = optim.Adam(small.parameters(), lr=1e-4)\n",
        "    crit_s = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(12):\n",
        "        small.train()\n",
        "        loss_ep = 0; corr = 0; tot = 0\n",
        "        for clips, labs in tiny_dl:\n",
        "            clips, labs = clips.to(device), labs.to(device)\n",
        "            opt_s.zero_grad()\n",
        "            out = small(clips)\n",
        "            loss = crit_s(out, labs)\n",
        "            loss.backward()\n",
        "            opt_s.step()\n",
        "            loss_ep += loss.item()*labs.size(0)\n",
        "            preds = out.argmax(1)\n",
        "            corr += (preds==labs).sum().item()\n",
        "            tot += labs.size(0)\n",
        "        print(f\"ep{ep+1:02d}: loss={loss_ep/tot:.4f}, acc={corr/tot:.3f}\")\n",
        "    print(\"If this does NOT climb near 1.0 acc â†’ something off in pipeline.\")\n",
        "\n",
        "# ========= 9. CHECK 2: LINEAR PROBE ON FROZEN FEATURES =========\n",
        "print(\"\\n=== CHECK 2: Linear probe on frozen features ===\")\n",
        "feat_model = build_r2plus1d_binary().to(device)\n",
        "feat_model.fc = nn.Identity()\n",
        "feat_model.eval()\n",
        "\n",
        "def extract_feat(p):\n",
        "    clip = load_clip(p, NUM_FRAMES).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        return feat_model(clip).cpu().numpy().flatten()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for p,l in zip(video_paths, labels):\n",
        "    X.append(extract_feat(p))\n",
        "    y.append(l)\n",
        "X = np.vstack(X)\n",
        "y = np.array(y)\n",
        "\n",
        "if len(np.unique(y)) < 2:\n",
        "    print(\"âš ï¸ Only one class; cannot run linear probe.\")\n",
        "else:\n",
        "    lin = LogisticRegression(max_iter=1000)\n",
        "    lin.fit(X, y)\n",
        "    y_hat = lin.predict(X)\n",
        "    print(f\"Linear probe TRAIN accuracy: {(y_hat==y).mean():.3f}\")\n",
        "    print(\"If reasonably high (>0.8), features capture Real vs AI signal.\")\n",
        "\n",
        "# ========= 10. CHECK 3: PREDICTION BIAS SUMMARY =========\n",
        "print(\"\\n=== CHECK 3: Bias summary ===\")\n",
        "if len(all_true) > 0:\n",
        "    print(\"Test labels:\", Counter(all_true))\n",
        "    print(\"Test preds :\", Counter(all_preds))\n",
        "else:\n",
        "    print(\"No held-out test set; will stabilize once you scale data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas scikit-learn opencv-python-headless tqdm\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "import cv2\n",
        "import hashlib\n",
        "\n",
        "# =========================\n",
        "# 0) CONFIG\n",
        "# =========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "RUN_DIR = \"/content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456\"\n",
        "near_csv = os.path.join(RUN_DIR, \"near_dupes_norm_multiframe.csv\")\n",
        "\n",
        "train_manifest = os.path.join(RUN_DIR, \"train_manifest_normalized.csv\")\n",
        "test_manifest  = os.path.join(RUN_DIR, \"test_manifest_normalized.csv\")\n",
        "\n",
        "assert os.path.exists(near_csv), f\"Missing: {near_csv}\"\n",
        "assert os.path.exists(train_manifest), f\"Missing: {train_manifest}\"\n",
        "assert os.path.exists(test_manifest),  f\"Missing: {test_manifest}\"\n",
        "\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "print(\"near_csv:\", near_csv)\n",
        "\n",
        "# =========================\n",
        "# 1) LOAD manifests\n",
        "# Expected columns: path,y,src (if your names differ, edit below)\n",
        "# =========================\n",
        "train_df = pd.read_csv(train_manifest)\n",
        "test_df  = pd.read_csv(test_manifest)\n",
        "\n",
        "def normalize_cols(df):\n",
        "    # try to be flexible with column names\n",
        "    colmap = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if lc in [\"path\",\"filepath\",\"file\",\"video_path\"]:\n",
        "            colmap[c] = \"path\"\n",
        "        elif lc in [\"y\",\"label\",\"target\"]:\n",
        "            colmap[c] = \"y\"\n",
        "        elif lc in [\"src\",\"source\",\"domain\"]:\n",
        "            colmap[c] = \"src\"\n",
        "    df = df.rename(columns=colmap)\n",
        "    assert \"path\" in df.columns and \"y\" in df.columns, f\"Need path,y columns. Found: {df.columns.tolist()}\"\n",
        "    if \"src\" not in df.columns:\n",
        "        df[\"src\"] = \"unknown\"\n",
        "    df[\"path\"] = df[\"path\"].astype(str)\n",
        "    df[\"y\"] = df[\"y\"].astype(int)\n",
        "    df[\"src\"] = df[\"src\"].astype(str)\n",
        "    return df\n",
        "\n",
        "train_df = normalize_cols(train_df)\n",
        "test_df  = normalize_cols(test_df)\n",
        "\n",
        "all_df = pd.concat([train_df.assign(split=\"train\"), test_df.assign(split=\"test\")], ignore_index=True)\n",
        "all_paths = set(all_df[\"path\"].tolist())\n",
        "\n",
        "print(\"Loaded normalized manifests:\")\n",
        "print(\" train:\", len(train_df), \" test:\", len(test_df), \" total:\", len(all_df))\n",
        "\n",
        "# =========================\n",
        "# 2) UNION-FIND clustering from near-dup edges\n",
        "# =========================\n",
        "near = pd.read_csv(near_csv)\n",
        "# expected columns: train_path,test_path (or similar)\n",
        "# make robust:\n",
        "def pick_col(df, candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "c_train = pick_col(near, [\"train_path\",\"a_path\",\"path_a\",\"p_train\",\"path1\"])\n",
        "c_test  = pick_col(near, [\"test_path\",\"b_path\",\"path_b\",\"p_test\",\"path2\"])\n",
        "c_dist  = pick_col(near, [\"min_hamming\",\"hamming\",\"dist\",\"distance\"])\n",
        "\n",
        "assert c_train and c_test, f\"near-dupe CSV missing expected columns. Found: {near.columns.tolist()}\"\n",
        "\n",
        "edges = []\n",
        "for _, r in near.iterrows():\n",
        "    a = str(r[c_train]); b = str(r[c_test])\n",
        "    if (a in all_paths) and (b in all_paths):\n",
        "        edges.append((a,b))\n",
        "\n",
        "print(\"Near-dup edges inside current dataset:\", len(edges))\n",
        "\n",
        "# Union-Find\n",
        "parent = {}\n",
        "rank = {}\n",
        "\n",
        "def find(x):\n",
        "    if parent[x] != x:\n",
        "        parent[x] = find(parent[x])\n",
        "    return parent[x]\n",
        "\n",
        "def union(a,b):\n",
        "    ra, rb = find(a), find(b)\n",
        "    if ra == rb:\n",
        "        return\n",
        "    if rank[ra] < rank[rb]:\n",
        "        parent[ra] = rb\n",
        "    elif rank[ra] > rank[rb]:\n",
        "        parent[rb] = ra\n",
        "    else:\n",
        "        parent[rb] = ra\n",
        "        rank[ra] += 1\n",
        "\n",
        "# init\n",
        "for p in all_df[\"path\"].tolist():\n",
        "    parent[p] = p\n",
        "    rank[p] = 0\n",
        "\n",
        "for a,b in edges:\n",
        "    union(a,b)\n",
        "\n",
        "# assign cluster ids\n",
        "root_to_id = {}\n",
        "cluster_id = []\n",
        "for p in all_df[\"path\"].tolist():\n",
        "    r = find(p)\n",
        "    if r not in root_to_id:\n",
        "        root_to_id[r] = len(root_to_id)\n",
        "    cluster_id.append(root_to_id[r])\n",
        "\n",
        "all_df[\"cluster\"] = cluster_id\n",
        "\n",
        "num_clusters = all_df[\"cluster\"].nunique()\n",
        "cluster_sizes = all_df[\"cluster\"].value_counts()\n",
        "print(\"Clusters:\", num_clusters)\n",
        "print(\"Clusters with size>1:\", int((cluster_sizes>1).sum()))\n",
        "\n",
        "# =========================\n",
        "# 3) GROUP-SAFE RESPLIT (no cluster crosses splits)\n",
        "# =========================\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
        "idx = np.arange(len(all_df))\n",
        "train_idx, test_idx = next(gss.split(idx, all_df[\"y\"].values, groups=all_df[\"cluster\"].values))\n",
        "\n",
        "new_train = all_df.iloc[train_idx].copy().reset_index(drop=True)\n",
        "new_test  = all_df.iloc[test_idx].copy().reset_index(drop=True)\n",
        "\n",
        "# sanity: ensure no overlap clusters\n",
        "overlap = set(new_train[\"cluster\"]).intersection(set(new_test[\"cluster\"]))\n",
        "print(\"\\n[GROUP SPLIT] cluster overlap:\", len(overlap))\n",
        "\n",
        "# class balance\n",
        "print(\"New split sizes:\", len(new_train), len(new_test))\n",
        "print(\"Train AI/REAL:\", int((new_train.y==1).sum()), \"/\", int((new_train.y==0).sum()))\n",
        "print(\"Test  AI/REAL:\", int((new_test.y==1).sum()),  \"/\", int((new_test.y==0).sum()))\n",
        "\n",
        "# save new manifests\n",
        "new_train_path = os.path.join(RUN_DIR, \"train_manifest_cluster_safe.csv\")\n",
        "new_test_path  = os.path.join(RUN_DIR, \"test_manifest_cluster_safe.csv\")\n",
        "new_train[[\"path\",\"y\",\"src\",\"cluster\"]].to_csv(new_train_path, index=False)\n",
        "new_test[[\"path\",\"y\",\"src\",\"cluster\"]].to_csv(new_test_path, index=False)\n",
        "print(\"Saved:\", new_train_path)\n",
        "print(\"Saved:\", new_test_path)\n",
        "\n",
        "# =========================\n",
        "# 4) Re-run leakage checks on NEW split (fast)\n",
        "# =========================\n",
        "def get_first_frame(path, size=(128,128)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return None\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if (not ok) or frame is None:\n",
        "        return None\n",
        "    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sha1_first_frame(path):\n",
        "    fr = get_first_frame(path, (128,128))\n",
        "    if fr is None:\n",
        "        return None\n",
        "    return hashlib.sha1(fr.tobytes()).hexdigest()\n",
        "\n",
        "def sha_set(df, name):\n",
        "    s = set()\n",
        "    bad = 0\n",
        "    for p in tqdm(df[\"path\"].tolist(), desc=f\"SHA1 {name}\"):\n",
        "        h = sha1_first_frame(p)\n",
        "        if h is None:\n",
        "            bad += 1\n",
        "            continue\n",
        "        s.add(h)\n",
        "    return s, bad\n",
        "\n",
        "train_sha, bad_tr = sha_set(new_train, \"train_cluster_safe\")\n",
        "test_sha,  bad_te = sha_set(new_test,  \"test_cluster_safe\")\n",
        "exact_overlap = len(train_sha.intersection(test_sha))\n",
        "print(\"\\n[POST-FIX CHECK] Exact SHA1 overlap:\", exact_overlap, \"| bad frames:\", bad_tr, bad_te)\n",
        "\n",
        "# =========================\n",
        "# 5) Metadata-only baseline again (should drop further)\n",
        "# =========================\n",
        "def video_metadata(path):\n",
        "    try:\n",
        "        size = os.path.getsize(path)\n",
        "    except:\n",
        "        size = np.nan\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        cap.release()\n",
        "        return [size, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
        "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    cap.release()\n",
        "    dur = frames/fps if (fps and fps>0) else np.nan\n",
        "    return [size,w,h,fps,frames,dur]\n",
        "\n",
        "def build_meta(df, name):\n",
        "    X, y = [], []\n",
        "    for p, lab in tqdm(df[[\"path\",\"y\"]].values, desc=f\"Metadata {name}\"):\n",
        "        X.append(video_metadata(p))\n",
        "        y.append(int(lab))\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    # fill NaNs with train medians later\n",
        "    return X, y\n",
        "\n",
        "Xtr, ytr = build_meta(new_train, \"train\")\n",
        "Xte, yte = build_meta(new_test,  \"test\")\n",
        "\n",
        "# fill NaNs using TRAIN medians\n",
        "for j in range(Xtr.shape[1]):\n",
        "    med = np.nanmedian(Xtr[:,j])\n",
        "    Xtr[:,j] = np.nan_to_num(Xtr[:,j], nan=med)\n",
        "    Xte[:,j] = np.nan_to_num(Xte[:,j], nan=med)\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "clf.fit(Xtr, ytr)\n",
        "proba = clf.predict_proba(Xte)[:,1]\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n[POST-FIX METADATA-ONLY]\")\n",
        "print(\"ACC:\", round(accuracy_score(yte,pred),4))\n",
        "print(\"F1 :\", round(f1_score(yte,pred),4))\n",
        "print(\"AUC:\", round(roc_auc_score(yte,proba),4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43H1pvLJhtLv",
        "outputId": "2e52f367-05e8-472a-a25c-b2faf738ca93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DIR: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456\n",
            "near_csv: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/near_dupes_norm_multiframe.csv\n",
            "Loaded normalized manifests:\n",
            " train: 1897  test: 473  total: 2370\n",
            "Near-dup edges inside current dataset: 41\n",
            "Clusters: 2338\n",
            "Clusters with size>1: 17\n",
            "\n",
            "[GROUP SPLIT] cluster overlap: 0\n",
            "New split sizes: 1887 483\n",
            "Train AI/REAL: 943 / 944\n",
            "Test  AI/REAL: 248 / 235\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/train_manifest_cluster_safe.csv\n",
            "Saved: /content/drive/MyDrive/ai_detection_eval/DATAFIX_NEUTRAL_PCA_20251214_195456/test_manifest_cluster_safe.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHA1 train_cluster_safe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1887/1887 [06:11<00:00,  5.09it/s]\n",
            "SHA1 test_cluster_safe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [01:33<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[POST-FIX CHECK] Exact SHA1 overlap: 0 | bad frames: 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Metadata train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1887/1887 [00:12<00:00, 147.84it/s]\n",
            "Metadata test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:03<00:00, 158.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[POST-FIX METADATA-ONLY]\n",
            "ACC: 0.677\n",
            "F1 : 0.608\n",
            "AUC: 0.6936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUM2_0xtl72M",
        "outputId": "f554fc35-f74b-4753-ce45-3003a1ca3709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found AI videos:   10\n",
            "Found Real videos: 10\n",
            "Using 20 videos total.\n",
            "Train videos: 14, Test videos: 6\n",
            "Train label dist: Counter({np.int64(1): 7, np.int64(0): 7})\n",
            "Test  label dist: Counter({np.int64(0): 3, np.int64(1): 3})\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "R(2+1)D deep features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:31<00:00,  6.50s/it]\n",
            "R(2+1)D deep features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:38<00:00,  6.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== R(2+1)D deep features ===\n",
            "Accuracy: 0.833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.67      0.80         3\n",
            "          AI       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.83         6\n",
            "   macro avg       0.88      0.83      0.83         6\n",
            "weighted avg       0.88      0.83      0.83         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 4, np.int64(0): 2})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Midtone-rich features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:21<00:00,  1.55s/it]\n",
            "Midtone-rich features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Midtone-rich features ===\n",
            "Accuracy: 0.333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.33      0.33      0.33         3\n",
            "          AI       0.33      0.33      0.33         3\n",
            "\n",
            "    accuracy                           0.33         6\n",
            "   macro avg       0.33      0.33      0.33         6\n",
            "weighted avg       0.33      0.33      0.33         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 3, np.int64(0): 3})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Forensic HF/noise/temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:16<00:00,  1.18s/it]\n",
            "Forensic HF/noise/temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forensic HF/noise/temporal ===\n",
            "Accuracy: 0.667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.60      1.00      0.75         3\n",
            "          AI       1.00      0.33      0.50         3\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.80      0.67      0.62         6\n",
            "weighted avg       0.80      0.67      0.62         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(0): 5, np.int64(1): 1})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pHash-style: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:15<00:00,  1.10s/it]\n",
            "pHash-style: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== pHash-style ===\n",
            "Accuracy: 0.833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.75      1.00      0.86         3\n",
            "          AI       1.00      0.67      0.80         3\n",
            "\n",
            "    accuracy                           0.83         6\n",
            "   macro avg       0.88      0.83      0.83         6\n",
            "weighted avg       0.88      0.83      0.83         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(0): 4, np.int64(1): 2})\n",
            "\n",
            "=== Fusion (R2+Midtone+Forensic+pHash) ===\n",
            "Accuracy: 0.833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.67      0.80         3\n",
            "          AI       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.83         6\n",
            "   macro avg       0.88      0.83      0.83         6\n",
            "weighted avg       0.88      0.83      0.83         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 4, np.int64(0): 2})\n",
            "\n",
            "=== SUMMARY (same split) ===\n",
            "R(2+1)D features            : 0.833\n",
            "Midtone-rich features       : 0.333\n",
            "Forensic HF/noise/temporal  : 0.667\n",
            "pHash-style                 : 0.833\n",
            "Fusion (all)                : 0.833\n",
            "\n",
            "Remember: with 10+10 videos this is a sanity check, not a benchmark. Improvements with more data are what you care about.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Real vs AI Video - Feature Comparisons (R(2+1)D + Forensic Ideas)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q kagglehub opencv-python-headless torch torchvision scikit-learn tqdm\n",
        "\n",
        "import os, random, math, sys\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# -------------------\n",
        "# 1. Mount Drive & config\n",
        "# -------------------\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/ai_video_project_test\")\n",
        "AI_DIR     = BASE_DIR / \"ai_videos_10\"\n",
        "REAL_DIR   = BASE_DIR / \"real_videos_10\"\n",
        "\n",
        "print(\"Found AI videos:  \", len(list(AI_DIR.glob(\"*.mp4\"))))\n",
        "print(\"Found Real videos:\", len(list(REAL_DIR.glob(\"*.avi\"))) + len(list(REAL_DIR.glob(\"*.mp4\"))))\n",
        "\n",
        "# Collect paths & labels\n",
        "video_paths = []\n",
        "labels = []  # 0 = real, 1 = AI\n",
        "\n",
        "for p in sorted(AI_DIR.glob(\"*.mp4\")):\n",
        "    video_paths.append(str(p))\n",
        "    labels.append(1)\n",
        "\n",
        "for ext in (\"*.avi\", \"*.mp4\"):\n",
        "    for p in sorted(REAL_DIR.glob(ext)):\n",
        "        video_paths.append(str(p))\n",
        "        labels.append(0)\n",
        "\n",
        "video_paths = np.array(video_paths)\n",
        "labels = np.array(labels, dtype=int)\n",
        "\n",
        "print(f\"Using {len(video_paths)} videos total.\")\n",
        "if len(video_paths) < 4 or len(set(labels)) < 2:\n",
        "    print(\"âŒ Need at least 1 AI and 1 Real video. Check your folders.\")\n",
        "    sys.exit(0)\n",
        "\n",
        "# -------------------\n",
        "# 2. Train/Test split\n",
        "# -------------------\n",
        "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
        "    video_paths,\n",
        "    labels,\n",
        "    test_size=0.3,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train videos: {len(X_train_paths)}, Test videos: {len(X_test_paths)}\")\n",
        "print(\"Train label dist:\", Counter(y_train))\n",
        "print(\"Test  label dist:\", Counter(y_test))\n",
        "\n",
        "# -------------------\n",
        "# 3. Device & R(2+1)D model\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "weights = R2Plus1D_18_Weights.DEFAULT\n",
        "r2 = r2plus1d_18(weights=weights)\n",
        "r2.fc = nn.Identity()     # get 512-d feature\n",
        "r2 = r2.to(device)\n",
        "r2.eval()\n",
        "\n",
        "# Transform for video frames going into R(2+1)D\n",
        "vid_transform = T.Compose([\n",
        "    T.ToPILImage(),                      # from numpy HWC\n",
        "    T.Resize((112, 112)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.43216, 0.394666, 0.37645],\n",
        "        std=[0.22803, 0.22145, 0.216989]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# 4. Frame sampling helper\n",
        "# -------------------\n",
        "def sample_frames(path, num_frames=32):\n",
        "    \"\"\"\n",
        "    Uniformly sample ~num_frames RGB frames from a video.\n",
        "    Pads by repeating last frame if too short.\n",
        "    Returns list of np.uint8 RGB frames or None.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = []\n",
        "\n",
        "    if total <= 0:\n",
        "        # fallback: read sequentially\n",
        "        while len(frames) < num_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "    else:\n",
        "        step = max(total // num_frames, 1)\n",
        "        idx = 0\n",
        "        grabbed = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if idx % step == 0 and grabbed < num_frames:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "                grabbed += 1\n",
        "            idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "\n",
        "    # pad if needed\n",
        "    while len(frames) < num_frames:\n",
        "        frames.append(frames[-1])\n",
        "    # trim if too many\n",
        "    return frames[:num_frames]\n",
        "\n",
        "# -------------------\n",
        "# 5. Feature extractors\n",
        "# -------------------\n",
        "\n",
        "# 5.1 R(2+1)D DEEP FEATURES\n",
        "def r2_features(path, num_frames=32):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(512, dtype=np.float32)\n",
        "\n",
        "    clip_tensors = []\n",
        "    for f in frames:\n",
        "        t = vid_transform(f)  # [3,112,112]\n",
        "        clip_tensors.append(t)\n",
        "\n",
        "    if not clip_tensors:\n",
        "        return np.zeros(512, dtype=np.float32)\n",
        "\n",
        "    clip = torch.stack(clip_tensors, dim=1).unsqueeze(0).to(device)  # [1,3,T,H,W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feat = r2(clip).cpu().numpy().reshape(-1)\n",
        "\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "\n",
        "# 5.2 MIDTONE-FOCUSED FEATURES (YCrCb mid-luma)\n",
        "def midtone_features(path, num_frames=24, bins=32):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(bins * 3, dtype=np.float32)\n",
        "\n",
        "    per_frame = []\n",
        "    for f in frames:\n",
        "        ycrcb = cv2.cvtColor(f, cv2.COLOR_RGB2YCrCb)\n",
        "        Y, Cr, Cb = cv2.split(ycrcb)\n",
        "\n",
        "        # midtone mask in Y\n",
        "        mask = (Y > 64) & (Y < 192)\n",
        "        if not np.any(mask):\n",
        "            mask = np.ones_like(Y, dtype=bool)\n",
        "\n",
        "        valsY  = Y[mask]\n",
        "        valsCr = Cr[mask]\n",
        "        valsCb = Cb[mask]\n",
        "\n",
        "        hY,  _ = np.histogram(valsY,  bins=bins, range=(0, 255), density=True)\n",
        "        hCr, _ = np.histogram(valsCr, bins=bins, range=(0, 255), density=True)\n",
        "        hCb, _ = np.histogram(valsCb, bins=bins, range=(0, 255), density=True)\n",
        "\n",
        "        per_frame.append(np.concatenate([hY, hCr, hCb]))\n",
        "\n",
        "    return np.mean(per_frame, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# 5.3 FORENSIC-STYLE: high-freq, noise, temporal diffs\n",
        "def forensic_features(path, num_frames=24, bins=16):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(bins * 3 + 2, dtype=np.float32)\n",
        "\n",
        "    hf_vals = []\n",
        "    noise_vals = []\n",
        "    diff_vals = []\n",
        "\n",
        "    prev_gray = None\n",
        "    for f in frames:\n",
        "        gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
        "\n",
        "        # high-frequency = |gray - blur|\n",
        "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        hf = np.abs(gray - blur).mean()\n",
        "        hf_vals.append(hf)\n",
        "\n",
        "        # noise residual approx = |gray - medianBlur|\n",
        "        med = cv2.medianBlur(gray.astype(np.uint8), 5).astype(np.float32)\n",
        "        noise = np.abs(gray - med).mean()\n",
        "        noise_vals.append(noise)\n",
        "\n",
        "        # temporal diff\n",
        "        if prev_gray is not None:\n",
        "            diff = np.abs(gray - prev_gray).mean()\n",
        "            diff_vals.append(diff)\n",
        "        prev_gray = gray\n",
        "\n",
        "    if len(hf_vals) == 0:\n",
        "        return np.zeros(bins * 3 + 2, dtype=np.float32)\n",
        "\n",
        "    hf_hist,    _ = np.histogram(hf_vals,    bins=bins, range=(0, 50), density=True)\n",
        "    noise_hist, _ = np.histogram(noise_vals, bins=bins, range=(0, 50), density=True)\n",
        "\n",
        "    if len(diff_vals) > 0:\n",
        "        diff_hist, _ = np.histogram(diff_vals, bins=bins, range=(0, 50), density=True)\n",
        "        diff_mean = float(np.mean(diff_vals))\n",
        "        diff_std  = float(np.std(diff_vals))\n",
        "    else:\n",
        "        diff_hist = np.zeros(bins)\n",
        "        diff_mean = 0.0\n",
        "        diff_std  = 0.0\n",
        "\n",
        "    feat = np.concatenate([hf_hist, noise_hist, diff_hist, [diff_mean, diff_std]])\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "\n",
        "# 5.4 PERCEPTUAL HASH-STYLE\n",
        "def ahash_frame(img, hash_size=8):\n",
        "    # img: RGB\n",
        "    small = cv2.resize(img, (hash_size, hash_size))\n",
        "    gray = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
        "    avg = gray.mean()\n",
        "    bits = (gray > avg).astype(np.float32)\n",
        "    return bits.flatten()  # length hash_size^2\n",
        "\n",
        "def phash_features(path, num_frames=16, hash_size=8):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(hash_size * hash_size, dtype=np.float32)\n",
        "    hashes = [ahash_frame(f, hash_size=hash_size) for f in frames]\n",
        "    return np.mean(hashes, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# 6. Helper: feature builder & SVM runner\n",
        "# -------------------\n",
        "def build_features(paths, kind):\n",
        "    feats = []\n",
        "    if kind == \"r2\":\n",
        "        desc = \"R(2+1)D deep features\"\n",
        "    elif kind == \"midtone\":\n",
        "        desc = \"Midtone-rich features\"\n",
        "    elif kind == \"forensic\":\n",
        "        desc = \"Forensic HF/noise/temporal\"\n",
        "    elif kind == \"phash\":\n",
        "        desc = \"pHash-style\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown kind\")\n",
        "\n",
        "    for p in tqdm(paths, desc=desc):\n",
        "        if kind == \"r2\":\n",
        "            feats.append(r2_features(p))\n",
        "        elif kind == \"midtone\":\n",
        "            feats.append(midtone_features(p))\n",
        "        elif kind == \"forensic\":\n",
        "            feats.append(forensic_features(p))\n",
        "        elif kind == \"phash\":\n",
        "            feats.append(phash_features(p))\n",
        "    return np.vstack(feats)\n",
        "\n",
        "def run_svm(name, Xtr, Xte, ytr, yte):\n",
        "    scaler = StandardScaler()\n",
        "    Xtr_s = scaler.fit_transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "    clf = SVC(kernel='rbf', class_weight='balanced', probability=False)\n",
        "    clf.fit(Xtr_s, ytr)\n",
        "    y_pred = clf.predict(Xte_s)\n",
        "\n",
        "    acc = accuracy_score(yte, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy: %.3f\" % acc)\n",
        "    print(classification_report(yte, y_pred, target_names=[\"Real\", \"AI\"]))\n",
        "    print(\"Test preds dist:\", Counter(y_pred))\n",
        "    return acc\n",
        "\n",
        "# -------------------\n",
        "# 7. Compute all feature sets & compare\n",
        "# -------------------\n",
        "# Main deep features\n",
        "Xtr_r2 = build_features(X_train_paths, \"r2\")\n",
        "Xte_r2 = build_features(X_test_paths, \"r2\")\n",
        "acc_r2 = run_svm(\"R(2+1)D deep features\", Xtr_r2, Xte_r2, y_train, y_test)\n",
        "\n",
        "# Midtone\n",
        "Xtr_mid = build_features(X_train_paths, \"midtone\")\n",
        "Xte_mid = build_features(X_test_paths, \"midtone\")\n",
        "acc_mid = run_svm(\"Midtone-rich features\", Xtr_mid, Xte_mid, y_train, y_test)\n",
        "\n",
        "# Forensic HF / noise / temporal\n",
        "Xtr_for = build_features(X_train_paths, \"forensic\")\n",
        "Xte_for = build_features(X_test_paths, \"forensic\")\n",
        "acc_for = run_svm(\"Forensic HF/noise/temporal\", Xtr_for, Xte_for, y_train, y_test)\n",
        "\n",
        "# pHash-style\n",
        "Xtr_ph = build_features(X_train_paths, \"phash\")\n",
        "Xte_ph = build_features(X_test_paths, \"phash\")\n",
        "acc_ph = run_svm(\"pHash-style\", Xtr_ph, Xte_ph, y_train, y_test)\n",
        "\n",
        "# Fusion of all\n",
        "Xtr_fuse = np.hstack([Xtr_r2, Xtr_mid, Xtr_for, Xtr_ph])\n",
        "Xte_fuse = np.hstack([Xte_r2, Xte_mid, Xte_for, Xte_ph])\n",
        "acc_fuse = run_svm(\"Fusion (R2+Midtone+Forensic+pHash)\", Xtr_fuse, Xte_fuse, y_train, y_test)\n",
        "\n",
        "print(\"\\n=== SUMMARY (same split) ===\")\n",
        "print(f\"R(2+1)D features            : {acc_r2:.3f}\")\n",
        "print(f\"Midtone-rich features       : {acc_mid:.3f}\")\n",
        "print(f\"Forensic HF/noise/temporal  : {acc_for:.3f}\")\n",
        "print(f\"pHash-style                 : {acc_ph:.3f}\")\n",
        "print(f\"Fusion (all)                : {acc_fuse:.3f}\")\n",
        "print(\"\\nRemember: with 10+10 videos this is a sanity check, \"\n",
        "      \"not a benchmark. Improvements with more data are what you care about.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S351lU3__0t-",
        "outputId": "a242a7fc-1f42-4234-83e1-528daee54eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found AI videos:   10\n",
            "Found Real videos: 10\n",
            "Train videos: 14, Test videos: 6\n",
            "Train label dist: Counter({np.int64(1): 7, np.int64(0): 7})\n",
            "Test  label dist: Counter({np.int64(0): 3, np.int64(1): 3})\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "R(2+1)D deep: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:31<00:00,  6.51s/it]\n",
            "R(2+1)D deep: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:39<00:00,  6.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== R(2+1)D deep features ===\n",
            "Accuracy: 0.833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.67      0.80         3\n",
            "          AI       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.83         6\n",
            "   macro avg       0.88      0.83      0.83         6\n",
            "weighted avg       0.88      0.83      0.83         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 4, np.int64(0): 2})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Midtone+Temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:20<00:00,  1.43s/it]\n",
            "Midtone+Temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.48s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Midtone+Temporal features ===\n",
            "Accuracy: 0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.00      0.00      0.00         3\n",
            "          AI       0.50      1.00      0.67         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.25      0.50      0.33         6\n",
            "weighted avg       0.25      0.50      0.33         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 6})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Forensic+Temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:16<00:00,  1.18s/it]\n",
            "Forensic+Temporal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Forensic+Temporal features ===\n",
            "Accuracy: 0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.50      0.67      0.57         3\n",
            "          AI       0.50      0.33      0.40         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(0): 4, np.int64(1): 2})\n",
            "\n",
            "=== Fusion (R2 + Midtone+T + Forensic+T) ===\n",
            "Accuracy: 0.833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.67      0.80         3\n",
            "          AI       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.83         6\n",
            "   macro avg       0.88      0.83      0.83         6\n",
            "weighted avg       0.88      0.83      0.83         6\n",
            "\n",
            "Test preds dist: Counter({np.int64(1): 4, np.int64(0): 2})\n",
            "\n",
            "=== SUMMARY (same split) ===\n",
            "R(2+1)D deep features          : 0.833\n",
            "Midtone+Temporal features      : 0.500\n",
            "Forensic+Temporal features     : 0.500\n",
            "Fusion (all three, temporal)   : 0.833\n",
            "\n",
            "This is now aligned with your idea: every branch encodes temporal behavior, not just static frame statistics.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Real vs AI: R(2+1)D + Midtone+Temporal + Forensic+Temporal\n",
        "# ================================\n",
        "\n",
        "!pip install -q opencv-python-headless torch torchvision scikit-learn tqdm\n",
        "\n",
        "import os, sys, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# -------------------\n",
        "# 1. Mount & paths\n",
        "# -------------------\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/ai_video_project_test\")\n",
        "AI_DIR   = BASE_DIR / \"ai_videos_10\"\n",
        "REAL_DIR = BASE_DIR / \"real_videos_10\"\n",
        "\n",
        "ai_paths   = sorted(AI_DIR.glob(\"*.mp4\"))\n",
        "real_paths = sorted(list(REAL_DIR.glob(\"*.avi\")) + list(REAL_DIR.glob(\"*.mp4\")))\n",
        "\n",
        "print(\"Found AI videos:  \", len(ai_paths))\n",
        "print(\"Found Real videos:\", len(real_paths))\n",
        "\n",
        "if len(ai_paths) == 0 or len(real_paths) == 0:\n",
        "    print(\"âŒ Need at least 1 AI and 1 Real video.\")\n",
        "    print(\"Check folders:\\n \", AI_DIR, \"\\n \", REAL_DIR)\n",
        "    sys.exit(0)\n",
        "\n",
        "video_paths = np.array([str(p) for p in ai_paths] + [str(p) for p in real_paths])\n",
        "labels      = np.array([1]*len(ai_paths) + [0]*len(real_paths), dtype=int)  # 1=AI, 0=Real\n",
        "\n",
        "# -------------------\n",
        "# 2. Train/Test split\n",
        "# -------------------\n",
        "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
        "    video_paths,\n",
        "    labels,\n",
        "    test_size=0.3,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train videos: {len(X_train_paths)}, Test videos: {len(X_test_paths)}\")\n",
        "print(\"Train label dist:\", Counter(y_train))\n",
        "print(\"Test  label dist:\", Counter(y_test))\n",
        "\n",
        "if len(set(y_train)) < 2 or len(set(y_test)) < 2:\n",
        "    print(\"âŒ Split collapsed to single class. Adjust dataset or random_state.\")\n",
        "    sys.exit(0)\n",
        "\n",
        "# -------------------\n",
        "# 3. Device & R(2+1)D\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "weights = R2Plus1D_18_Weights.DEFAULT\n",
        "r2 = r2plus1d_18(weights=weights)\n",
        "r2.fc = nn.Identity()  # 512-d temporal feature\n",
        "r2 = r2.to(device)\n",
        "r2.eval()\n",
        "\n",
        "vid_transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((112, 112)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.43216, 0.394666, 0.37645],\n",
        "        std=[0.22803, 0.22145, 0.216989]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# 4. Frame sampling\n",
        "# -------------------\n",
        "def sample_frames(path, num_frames=32):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = []\n",
        "\n",
        "    if total <= 0:\n",
        "        # fallback: read sequentially\n",
        "        while len(frames) < num_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "    else:\n",
        "        step = max(total // num_frames, 1)\n",
        "        idx = 0\n",
        "        grabbed = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if idx % step == 0 and grabbed < num_frames:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "                grabbed += 1\n",
        "            idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "\n",
        "    while len(frames) < num_frames:\n",
        "        frames.append(frames[-1])\n",
        "    return frames[:num_frames]\n",
        "\n",
        "# -------------------\n",
        "# 5. Feature extractors (with temporal awareness)\n",
        "# -------------------\n",
        "\n",
        "# 5.1 R(2+1)D deep features (inherently temporal)\n",
        "def r2_features(path, num_frames=32):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(512, dtype=np.float32)\n",
        "\n",
        "    clip_tensors = [vid_transform(f) for f in frames]\n",
        "    if not clip_tensors:\n",
        "        return np.zeros(512, dtype=np.float32)\n",
        "\n",
        "    clip = torch.stack(clip_tensors, dim=1).unsqueeze(0).to(device)  # [1,3,T,H,W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feat = r2(clip).cpu().numpy().reshape(-1)\n",
        "\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "# 5.2 Midtone-focused + temporal dynamics\n",
        "def midtone_temporal_features(path, num_frames=24, bins=32):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(bins * 3 * 3, dtype=np.float32)  # mean/std/delta blocks\n",
        "\n",
        "    hist_list = []\n",
        "\n",
        "    for f in frames:\n",
        "        ycrcb = cv2.cvtColor(f, cv2.COLOR_RGB2YCrCb)\n",
        "        Y, Cr, Cb = cv2.split(ycrcb)\n",
        "\n",
        "        # Focus on midtones\n",
        "        mask = (Y > 64) & (Y < 192)\n",
        "        if not np.any(mask):\n",
        "            mask = np.ones_like(Y, dtype=bool)\n",
        "\n",
        "        valsY  = Y[mask]\n",
        "        valsCr = Cr[mask]\n",
        "        valsCb = Cb[mask]\n",
        "\n",
        "        hY,  _ = np.histogram(valsY,  bins=bins, range=(0, 255), density=True)\n",
        "        hCr, _ = np.histogram(valsCr, bins=bins, range=(0, 255), density=True)\n",
        "        hCb, _ = np.histogram(valsCb, bins=bins, range=(0, 255), density=True)\n",
        "\n",
        "        hist = np.concatenate([hY, hCr, hCb])\n",
        "        hist_list.append(hist)\n",
        "\n",
        "    H = np.stack(hist_list, axis=0)  # [T, 3*bins]\n",
        "\n",
        "    # Temporal-aware summary:\n",
        "    # - mean over time\n",
        "    # - std over time\n",
        "    # - mean absolute change between consecutive frames (temporal smoothness / jitter)\n",
        "    mean_hist = H.mean(axis=0)\n",
        "    std_hist  = H.std(axis=0)\n",
        "    if H.shape[0] > 1:\n",
        "        delta_hist = np.abs(H[1:] - H[:-1]).mean(axis=0)\n",
        "    else:\n",
        "        delta_hist = np.zeros_like(mean_hist)\n",
        "\n",
        "    feat = np.concatenate([mean_hist, std_hist, delta_hist])\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "# 5.3 Forensic-ish + temporal dynamics\n",
        "def forensic_temporal_features(path, num_frames=24, bins=16):\n",
        "    frames = sample_frames(path, num_frames)\n",
        "    if frames is None:\n",
        "        return np.zeros(bins * 3 + 6, dtype=np.float32)\n",
        "\n",
        "    hf_vals = []\n",
        "    noise_vals = []\n",
        "    diff_vals = []\n",
        "\n",
        "    hf_frame = []\n",
        "    noise_frame = []\n",
        "\n",
        "    prev_gray = None\n",
        "    for f in frames:\n",
        "        gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
        "\n",
        "        # High-frequency component\n",
        "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        hf = np.abs(gray - blur)\n",
        "        hf_vals.append(hf.mean())\n",
        "        hf_frame.append(hf.mean())\n",
        "\n",
        "        # Noise residual (median-based)\n",
        "        med = cv2.medianBlur(gray.astype(np.uint8), 5).astype(np.float32)\n",
        "        noise = np.abs(gray - med)\n",
        "        noise_vals.append(noise.mean())\n",
        "        noise_frame.append(noise.mean())\n",
        "\n",
        "        # Temporal difference\n",
        "        if prev_gray is not None:\n",
        "            diff = np.abs(gray - prev_gray).mean()\n",
        "            diff_vals.append(diff)\n",
        "        prev_gray = gray\n",
        "\n",
        "    if len(hf_vals) == 0:\n",
        "        return np.zeros(bins * 3 + 6, dtype=np.float32)\n",
        "\n",
        "    # Histograms of per-frame HF and noise\n",
        "    hf_hist,    _ = np.histogram(hf_vals,    bins=bins, range=(0, 50), density=True)\n",
        "    noise_hist, _ = np.histogram(noise_vals, bins=bins, range=(0, 50), density=True)\n",
        "\n",
        "    # Temporal hist for frame-to-frame diff\n",
        "    if len(diff_vals) > 0:\n",
        "        diff_hist, _ = np.histogram(diff_vals, bins=bins, range=(0, 50), density=True)\n",
        "        diff_mean = float(np.mean(diff_vals))\n",
        "        diff_std  = float(np.std(diff_vals))\n",
        "    else:\n",
        "        diff_hist = np.zeros(bins)\n",
        "        diff_mean = 0.0\n",
        "        diff_std  = 0.0\n",
        "\n",
        "    # Extra temporal stats: variability of hf/noise over time\n",
        "    hf_mean = float(np.mean(hf_frame))\n",
        "    hf_std  = float(np.std(hf_frame))\n",
        "    noise_mean = float(np.mean(noise_frame))\n",
        "    noise_std  = float(np.std(noise_frame))\n",
        "\n",
        "    feat = np.concatenate([\n",
        "        hf_hist,\n",
        "        noise_hist,\n",
        "        diff_hist,\n",
        "        [hf_mean, hf_std, noise_mean, noise_std, diff_mean, diff_std],\n",
        "    ])\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "# -------------------\n",
        "# 6. Build features & SVM helper\n",
        "# -------------------\n",
        "def build_features(paths, kind):\n",
        "    feats = []\n",
        "    if kind == \"r2\":\n",
        "        desc = \"R(2+1)D deep\"\n",
        "    elif kind == \"midtone\":\n",
        "        desc = \"Midtone+Temporal\"\n",
        "    elif kind == \"forensic\":\n",
        "        desc = \"Forensic+Temporal\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown kind\")\n",
        "\n",
        "    for p in tqdm(paths, desc=desc):\n",
        "        if kind == \"r2\":\n",
        "            feats.append(r2_features(p))\n",
        "        elif kind == \"midtone\":\n",
        "            feats.append(midtone_temporal_features(p))\n",
        "        elif kind == \"forensic\":\n",
        "            feats.append(forensic_temporal_features(p))\n",
        "\n",
        "    return np.vstack(feats)\n",
        "\n",
        "def run_svm(name, Xtr, Xte, ytr, yte):\n",
        "    scaler = StandardScaler()\n",
        "    Xtr_s = scaler.fit_transform(Xtr)\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "\n",
        "    clf = SVC(kernel='rbf', class_weight='balanced')\n",
        "    clf.fit(Xtr_s, ytr)\n",
        "    y_pred = clf.predict(Xte_s)\n",
        "\n",
        "    acc = accuracy_score(yte, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy: %.3f\" % acc)\n",
        "    print(classification_report(yte, y_pred, target_names=[\"Real\", \"AI\"]))\n",
        "    print(\"Test preds dist:\", Counter(y_pred))\n",
        "    return acc\n",
        "\n",
        "# -------------------\n",
        "# 7. Extract & evaluate\n",
        "# -------------------\n",
        "Xtr_r2 = build_features(X_train_paths, \"r2\")\n",
        "Xte_r2 = build_features(X_test_paths, \"r2\")\n",
        "acc_r2 = run_svm(\"R(2+1)D deep features\", Xtr_r2, Xte_r2, y_train, y_test)\n",
        "\n",
        "Xtr_mid = build_features(X_train_paths, \"midtone\")\n",
        "Xte_mid = build_features(X_test_paths, \"midtone\")\n",
        "acc_mid = run_svm(\"Midtone+Temporal features\", Xtr_mid, Xte_mid, y_train, y_test)\n",
        "\n",
        "Xtr_for = build_features(X_train_paths, \"forensic\")\n",
        "Xte_for = build_features(X_test_paths, \"forensic\")\n",
        "acc_for = run_svm(\"Forensic+Temporal features\", Xtr_for, Xte_for, y_train, y_test)\n",
        "\n",
        "# Fusion: all temporal-aware channels\n",
        "Xtr_fuse = np.hstack([Xtr_r2, Xtr_mid, Xtr_for])\n",
        "Xte_fuse = np.hstack([Xte_r2, Xte_mid, Xte_for])\n",
        "acc_fuse = run_svm(\"Fusion (R2 + Midtone+T + Forensic+T)\", Xtr_fuse, Xte_fuse, y_train, y_test)\n",
        "\n",
        "print(\"\\n=== SUMMARY (same split) ===\")\n",
        "print(f\"R(2+1)D deep features          : {acc_r2:.3f}\")\n",
        "print(f\"Midtone+Temporal features      : {acc_mid:.3f}\")\n",
        "print(f\"Forensic+Temporal features     : {acc_for:.3f}\")\n",
        "print(f\"Fusion (all three, temporal)   : {acc_fuse:.3f}\")\n",
        "print(\"\\nThis is now aligned with your idea: every branch encodes temporal behavior, \"\n",
        "      \"not just static frame statistics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdsfUbBhieDL",
        "outputId": "032d4c10-7a0d-4900-f8ab-d936cd8634cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found AI videos:   10\n",
            "Found Real videos: 10\n",
            "Train videos: 14  Test videos: 6\n",
            "Train label dist: Counter({np.int64(1): 7, np.int64(0): 7})\n",
            "Test  label dist: Counter({np.int64(0): 3, np.int64(1): 3})\n",
            "Using device: cpu\n",
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120M/120M [00:00<00:00, 156MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready. Training ONLY fusion head over 3x R(2+1)D branches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - loss: 2.3166 - acc: 0.357\n",
            "Epoch 2/5 - loss: 0.9657 - acc: 0.357\n",
            "Epoch 3/5 - loss: 0.6933 - acc: 0.571\n",
            "Epoch 4/5 - loss: 0.6575 - acc: 0.571\n",
            "Epoch 5/5 - loss: 0.7390 - acc: 0.500\n",
            "\n",
            "=== TEST RESULTS: Multi-Stream R(2+1)D Fusion ===\n",
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.50      1.00      0.67         3\n",
            "          AI       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.25      0.50      0.33         6\n",
            "weighted avg       0.25      0.50      0.33         6\n",
            "\n",
            "Label dist: Counter({0: 3, 1: 3})\n",
            "Pred  dist: Counter({0: 6})\n",
            "\n",
            "What this is doing:\n",
            "- Stream 1: R(2+1)D on raw frames (temporal baseline).\n",
            "- Stream 2: R(2+1)D on midtone-only frames (your midtone cue, but temporal).\n",
            "- Stream 3: R(2+1)D on high-frequency residual frames (forensic-ish cue, temporal).\n",
            "- All temporal (3D conv), all fused into ONE classifier head we train.\n",
            "\n",
            "When you scale:\n",
            "- Drop in 1200 AI + 800 real into those folders.\n",
            "- Increase EPOCHS / batch size.\n",
            "- Same architecture; youâ€™re just feeding more data.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless torch torchvision scikit-learn tqdm\n",
        "\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ================== CONFIG ==================\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/ai_video_project_test\")\n",
        "AI_DIR   = BASE_DIR / \"ai_videos_10\"\n",
        "REAL_DIR = BASE_DIR / \"real_videos_10\"\n",
        "\n",
        "NUM_FRAMES   = 16          # temporal window\n",
        "FRAME_SIZE   = 112         # R(2+1)D default\n",
        "TEST_SIZE    = 0.3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ================== DISCOVER VIDEOS ==================\n",
        "def list_videos():\n",
        "    ai_v = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        ai_v += list(AI_DIR.glob(ext))\n",
        "    real_v = []\n",
        "    for ext in (\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\"):\n",
        "        real_v += list(REAL_DIR.glob(ext))\n",
        "    return ai_v, real_v\n",
        "\n",
        "ai_videos, real_videos = list_videos()\n",
        "print(f\"Found AI videos:   {len(ai_videos)} in {AI_DIR}\")\n",
        "print(f\"Found Real videos: {len(real_videos)} in {REAL_DIR}\")\n",
        "\n",
        "if len(ai_videos) < 1 or len(real_videos) < 1:\n",
        "    raise SystemExit(\n",
        "        f\"âŒ Need at least 1 AI and 1 Real video.\\n\"\n",
        "        f\"AI found: {len(ai_videos)}, Real found: {len(real_videos)}.\\n\"\n",
        "        f\"Check:\\n  {AI_DIR}\\n  {REAL_DIR}\"\n",
        "    )\n",
        "\n",
        "videos = [str(p) for p in ai_videos] + [str(p) for p in real_videos]\n",
        "labels = [1] * len(ai_videos) + [0] * len(real_videos)  # 1=AI, 0=Real\n",
        "\n",
        "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
        "    videos,\n",
        "    labels,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(f\"Train videos: {len(X_train_paths)}  Test videos: {len(X_test_paths)}\")\n",
        "print(\"Train label dist:\", Counter(y_train))\n",
        "print(\"Test  label dist:\", Counter(y_test))\n",
        "\n",
        "# ================== VIDEO â†’ CLIP ==================\n",
        "def read_video_frames(path, max_frames=NUM_FRAMES*4):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    while True and len(frames) < max_frames:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def sample_indices(n_frames, num_samples):\n",
        "    if n_frames <= num_samples:\n",
        "        idx = list(range(n_frames))\n",
        "        while len(idx) < num_samples:\n",
        "            idx.append(idx[-1])\n",
        "        return idx\n",
        "    step = n_frames / float(num_samples)\n",
        "    return [int(i * step) for i in range(num_samples)]\n",
        "\n",
        "resize_tf = T.Resize((FRAME_SIZE, FRAME_SIZE))\n",
        "\n",
        "def make_clip_tensor(path, variant=\"raw\"):\n",
        "    \"\"\"\n",
        "    Returns clip tensor of shape (3, T, H, W) for R(2+1)D.\n",
        "    variant: raw | midtone | highfreq | grad\n",
        "    \"\"\"\n",
        "    frames = read_video_frames(path, max_frames=NUM_FRAMES*8)\n",
        "    if len(frames) == 0:\n",
        "        # fallback: dummy noise (won't happen if videos exist)\n",
        "        f = np.random.randint(0, 255, (FRAME_SIZE, FRAME_SIZE, 3), dtype=np.uint8)\n",
        "        frames = [f] * NUM_FRAMES\n",
        "\n",
        "    idxs = sample_indices(len(frames), NUM_FRAMES)\n",
        "    clip = []\n",
        "    for i in idxs:\n",
        "        img = frames[i]\n",
        "\n",
        "        # --- apply variant transforms ---\n",
        "        if variant == \"midtone\":\n",
        "            # simple midtone mask on luminance\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            mask = (gray > 64) & (gray < 192)\n",
        "            mt = np.zeros_like(img)\n",
        "            mt[mask] = img[mask]\n",
        "            img = mt\n",
        "\n",
        "        elif variant == \"highfreq\":\n",
        "            # high frequency via Laplacian\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "            hf = cv2.Laplacian(blur, cv2.CV_32F)\n",
        "            hf = cv2.normalize(hf, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            img = cv2.cvtColor(hf, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        elif variant == \"grad\":\n",
        "            # gradient magnitude (Sobel)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "            gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "            mag = cv2.magnitude(gx, gy)\n",
        "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            img = cv2.cvtColor(mag, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # to tensor [C,H,W]\n",
        "        t = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "        t = resize_tf(t)\n",
        "        clip.append(t)\n",
        "\n",
        "    clip = torch.stack(clip, dim=1)  # (3, T, H, W)\n",
        "    return clip\n",
        "\n",
        "# ================== R(2+1)D BACKBONE ==================\n",
        "print(\"Loading R(2+1)D-18 backbone...\")\n",
        "weights = R2Plus1D_18_Weights.KINETICS400_V1\n",
        "r2 = r2plus1d_18(weights=weights)\n",
        "# Strip classifier head â†’ 512-d features\n",
        "r2.fc = nn.Identity()\n",
        "r2 = r2.to(device)\n",
        "r2.eval()\n",
        "print(\"Model ready.\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def r2_features(path, variant=\"raw\"):\n",
        "    clip = make_clip_tensor(path, variant=variant).unsqueeze(0).to(device)  # (1,3,T,H,W)\n",
        "    feat = r2(clip)  # (1, 512)\n",
        "    return feat.squeeze(0).cpu().numpy()\n",
        "\n",
        "def build_feature_matrix(paths, variant):\n",
        "    X = []\n",
        "    for p in tqdm(paths, desc=f\"R(2+1)D {variant}\"):\n",
        "        try:\n",
        "            X.append(r2_features(p, variant=variant))\n",
        "        except Exception as e:\n",
        "            # if anything breaks, use zeros to keep shapes consistent\n",
        "            print(f\"[WARN] {variant} failed on {p}: {e}\")\n",
        "            X.append(np.zeros(512, dtype=np.float32))\n",
        "    return np.vstack(X)\n",
        "\n",
        "# ================== TRAIN / EVAL HELPERS ==================\n",
        "def run_svm(name, Xtr, Xte, ytr, yte):\n",
        "    clf = SVC(kernel=\"rbf\", class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, ytr)\n",
        "    y_pred = clf.predict(Xte)\n",
        "    acc = accuracy_score(yte, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(classification_report(yte, y_pred, target_names=[\"Real(0)\", \"AI(1)\"]))\n",
        "    print(\"Test preds dist:\", Counter(y_pred))\n",
        "    return acc\n",
        "\n",
        "def run_knn(name, Xtr, Xte, ytr, yte, k=3):\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    y_pred = clf.predict(Xte)\n",
        "    acc = accuracy_score(yte, y_pred)\n",
        "    print(f\"\\n=== {name} (k={k}) ===\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(classification_report(yte, y_pred, target_names=[\"Real(0)\", \"AI(1)\"]))\n",
        "    print(\"Test preds dist:\", Counter(y_pred))\n",
        "    return acc\n",
        "\n",
        "# ================== BUILD FEATURES & RUN ==================\n",
        "# R(2+1)D on:\n",
        "#  - raw\n",
        "#  - midtone\n",
        "#  - highfreq\n",
        "#  - grad\n",
        "print(\"\\nExtracting features for all variants (this is the core of your idea)...\")\n",
        "\n",
        "Xtr_raw = build_feature_matrix(X_train_paths, \"raw\")\n",
        "Xte_raw = build_feature_matrix(X_test_paths, \"raw\")\n",
        "\n",
        "Xtr_mid = build_feature_matrix(X_train_paths, \"midtone\")\n",
        "Xte_mid = build_feature_matrix(X_test_paths, \"midtone\")\n",
        "\n",
        "Xtr_hf = build_feature_matrix(X_train_paths, \"highfreq\")\n",
        "Xte_hf = build_feature_matrix(X_test_paths, \"highfreq\")\n",
        "\n",
        "Xtr_grad = build_feature_matrix(X_train_paths, \"grad\")\n",
        "Xte_grad = build_feature_matrix(X_test_paths, \"grad\")\n",
        "\n",
        "# Fusion of all forensic cues + temporal backbone\n",
        "Xtr_fuse = np.concatenate([Xtr_raw, Xtr_mid, Xtr_hf, Xtr_grad], axis=1)\n",
        "Xte_fuse = np.concatenate([Xte_raw, Xte_mid, Xte_hf, Xte_grad], axis=1)\n",
        "\n",
        "print(\"\\n========== RESULTS (same train/test split) ==========\")\n",
        "acc_raw  = run_svm(\"R(2+1)D RAW only\", Xtr_raw,  Xte_raw,  y_train, y_test)\n",
        "acc_mid  = run_svm(\"R(2+1)D MIDTONE only\", Xtr_mid, Xte_mid, y_train, y_test)\n",
        "acc_hf   = run_svm(\"R(2+1)D HIGH-FREQ only\", Xtr_hf, Xte_hf, y_train, y_test)\n",
        "acc_grad = run_svm(\"R(2+1)D GRAD-MAG only\", Xtr_grad, Xte_grad, y_train, y_test)\n",
        "acc_fuse = run_svm(\"R(2+1)D FUSION (raw+mid+hf+grad)\", Xtr_fuse, Xte_fuse, y_train, y_test)\n",
        "\n",
        "# CNN (R(2+1)D raw) + KNN on embeddings\n",
        "acc_knn = run_knn(\"R(2+1)D RAW + KNN\", Xtr_raw, Xte_raw, y_train, y_test, k=3)\n",
        "\n",
        "print(\"\\n=== SUMMARY (toy, Nâ‰ˆ20) ===\")\n",
        "print(f\"RAW only              : {acc_raw:.3f}\")\n",
        "print(f\"MIDTONE only          : {acc_mid:.3f}\")\n",
        "print(f\"HIGH-FREQ only        : {acc_hf:.3f}\")\n",
        "print(f\"GRAD-MAG only         : {acc_grad:.3f}\")\n",
        "print(f\"FUSION (all cues)     : {acc_fuse:.3f}\")\n",
        "print(f\"RAW + KNN (embeddings): {acc_knn:.3f}\")\n",
        "print(\"\\nRemember: with 10+10 videos this is extremely noisy; trends matter, not the exact numbers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaW2I0sSErjj",
        "outputId": "56a2fdf1-2389-4931-da7c-062a04d163f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n",
            "Found Sora AI vids   : 10\n",
            "Found curated AI vids: 10\n",
            "Found HF AI vids     : 0\n",
            "Total AI vids (uniq) : 20\n",
            "Total Real vids      : 10\n",
            "\n",
            "Total videos used: 30\n",
            "Label dist: Counter({1: 20, 0: 10})\n",
            "\n",
            "Train videos: 21  Test videos: 9\n",
            "Train label dist: Counter({1: 14, 0: 7})\n",
            "Test  label dist: Counter({1: 6, 0: 3})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rR(2+1)D[raw]:   0%|          | 0/21 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# ======================\n",
        "# MOUNT DRIVE & CONFIG\n",
        "# ======================\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Your folders\n",
        "SORA_DIR  = Path(\"/content/drive/MyDrive/soravideo\")                    # your 10 Sora AI videos\n",
        "BASE_DIR  = Path(\"/content/drive/MyDrive/ai_video_project_test\")\n",
        "AI_DIR    = BASE_DIR / \"ai_videos_10\"                                   # extra AI (if any)\n",
        "REAL_DIR  = BASE_DIR / \"real_videos_10\"                                 # UCF101 real videos, etc.\n",
        "\n",
        "HF_ROOT   = Path(\"/root/.cache/huggingface\")                            # optional: if you've cached HF deepfake sets\n",
        "\n",
        "MAX_SORA_AI = 10\n",
        "MAX_HF_AI   = 10\n",
        "MAX_REAL    = 10\n",
        "\n",
        "TEST_SIZE   = 0.3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "NUM_FRAMES  = 16      # frames per clip\n",
        "FRAME_STEP  = 2       # sample every Nth frame\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ======================\n",
        "# HELPER: LIST VIDEOS\n",
        "# ======================\n",
        "\n",
        "def list_videos(path, exts=(\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        return []\n",
        "    return [f for f in p.rglob(\"*\") if f.suffix.lower() in exts]\n",
        "\n",
        "# 1) Sora AI\n",
        "sora_ai = list_videos(SORA_DIR)\n",
        "if len(sora_ai) > MAX_SORA_AI:\n",
        "    sora_ai = random.sample(sora_ai, MAX_SORA_AI)\n",
        "\n",
        "# 2) Curated AI\n",
        "curated_ai = list_videos(AI_DIR)\n",
        "\n",
        "# 3) HF AI (optional heuristic)\n",
        "hf_ai = []\n",
        "if HF_ROOT.exists():\n",
        "    for p in HF_ROOT.rglob(\"*\"):\n",
        "        if p.suffix.lower() in (\".mp4\", \".avi\", \".mkv\"):\n",
        "            s = str(p).lower()\n",
        "            if any(k in s for k in [\"deepfake\", \"fake\", \"manip\", \"faceforensics\", \"forgery\", \"neural\"]):\n",
        "                hf_ai.append(p)\n",
        "if len(hf_ai) > MAX_HF_AI:\n",
        "    hf_ai = random.sample(hf_ai, MAX_HF_AI)\n",
        "\n",
        "# 4) Real\n",
        "real_vids = list_videos(REAL_DIR)\n",
        "if len(real_vids) > MAX_REAL:\n",
        "    real_vids = random.sample(real_vids, MAX_REAL)\n",
        "\n",
        "# Merge AI (ensure unique)\n",
        "ai_vids = list(dict.fromkeys(sora_ai + curated_ai + hf_ai))\n",
        "\n",
        "print(f\"Found Sora AI vids   : {len(sora_ai)}\")\n",
        "print(f\"Found curated AI vids: {len(curated_ai)}\")\n",
        "print(f\"Found HF AI vids     : {len(hf_ai)}\")\n",
        "print(f\"Total AI vids (uniq) : {len(ai_vids)}\")\n",
        "print(f\"Total Real vids      : {len(real_vids)}\")\n",
        "\n",
        "if len(ai_vids) < 5 or len(real_vids) < 5:\n",
        "    raise SystemExit(\n",
        "        \"Need at least 5 AI and 5 Real videos.\\n\"\n",
        "        f\"AI: {len(ai_vids)}, Real: {len(real_vids)}.\\n\"\n",
        "        \"Check your folders:\"\n",
        "        \"\\n  /content/drive/MyDrive/soravideo\"\n",
        "        \"\\n  /content/drive/MyDrive/ai_video_project_test/ai_videos_10\"\n",
        "        \"\\n  /content/drive/MyDrive/ai_video_project_test/real_videos_10\"\n",
        "    )\n",
        "\n",
        "# ======================\n",
        "# BUILD LABELED LIST\n",
        "# ======================\n",
        "\n",
        "all_paths = []\n",
        "all_labels = []  # 0=Real, 1=AI\n",
        "\n",
        "for p in real_vids:\n",
        "    all_paths.append(str(p))\n",
        "    all_labels.append(0)\n",
        "\n",
        "for p in ai_vids:\n",
        "    all_paths.append(str(p))\n",
        "    all_labels.append(1)\n",
        "\n",
        "print(f\"\\nTotal videos used: {len(all_paths)}\")\n",
        "print(\"Label dist:\", Counter(all_labels))\n",
        "\n",
        "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
        "    all_paths,\n",
        "    all_labels,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=all_labels,\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain videos: {len(X_train_paths)}  Test videos: {len(X_test_paths)}\")\n",
        "print(\"Train label dist:\", Counter(y_train))\n",
        "print(\"Test  label dist:\", Counter(y_test))\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test  = np.array(y_test)\n",
        "\n",
        "# ======================\n",
        "# R(2+1)D BACKBONE (FROZEN)\n",
        "# ======================\n",
        "\n",
        "weights = torchvision.models.video.R2Plus1D_18_Weights.KINETICS400_V1\n",
        "r2 = torchvision.models.video.r2plus1d_18(weights=weights)\n",
        "r2.fc = nn.Identity()     # 512-d embedding\n",
        "r2 = r2.to(device)\n",
        "r2.eval()\n",
        "\n",
        "# Use official normalization stats\n",
        "_norm = weights.transforms()\n",
        "mean = np.array(_norm.mean, dtype=np.float32)\n",
        "std  = np.array(_norm.std, dtype=np.float32)\n",
        "\n",
        "# ======================\n",
        "# FRAME SAMPLING\n",
        "# ======================\n",
        "\n",
        "def sample_frames(path, num_frames=NUM_FRAMES, every=FRAME_STEP):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if idx % every == 0:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "\n",
        "    if not frames:\n",
        "        return None\n",
        "\n",
        "    # uniform sample / pad\n",
        "    if len(frames) >= num_frames:\n",
        "        ids = np.linspace(0, len(frames) - 1, num_frames).astype(int)\n",
        "        frames = [frames[i] for i in ids]\n",
        "    else:\n",
        "        while len(frames) < num_frames:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "    return frames  # list of HxWx3 uint8\n",
        "\n",
        "# ======================\n",
        "# FORENSIC TRANSFORMS\n",
        "# ======================\n",
        "\n",
        "def midband_bandpass(frames):\n",
        "    \"\"\"Approx mid-band: DoG between small and large blur.\"\"\"\n",
        "    out = []\n",
        "    for f in frames:\n",
        "        f32 = f.astype(np.float32)\n",
        "        blur_small = cv2.GaussianBlur(f32, (5, 5), 0)\n",
        "        blur_large = cv2.GaussianBlur(f32, (21, 21), 0)\n",
        "        mid = blur_small - blur_large\n",
        "        mid = np.clip(mid + 128.0, 0, 255).astype(np.uint8)\n",
        "        out.append(mid)\n",
        "    return out\n",
        "\n",
        "def highfreq_residual(frames):\n",
        "    \"\"\"High-frequency residual (unsharp-like).\"\"\"\n",
        "    out = []\n",
        "    for f in frames:\n",
        "        f32 = f.astype(np.float32)\n",
        "        blur = cv2.GaussianBlur(f32, (9, 9), 0)\n",
        "        hf = f32 - blur\n",
        "        hf = np.clip(hf + 128.0, 0, 255).astype(np.uint8)\n",
        "        out.append(hf)\n",
        "    return out\n",
        "\n",
        "def to_tensor_clip(frames):\n",
        "    \"\"\"\n",
        "    List of uint8 HxWx3 -> (C,T,H,W) float32 with Kinetics normalization.\n",
        "    \"\"\"\n",
        "    clip = np.stack(frames, axis=0).astype(np.float32) / 255.0  # (T,H,W,3)\n",
        "    clip = (clip - mean) / std\n",
        "    clip = clip.astype(np.float32)\n",
        "    clip = np.transpose(clip, (3, 0, 1, 2))  # (C,T,H,W)\n",
        "    return torch.from_numpy(clip)\n",
        "\n",
        "@torch.no_grad()\n",
        "def r2_feature(path, mode=\"raw\"):\n",
        "    frames = sample_frames(path)\n",
        "    if frames is None:\n",
        "        # if something is broken, return zero vector so shapes stay consistent\n",
        "        return np.zeros(512, dtype=np.float32)\n",
        "\n",
        "    if mode == \"raw\":\n",
        "        proc = frames\n",
        "    elif mode == \"mid\":\n",
        "        proc = midband_bandpass(frames)\n",
        "    elif mode == \"high\":\n",
        "        proc = highfreq_residual(frames)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "    clip = to_tensor_clip(proc).unsqueeze(0).to(device)  # (1,C,T,H,W), float32\n",
        "    feat = r2(clip)                                      # (1,512)\n",
        "    feat = feat.squeeze(0).cpu().numpy().astype(np.float32)\n",
        "    return feat  # (512,)\n",
        "\n",
        "def build_features(paths, mode):\n",
        "    feats = []\n",
        "    for p in tqdm(paths, desc=f\"R(2+1)D[{mode}]\"):\n",
        "        f = r2_feature(p, mode=mode)\n",
        "        feats.append(f)\n",
        "    return np.vstack(feats)\n",
        "\n",
        "# ======================\n",
        "# CLASSIFIER HELPERS\n",
        "# ======================\n",
        "\n",
        "def run_svm(name, Xtr, Xte, ytr, yte):\n",
        "    clf = SVC(kernel=\"rbf\", class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, ytr)\n",
        "    yp = clf.predict(Xte)\n",
        "    acc = accuracy_score(yte, yp)\n",
        "    print(f\"\\n=== {name} (SVM) ===\")\n",
        "    print(\"Accuracy:\", round(acc, 3))\n",
        "    print(classification_report(yte, yp, target_names=[\"Real\", \"AI\"], digits=3))\n",
        "    print(\"Pred dist:\", Counter(yp))\n",
        "    return acc\n",
        "\n",
        "def run_knn(name, Xtr, Xte, ytr, yte, k=3):\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    yp = clf.predict(Xte)\n",
        "    acc = accuracy_score(yte, yp)\n",
        "    print(f\"\\n=== {name} (kNN, k={k}) ===\")\n",
        "    print(\"Accuracy:\", round(acc, 3))\n",
        "    print(classification_report(yte, yp, target_names=[\"Real\", \"AI\"], digits=3))\n",
        "    print(\"Pred dist:\", Counter(yp))\n",
        "    return acc\n",
        "\n",
        "# ======================\n",
        "# BUILD FEATURES FOR EACH STREAM\n",
        "# ======================\n",
        "\n",
        "Xtr_raw  = build_features(X_train_paths, \"raw\")\n",
        "Xte_raw  = build_features(X_test_paths,  \"raw\")\n",
        "\n",
        "Xtr_mid  = build_features(X_train_paths, \"mid\")\n",
        "Xte_mid  = build_features(X_test_paths,  \"mid\")\n",
        "\n",
        "Xtr_high = build_features(X_train_paths, \"high\")\n",
        "Xte_high = build_features(X_test_paths,  \"high\")\n",
        "\n",
        "# ======================\n",
        "# PER-STREAM PERFORMANCE\n",
        "# ======================\n",
        "\n",
        "acc_raw  = run_svm(\"R2+1D RAW\",       Xtr_raw,  Xte_raw,  y_train, y_test)\n",
        "acc_mid  = run_svm(\"R2+1D MID-BAND\",  Xtr_mid,  Xte_mid,  y_train, y_test)\n",
        "acc_high = run_svm(\"R2+1D HIGH-FREQ\", Xtr_high, Xte_high, y_train, y_test)\n",
        "\n",
        "# ======================\n",
        "# FUSION: RAW + MID + HIGH\n",
        "# ======================\n",
        "\n",
        "Xtr_fuse = np.concatenate([Xtr_raw, Xtr_mid, Xtr_high], axis=1)\n",
        "Xte_fuse = np.concatenate([Xte_raw, Xte_mid, Xte_high], axis=1)\n",
        "\n",
        "acc_fuse_svm = run_svm(\"FUSION (RAW+MID+HIGH)\", Xtr_fuse, Xte_fuse, y_train, y_test)\n",
        "acc_fuse_knn = run_knn(\"FUSION (RAW+MID+HIGH)\", Xtr_fuse, Xte_fuse, y_train, y_test, k=3)\n",
        "\n",
        "print(\"\\n================ SUMMARY (same split) ================\")\n",
        "print(f\"R2+1D RAW (temporal)            : {acc_raw:.3f}\")\n",
        "print(f\"R2+1D MID-BAND (temporal)       : {acc_mid:.3f}\")\n",
        "print(f\"R2+1D HIGH-FREQ (temporal)      : {acc_high:.3f}\")\n",
        "print(f\"FUSION SVM (RAW+MID+HIGH)       : {acc_fuse_svm:.3f}\")\n",
        "print(f\"FUSION kNN (RAW+MID+HIGH, k=3)  : {acc_fuse_knn:.3f}\")\n",
        "print(\"======================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYx7W-uglnCl",
        "outputId": "67c9535d-b072-4465-e533-c03949bffe6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading UCF101 (real videos) via KaggleHub...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/matthewjansen/ucf101-action-recognition?dataset_version_number=4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.53G/6.53G [01:15<00:00, 92.7MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UCF101 path: /root/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\n",
            "Found 13451 candidate real videos in UCF101.\n",
            "Copying 1200 real videos â†’ /content/drive/MyDrive/ai_video_project_full/real_1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying real: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:22<00:00, 53.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting AI videos from: /content/drive/MyDrive/ai_video_project_test/ai_videos_10\n",
            "Found 10 candidate AI videos.\n",
            "âš ï¸ Only 10 AI videos available, less than requested 1200.\n",
            "    â†’ I will copy all of them. Add more AI data later into this folder and rerun if needed.\n",
            "Copying 10 AI videos â†’ /content/drive/MyDrive/ai_video_project_full/ai_1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying AI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… DONE.\n",
            "Real videos in /content/drive/MyDrive/ai_video_project_full/real_1200: 1200\n",
            "AI videos   in /content/drive/MyDrive/ai_video_project_full/ai_1200: 10\n",
            "\n",
            "You can now train your R(2+1)D / midtone / high-frequency / fusion models on these folders.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kagglehub tqdm\n",
        "\n",
        "import os, random, shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "# Where to save the final 1200+1200 set\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/ai_video_project_full\")\n",
        "REAL_DIR = BASE_DIR / \"real_1200\"\n",
        "AI_DIR   = BASE_DIR / \"ai_1200\"\n",
        "\n",
        "# <-- IMPORTANT:\n",
        "# Point this to your existing AI-generated / deepfake videos.\n",
        "# For example:\n",
        "#   - Your Sora clips folder\n",
        "#   - Or a folder where you've extracted an AI/deepfake dataset from Kaggle/HF\n",
        "AI_SOURCE_DIR = Path(\"/content/drive/MyDrive/ai_video_project_test/ai_videos_10\")\n",
        "# Replace ^ with the real path that contains MANY AI videos.\n",
        "\n",
        "# Target counts\n",
        "N_REAL = 1200\n",
        "N_AI   = 1200\n",
        "\n",
        "# ========== SETUP ==========\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "REAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "AI_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ========== 1) REAL VIDEOS: UCF101 ==========\n",
        "print(\"Downloading UCF101 (real videos) via KaggleHub...\")\n",
        "ucf_path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")\n",
        "print(\"UCF101 path:\", ucf_path)\n",
        "\n",
        "all_real = list(Path(ucf_path).rglob(\"*.avi\"))\n",
        "print(f\"Found {len(all_real)} candidate real videos in UCF101.\")\n",
        "\n",
        "if len(all_real) < N_REAL:\n",
        "    print(f\"âš ï¸ Only {len(all_real)} real videos available, less than requested {N_REAL}. Using all of them.\")\n",
        "    real_sample = all_real\n",
        "else:\n",
        "    random.shuffle(all_real)\n",
        "    real_sample = all_real[:N_REAL]\n",
        "\n",
        "print(f\"Copying {len(real_sample)} real videos â†’ {REAL_DIR}\")\n",
        "for v in tqdm(real_sample, desc=\"Copying real\"):\n",
        "    out = REAL_DIR / v.name\n",
        "    if not out.exists():\n",
        "        shutil.copy(v, out)\n",
        "\n",
        "# ========== 2) AI VIDEOS: FROM YOUR SOURCE DIR ==========\n",
        "print(\"\\nCollecting AI videos from:\", AI_SOURCE_DIR)\n",
        "\n",
        "if not AI_SOURCE_DIR.exists():\n",
        "    raise SystemExit(f\"âŒ AI_SOURCE_DIR does not exist: {AI_SOURCE_DIR}\\n\"\n",
        "                     \"Update AI_SOURCE_DIR to the folder that holds your AI videos.\")\n",
        "\n",
        "ai_exts = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
        "all_ai = [p for p in AI_SOURCE_DIR.rglob(\"*\") if p.suffix.lower() in ai_exts]\n",
        "\n",
        "print(f\"Found {len(all_ai)} candidate AI videos.\")\n",
        "\n",
        "if len(all_ai) < N_AI:\n",
        "    print(f\"âš ï¸ Only {len(all_ai)} AI videos available, less than requested {N_AI}.\")\n",
        "    print(\"    â†’ I will copy all of them. Add more AI data later into this folder and rerun if needed.\")\n",
        "    ai_sample = all_ai\n",
        "else:\n",
        "    random.shuffle(all_ai)\n",
        "    ai_sample = all_ai[:N_AI]\n",
        "\n",
        "print(f\"Copying {len(ai_sample)} AI videos â†’ {AI_DIR}\")\n",
        "for v in tqdm(ai_sample, desc=\"Copying AI\"):\n",
        "    out = AI_DIR / v.name\n",
        "    if not out.exists():\n",
        "        shutil.copy(v, out)\n",
        "\n",
        "# ========== SUMMARY ==========\n",
        "final_real = len(list(REAL_DIR.glob(\"*\")))\n",
        "final_ai   = len(list(AI_DIR.glob(\"*\")))\n",
        "\n",
        "print(\"\\nâœ… DONE.\")\n",
        "print(f\"Real videos in {REAL_DIR}: {final_real}\")\n",
        "print(f\"AI videos   in {AI_DIR}: {final_ai}\")\n",
        "print(\"\\nYou can now train your R(2+1)D / midtone / high-frequency / fusion models on these folders.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002e98318e1f4188873f9148083a441a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b18477b46747d297eb9c09082508dd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_84e25370886c4477a89ef99b71767a4d",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡298.77â€‡examples/s]"
          }
        },
        "00334c0b2c614a9e9c30c4ba52e54dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0060a45aba4e44d18cb406730a7cc0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "012655270da748158cc040c47f973159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03234eabb31d4ceabff8a4e84448817b",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_357fc0a6b3a74ae5b67c0dc97da53f74",
            "value": 500
          }
        },
        "012bfec4c75f4e45917e35934a70793c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "018982629b294bc1b26b9062e3216370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e99d78fceee4785b49de84cb5b81275",
              "IPY_MODEL_5555dc4a35df4672aff58f9f02f273f2",
              "IPY_MODEL_37c1ecc7a57e4578adde0493adc570ad"
            ],
            "layout": "IPY_MODEL_9a95ef09ed0f4563a9952936f844e430"
          }
        },
        "023d4e8adc3a477b9a484b19d5e51171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023f04357113484fa2936430eb888454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03234eabb31d4ceabff8a4e84448817b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035247d2aff746e091c59883732dc1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0379a703b8d1426e9927268728b3f96e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "038af559a4df4dff867cc3eeab6f866a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c0b7b34d604a6a9f5525a0dd48157c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_edea68cdf5d942098f18e162fd996a68",
            "value": "â€‡2.26k/?â€‡[00:00&lt;00:00,â€‡80.5kB/s]"
          }
        },
        "044f21ed61424b1195823013d68d680f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e45fe764c6a4a29bea85ed63048cdee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_499af40f2a724aa6b68f8d11ec01e599",
            "value": 1
          }
        },
        "047913100e204459aa7ffe9d03688d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b6e817c3af4f63a13b9fd12684fe77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ead14343a747e1a24cd7c78a2cfaef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ffc654e12e3403fa364b52125e5a30d",
            "max": 91420126,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa0f1c7af95c473dbd17efa8177edd62",
            "value": 91420126
          }
        },
        "04ff58fbd51e4335bf0fccd7bf778e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b8b976a237c466ba617da7a5e616955",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd121d51834c4adf94fff006c0922817",
            "value": "split_20250821_132430.json:â€‡"
          }
        },
        "050b00824a714e5eadb351e4caf2b607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05354ab1f9254b41b9ac9e26295e83c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0655360d0d60492dbbba1e8df3c5dc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e1a279c1da41e8a81d65c3ca2812b7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e47062a0f9da44cdb8071a7f5c7a1c3c",
            "value": "videos/split_20250821_131935.tar.gz:â€‡100%"
          }
        },
        "066a0c4fce264782acdf10f6f6e3622c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbddcd9e35341abbe25009ea1f1303c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c51d43c099884903867e2b1086fe7ed2",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "072b7b4b4f8e4155a30344a01487ce00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "072cae87b93a4d5c8dc445dd4e3d6e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075261f66e5a41b696a51553262d0d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68208ef15ec64189921227c619f88b76",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a47eaed8224142899e3d195e3b12e99e",
            "value": "videos/split_20250821_191635.tar.gz:â€‡100%"
          }
        },
        "07a2aa1d002e4412b94ef72319a350f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_190e220e7adf4bcf9474c834f35dc54e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f10dc417c644442789840938328a84a4",
            "value": 10
          }
        },
        "085e513ab0664259b706f91d55b33556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57093d32b92c4d7c99f90924ebd94efd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ff4eef111cf84098a02006a002a3e8cd",
            "value": "videos/split_20250821_204955.tar.gz:â€‡100%"
          }
        },
        "08f9e20118164693b1161f56f82a888d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09243d18aef041b3bc1cf05435e67ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0964f6c7be3243618c5ec0282463ced3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d715bc644949f79c78ea3a663a2176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a0bed94338241a8a38122aa263167b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a19f9ed65e44862b193d3a761967cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0e7e707eb8f401d91bb5771036cb83e",
              "IPY_MODEL_52cb77a2def74218af8e4f2de339ccb9",
              "IPY_MODEL_e41caf959306471ebb32bf31c6e665b2"
            ],
            "layout": "IPY_MODEL_f6739cce2d8843a8a48691717abb2277"
          }
        },
        "0a5c7ffac7d84fc0b62b7496362309fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fff43324809462d906600d21b86e7ff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8d65df63bdf43ffbc834cf3d8379487",
            "value": 1
          }
        },
        "0a6f5088348e428594e0c6e0f1439d21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aaf47e231ac4d798d55e1ae6493879e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab80f6bf9cb4f30b6e2614d45107efa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b366897863341bc902aa35e92dfe9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b5ec895ecd547f19fb6bcddfe5c6c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d062b042c92b4a8197ab115bb61d7158",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caa698276a7748eea26bce40b44512c2",
            "value": 10
          }
        },
        "0b6951f1264244eb8147d4e32a4dfe84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c0a809e783e483fbc3387b51b2a5abe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0c6b80f9422b487aa78d272f91285321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652bf8af766c4b3b93a153ffba2da202",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c22b9c86a5454ec88ea803b72ea2f2ae",
            "value": "â€‡157M/157Mâ€‡[00:03&lt;00:00,â€‡54.0MB/s]"
          }
        },
        "0ca6cbb4a862470bba7a7d3408f5f684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3f0ca91814b4b4aa466a88a084b0e1a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a14a469ffd9f4fce9c2b10aabba793bb",
            "value": "â€‡180M/180Mâ€‡[00:05&lt;00:00,â€‡42.5MB/s]"
          }
        },
        "0cbf4452135f4c1db96f27ed051b7346": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d9cccefa60c449689e9cabd04857fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2994e2991c44db48cf94852b22b1142",
              "IPY_MODEL_0e77ea7d5ec7499a9c4d007aa3b6efa9",
              "IPY_MODEL_90ce8df4723b4a0f9f3b68ab92beef2f"
            ],
            "layout": "IPY_MODEL_7e067139bacc4f05a7573610abe098f3"
          }
        },
        "0da1fbd111e14c95ad09d0d062d9e0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd2e9fbbd5a45cb919cb53b63968f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e19b2fdbb14419bb9a7e5cb80e85cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83abbbe2fc874dc1985edf756c4364c2",
              "IPY_MODEL_dbcc0c23d94742a88c00553261dbfd93",
              "IPY_MODEL_dcfff566048840c08210805e690c8b0c"
            ],
            "layout": "IPY_MODEL_93af1c5299534a788a16ef462bb80427"
          }
        },
        "0e48cbe8df5a41b9a0a4142f34859db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e77ea7d5ec7499a9c4d007aa3b6efa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb1b45c3e48c46aa92844961abd15913",
            "max": 178120729,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adca45cd41014108b0d1c2585d6dbfea",
            "value": 178120729
          }
        },
        "0e90ca79a4c6466bafb46c2850cf4ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fd196ad3cc64212a310a52ba497d5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fff43324809462d906600d21b86e7ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "101ac4c7ccc64a22ac2b9459240087cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4810245d716e4ebb99cbbd95dbe28f5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_564a36ad5dea4535b737feea5e3dad18",
            "value": "â€‡500/500â€‡[00:00&lt;00:00,â€‡1830.18â€‡examples/s]"
          }
        },
        "104801c45ec047019388b3f18f243b40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108dc838b7e54caca61e8b1907bb2ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10c25c9a4a14445d91ad0a89ec850b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c88fdbc5af5049d290b25ac9956c8016",
              "IPY_MODEL_25a818a66ec14710a2d0a8a41bf712a4",
              "IPY_MODEL_56c25f6e4c8b419ea020fd0dae681a09"
            ],
            "layout": "IPY_MODEL_2d54054a61b246e9b7be96a1d90ea764"
          }
        },
        "10d25175e79f4f3b8bf3d90ac8b883ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1137435065b240929e551ac436a614e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dec69aa4fd654d5bb23b5f6229a7f357",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3b838e2e4a04977be608f22b6b858db",
            "value": 1
          }
        },
        "11fd5a389843487082f83a63327302bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d4c195c59ee48cfb62cd153bff752d8",
              "IPY_MODEL_d856e6038687496f83319882f62cc188",
              "IPY_MODEL_87e2326b64e04116bde463995c2ade1b"
            ],
            "layout": "IPY_MODEL_74117064d79f406d9bd669a49b826a11"
          }
        },
        "121deeca9bb546d28224240a3d1a6dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12cb178f70fe43108dddfcf88bcd1329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13104e2205654db6bef189fc2314d20d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13524c0c8e9d44ff96e7ffe6b7e05c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13babc9a8a2242519f660d9c9b232f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1412c248a50b4aba833992cad365eff8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "155492956989422a97a34d9a3371a7f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168057af343348cbb415c9730da44bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d17455c28049ae8a00661e4cb400f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7001087bf05439182343a1927ca553a",
              "IPY_MODEL_93aebab0e618451da2d89b13722cc4f9",
              "IPY_MODEL_721383979a7c45399264d6439a6b3639"
            ],
            "layout": "IPY_MODEL_b4766cdd10724ac3826f4caf25c40099"
          }
        },
        "1759c96001a94b0c9b2c704763e25763": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1773390e287247c7a11bba965974b737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe2911ab94654deab8eefa851a84d992",
              "IPY_MODEL_5396cb9938d946d6a3e911c0dcf1c38c",
              "IPY_MODEL_aaecebc7516f4767a096969e33879d18"
            ],
            "layout": "IPY_MODEL_26199fbc942441da9324ad112cca4d34"
          }
        },
        "17846ac1d87e4acb9db044dd96197345": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2dd711adab9463bbdea128a4439a138",
            "max": 143663378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df3bb6c7935d430b9f3f3b29144dd391",
            "value": 143663378
          }
        },
        "17b080ff18024e25b0e22d8f1522dbdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17b9dbe38e1741308ffa09dba84a7861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6241aedf9e10400ca80a6028829f0fd3",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71bb93edc2ae4a0aa239175773e40047",
            "value": 10
          }
        },
        "18148b5b771e469dadf1d1e77371606a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a59635b5aa2741418880f7f7bad1d110",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_486570f741134bf9a951185baee02393",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "186859350d8a4db3b194d4633d795506": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "188f9b7af1044717b0de58efc8567e00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190e220e7adf4bcf9474c834f35dc54e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d8e43fbd8041f3967bf8fbb1211a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a03a4d9af0240d299a2864c94821a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a338f9a15ad43ccbce0cf5aab1704f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af2e45500f14a8087dc014df39e2fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d8e36904e174a22ba66ca4cdfc3471e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b1378620bbb418bb05a1a1f9a607b47",
            "value": "videos/split_20250821_142935.tar.gz:â€‡100%"
          }
        },
        "1b2293f9dbdc4ff7bdf9ce757b4e41b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b50620b43c54d79aefb14de592936d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1d1ce762b0f24629856d91b148503e73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d348d48850245c09de3e3787dbf13c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b9e9ef86ed948ea83f520730eadf1e4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27b26df4cd514533831a95a8699e6b81",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "1d9d112daa4a4ec58f3980518518f394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ddf8fccdcd0489eaeb661467b3c4425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_402a99abc7e54df686e48301bf0b502c",
              "IPY_MODEL_1137435065b240929e551ac436a614e5",
              "IPY_MODEL_564041ced0c44d69bc35e8f9a6346e3d"
            ],
            "layout": "IPY_MODEL_82359936ea6a409e9ab645b92ea23853"
          }
        },
        "1e99d78fceee4785b49de84cb5b81275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beac7da3b3f2441590daefb331289256",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82daef89f74442f993380d83196be2f0",
            "value": "videos/split_20250821_194003.tar.gz:â€‡100%"
          }
        },
        "20311e97bb3b45e6ae899826074bcfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20fe5430471d4eb39a4e0d03530e0079": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2311373a86c645cc8da4e90bde6f295e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2473253d855d4bff885e5994f7dc11b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "254053cdb8a042df91004671bd08a4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "256bbb1035ea4d29970a47d55c1d6cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2593a1f747d8427c8cdfa489e2a87b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25a818a66ec14710a2d0a8a41bf712a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb46b70322b40f9a3808edc859a334a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb601496f3684f138e379a7ef0f159e6",
            "value": 1
          }
        },
        "25d9169aef0d4395b26e8ebc5d526fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26199fbc942441da9324ad112cca4d34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2695419ccb9f41c997972ca73ee2ff16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273b09e7cc2b416eb5db565d501fa805": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "27b26df4cd514533831a95a8699e6b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27f7d5a291cb4cca8b2a600673c401d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28a3744d84b04db6829425dbf791aa3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294543597d8044849d23477fa9138178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2a15cadc1c9b4eeb9b40da6925b3fa45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f69ec6544e9046f898a943d0ac6a0ca6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b6c5d635f7174548b7a28264c2bb2ba9",
            "value": "â€‡144M/144Mâ€‡[00:03&lt;00:00,â€‡55.7MB/s]"
          }
        },
        "2a57c6e41e24432cb7729f992320a8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa9e9cc4b93f4a4d9f0af0272dc79b70",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dde1173b23e440c0ab7087d8f58bf261",
            "value": "â€‡114k/?â€‡[00:00&lt;00:00,â€‡6.65MB/s]"
          }
        },
        "2a6501c92294442ea4462114695906c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab99f52c5bb4af6ad473104994f0f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2aee65d7bf304ee8ac88eb1550217c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f752b8d2e55d41a4910541ea6803879c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e69857e738b24aa5ae36559fea20504b",
            "value": "videos/split_20250821_202505.tar.gz:â€‡100%"
          }
        },
        "2b0d207e58204933afe05f227a86fd04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b2a80e57ecc4f268c11508e61ca5808": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bb03acb2e244e94975548accc82d063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_023d4e8adc3a477b9a484b19d5e51171",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3092fd091ad54160942f421d13ae3721",
            "value": "videos/split_20250821_175929.tar.gz:â€‡100%"
          }
        },
        "2bd7126ba7b04474966736cfc88717fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf0e5c1bff84b08900159f20a47e504": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1f53b43ffa44b492ccac077d69f74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c58e331be3142f2a9ff6b8dbb4775d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8bd145eea248988deefd97df1f73ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc50b3e3c7a4e0297d8c6d1bee75e54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4c195c59ee48cfb62cd153bff752d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0ea45f8c7949cbb567676fac340962",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4b002cd733a649ba941aa29d1d584b8b",
            "value": "videos/split_20250821_195516.tar.gz:â€‡100%"
          }
        },
        "2d54054a61b246e9b7be96a1d90ea764": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da35df8f2864db4afff409abdec5e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186859350d8a4db3b194d4633d795506",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d814cd564d94b389f1e12018fc08f55",
            "value": 1
          }
        },
        "2e2c3bb30fd348d890c28e35a5b3342a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e45fe764c6a4a29bea85ed63048cdee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2e52ef4addf3479d96baabe991237861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_519a89157ae34414b23ba151c3be0d0b",
              "IPY_MODEL_c6bd9b2c59884fb79c33a9eb08fefe05",
              "IPY_MODEL_45035a0f58ab45cabc2729a38d8d7933"
            ],
            "layout": "IPY_MODEL_dc1edb786a4b43fcbd8963ffd9c95757"
          }
        },
        "2e95e76997e3429193ce410e3cbea01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c5f244f24eb466da7cf4fd3a126b484",
              "IPY_MODEL_b890fbb77e024761b917c79dc6047b86",
              "IPY_MODEL_93650f3e4d864114b989179929250948"
            ],
            "layout": "IPY_MODEL_5c6a04a1dce34f4da5f2d533e78c3002"
          }
        },
        "2eab081484424f40b03a241b68d70584": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2f431d07e04c4de98c19ce92553defd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f4874cc43aa478d80fc3d5cd999c0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f74dbfbaefb425f887a7c322577f844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fccd67badc142c3931d23ff28646a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09243d18aef041b3bc1cf05435e67ab4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36bec5b206644178a28705016f57006c",
            "value": 1
          }
        },
        "2ff895dfacbd4f1d87b0681b6dd545b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303be2e8dcee4c97a2167652c4137529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "303d650c7fc64aee8f0660e60b0b53a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3dfb245de7f43baa44d9ff261712f86",
            "max": 7406999714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f74dbfbaefb425f887a7c322577f844",
            "value": 7406999714
          }
        },
        "304112ad685a45f5a29ff23e870d5def": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3092fd091ad54160942f421d13ae3721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30e60f278b20411eb481c38300d258f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3131227f6fb84ff1b706c6f012a9b619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5722b0115fce4f03ab8da0a0a4ddac04",
              "IPY_MODEL_b4fb41df76ba47a3a5a035252b3fc965",
              "IPY_MODEL_e9e75f26184e4b69bdfc11e7627a90d9"
            ],
            "layout": "IPY_MODEL_00334c0b2c614a9e9c30c4ba52e54dcd"
          }
        },
        "31904a1d1cee42328afe1a25706a7667": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3198cbacd8a9400680ee4a9e138b9721": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d75d2b530d4a0483133aa0c679c596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab65b4ad51bc47249d842912d024d2f3",
              "IPY_MODEL_a471b8d5bf0b4b92b9b45239910ee70f",
              "IPY_MODEL_7e5b2669a4fa4a7eab17d6db8101522a"
            ],
            "layout": "IPY_MODEL_af571e291ba9431cabf40c0fe8b5c91e"
          }
        },
        "3210c647eca74de7b36817b751c5c28a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32bda37c20024511a5042bb4c2a48b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847b5cf923bc40f1a4d35a0852915f22",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_25d9169aef0d4395b26e8ebc5d526fd5",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "3328ced3b825465798aefb3475c031d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3435c25b254742a7a82d9de5f17988d8",
              "IPY_MODEL_36364a311b354120a02e2b841b6f2858",
              "IPY_MODEL_2a57c6e41e24432cb7729f992320a8fa"
            ],
            "layout": "IPY_MODEL_1412c248a50b4aba833992cad365eff8"
          }
        },
        "335af0924d014d40a056ee195fb21612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33785dea9e2142078e20af4cb7430256": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3435c25b254742a7a82d9de5f17988d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c00b95fe9f436d8b390d9a31962f5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_77e3d0726193424a808e3fa494307a6d",
            "value": "split_20250822_142242.json:â€‡"
          }
        },
        "349a2cd6777940c5806354c6cefa1f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f8c93897044bad874a79494d296647",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d0817013e5049ab8eac2e46f52766ac",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "3521cf83dc6140d48cb085eb5a877b81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357fc0a6b3a74ae5b67c0dc97da53f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35dc3cf6fe5348b3bcff011fd269547a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ef6876004e47fbb0d43cc8306000e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36364a311b354120a02e2b841b6f2858": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_294543597d8044849d23477fa9138178",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2d5d97dbbaa4a5bb1a65ec42ff18c0f",
            "value": 1
          }
        },
        "36bec5b206644178a28705016f57006c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37c1ecc7a57e4578adde0493adc570ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eff6e3c4dc694ca6bc4366b6fe234fb8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c7bcd64db42f44b5813024715e3a954f",
            "value": "â€‡146M/146Mâ€‡[00:02&lt;00:00,â€‡32.0MB/s]"
          }
        },
        "37e25cbbd7604619a5b588cebf2efd80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e54b1fbf2443c7a64442ab02da654a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39608cd9847442f3a5726bf48635ce21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3988f39e200242f9b6a1bc2595295c01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d766177faf4f68bedee1bdf9b7d5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e1a279c1da41e8a81d65c3ca2812b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3417e229664d50b791accd5c788a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a384672df804e2eb302f2c262ac1b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c07d6b29554e64ad5ac43bb51eefda",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a01012fc829d430f96f001834b45ceef",
            "value": "videos/split_20250823_072111.tar.gz:â€‡100%"
          }
        },
        "3b1378620bbb418bb05a1a1f9a607b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b692e76478e4d11842900add1448c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a24e54bfe14d50bd293eebb5299b9f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_335af0924d014d40a056ee195fb21612",
            "value": "videos/split_20250821_155434.tar.gz:â€‡100%"
          }
        },
        "3beb70d3bbe842739774339225d2a42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e012111d634802b9c654cb09aab716",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_93fa188741784837be98c7cb79e8b377",
            "value": "â€‡2.25k/?â€‡[00:00&lt;00:00,â€‡148kB/s]"
          }
        },
        "3c4766483ab74a3baa08eba6ac03d65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c998bacc1d245d0b2298a52404d2a64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cfaf1ebcae745fbad379ec1e9adbbf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d065227a70d4903b3073bf8eb68af58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_655d3390fb0d47baaed9ee33fd13c74c",
              "IPY_MODEL_83045cbb91e04c69bb6a8345fc617590",
              "IPY_MODEL_43a4309129bd4b2fa6785745183b2bb8"
            ],
            "layout": "IPY_MODEL_0ab80f6bf9cb4f30b6e2614d45107efa"
          }
        },
        "3d07736d18f74795b45e3c86fb32fcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd7126ba7b04474966736cfc88717fc",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7467e9bde9494565bd053f153e4bbdf6",
            "value": 10
          }
        },
        "3d0817013e5049ab8eac2e46f52766ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d52615582bb4accbaccf10b62388699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e3b6a79621c48d48167036b3f7b42b9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6ade4aff31f54202b82204a185a6627f",
            "value": "â€‡2.28k/?â€‡[00:00&lt;00:00,â€‡78.8kB/s]"
          }
        },
        "3e8fd5194efe4d4f975e261cdb397341": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3eca679cd1924d6fbd7292698fc76279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef9643f5c564deab9ac6b549bc86dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b14b799c11043078a7f110920e6f32f",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ffe980df3ce46afbbb9b3eb612d2787",
            "value": 10
          }
        },
        "3f70355aa35a4067ae35cd6d3f272dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8efe9366ea47e1a16eba3dfded0337": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e8fd5194efe4d4f975e261cdb397341",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_721455084e1b437a865bcd17d99244bf",
            "value": 1
          }
        },
        "3fb46b70322b40f9a3808edc859a334a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "400408cf4b064cff8f114132d923cc7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "402a99abc7e54df686e48301bf0b502c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb163f12c39432683915cd4f29f8c80",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e9977e277b664d8dbd5f51210ec43f0b",
            "value": "split_20250821_150431.json:â€‡"
          }
        },
        "40a5a19395ab4371a429438892caab54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b50e03915142c2b68f9ddc6d12ad21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd4746cb35a4e1fa3f1ed0d896ba415",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b50f14fa80b3428f9951007d60c9c41d",
            "value": 1
          }
        },
        "40c0b7b34d604a6a9f5525a0dd48157c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d5144411c74c44ac65999ddc12ce63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4147e4bf3a304099ac0bb6d6ecdb4479": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_066a0c4fce264782acdf10f6f6e3622c",
              "IPY_MODEL_07a2aa1d002e4412b94ef72319a350f9",
              "IPY_MODEL_a6106767a2eb40ab8ddcc6be7941a93f"
            ],
            "layout": "IPY_MODEL_92b9e4ff38b343aeb50c5f50066aa267"
          }
        },
        "418b357b28e047dca04f8f17f3fc375c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41f87dec136b4891bd00d4bafe09e4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04ff58fbd51e4335bf0fccd7bf778e03",
              "IPY_MODEL_d3bcdc103bb646678383751221c6dfde",
              "IPY_MODEL_3beb70d3bbe842739774339225d2a42c"
            ],
            "layout": "IPY_MODEL_4404c8895d9240a288f0a4c316ee83a6"
          }
        },
        "42c65d5520884173b3dd3fe0eaa68a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff895dfacbd4f1d87b0681b6dd545b7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e0a9476cad04eca953789470f7f2ac0",
            "value": "â€‡163M/163Mâ€‡[00:03&lt;00:00,â€‡55.3MB/s]"
          }
        },
        "4321ea6fb103495ea155e50adb4e79a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "439179fe810649a8b55a0859fb38dbea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "439592cf8575406c9ac60c6385f17224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "439d2860d0bb487fb9ce58f41b8ef819": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bec8233b54db429399b930d408fe8804",
              "IPY_MODEL_80e5368351ad47dbb6629cd584e05492",
              "IPY_MODEL_89ca8fb1b43e43fca2d1261b976e0b85"
            ],
            "layout": "IPY_MODEL_0379a703b8d1426e9927268728b3f96e"
          }
        },
        "43a4309129bd4b2fa6785745183b2bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5047f7a0fe9409cb40abe6ef3404ffe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0964f6c7be3243618c5ec0282463ced3",
            "value": "â€‡125M/125Mâ€‡[00:03&lt;00:00,â€‡22.7MB/s]"
          }
        },
        "43a94077573a4ba090c8f2af73215d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78e2ce18e4c436991ada9b4f790ce58",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_98044f2a74a145f6b74a1b9f35863a51",
            "value": "videos/split_20250821_200502.tar.gz:â€‡100%"
          }
        },
        "4404c8895d9240a288f0a4c316ee83a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "447280b5d3684b32bfc66e6591e0ca95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "44c469928a1c4311975e23f8ba0a6775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45035a0f58ab45cabc2729a38d8d7933": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_462087b584f742da84800f42f72638fc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13babc9a8a2242519f660d9c9b232f11",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡137.79â€‡examples/s]"
          }
        },
        "45148cf2f5ea4a708125fdc409395950": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_418b357b28e047dca04f8f17f3fc375c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d96cc789a364484cacbf109a98238709",
            "value": "â€‡108M/108Mâ€‡[00:03&lt;00:00,â€‡36.4MB/s]"
          }
        },
        "45cfca5b10b945f4a1ac0bb975aebc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee6476343494893bee2a375c24f2040",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_985d7cba227047b9845aad66b3d33e19",
            "value": "videos/split_20250821_173006.tar.gz:â€‡100%"
          }
        },
        "4605a8c9aef649b894a57dac614163f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "462087b584f742da84800f42f72638fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46286a88b8ca4cbebe99594e05163c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472e685d98b74fb68274eb2c893b386d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4810245d716e4ebb99cbbd95dbe28f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "481772e766534b7f962e6f476e230ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "483611d1fd7d4680b415d0b140ac2e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "486570f741134bf9a951185baee02393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48bd381f73554708a85608ab8333e639": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc50b3e3c7a4e0297d8c6d1bee75e54",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3de5bac352a4ef1b393d8bd54f22497",
            "value": "â€‡5.80G/5.80Gâ€‡[01:03&lt;00:00,â€‡74.4MB/s]"
          }
        },
        "48f598586ab94ec49003ba4e92bc8481": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490fa60f449d4a53a7daa8faf04cdeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d84b4bf0fdbd44878afd843ab639cfd2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ac2952f5d24245789649586fda6d3285",
            "value": "videos/split_20250821_125931.tar.gz:â€‡100%"
          }
        },
        "499af40f2a724aa6b68f8d11ec01e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a31ef1c2129427a807a1b5639f8eaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8734466a8c4ed6a1ba3afbbbb68ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a9359d785ec48a8b9e6673531fd49af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab56d3720a5410b87e4a0e71e698522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d348d48850245c09de3e3787dbf13c4",
              "IPY_MODEL_921389b39fbc4e3b97956f41b6199fe6",
              "IPY_MODEL_8fefddf659334043be219829f9cdd582"
            ],
            "layout": "IPY_MODEL_3cfaf1ebcae745fbad379ec1e9adbbf8"
          }
        },
        "4aff4aeaa89c4d8580f25d7898cf242f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b002cd733a649ba941aa29d1d584b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b0a1043763045269b43b29552d7a71c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b65fe6a5c184634b2e67d35af7c78db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d41ca7bf21d74aedbcae34a69b7d0bdd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fd4e22dd29154990bd22fc05936b6471",
            "value": "â€‡500/500â€‡[00:00&lt;00:00,â€‡7112.58â€‡examples/s]"
          }
        },
        "4b682354338a42c69ae71167fee89580": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e447c0b7a045808bff215b13e87a38",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdc3ceda0c354b2c92bc7b57d21ba996",
            "value": 1
          }
        },
        "4c39c23af87e4daebb8779671dbb67fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddae853cd6c142eb9a5632c5af9362e6",
            "max": 112539147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9e7fe53b70444f58c2542597dca0526",
            "value": 112539147
          }
        },
        "4d06949235904c56add415de70e827bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d8e36904e174a22ba66ca4cdfc3471e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e755447389744658b4f44c518069eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e781128c74641b4b55bb4d37f1cb347": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4e7fbcc3469f48e1a9ebc73f7aee4f33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee8d9603a8e4118a2888ca4bad6889f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef20d2158fa427dad7227469a690099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f9b90fc3132458b8253dfe59c622d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51368a1bdc574e5b91d4a64446851127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5178cd7c967b40f6aa06b326ff65534b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5192a52a52214369b17cc706dbc9f94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1759c96001a94b0c9b2c704763e25763",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0e90ca79a4c6466bafb46c2850cf4ed0",
            "value": "â€‡147M/147Mâ€‡[00:03&lt;00:00,â€‡50.3MB/s]"
          }
        },
        "519a89157ae34414b23ba151c3be0d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf4047907c44fdaab9283e6f191a8ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c959cff98944fcba99f3a720fadc656",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "51b3e90661da40cba555b6aba7519ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c4c765782247a3bfc1e5ef2df604a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51c6a4a4ab9a4500b07c8019f86d29e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee29d82a1fd04d24961b8a1ca95b82ba",
            "max": 233418440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b2a80e57ecc4f268c11508e61ca5808",
            "value": 233418440
          }
        },
        "5207724128d44f60bced38302e130961": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c749f822062b47b1a5f44c5b1bf80a5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9751fb094c5f43dbb55181033dc95b6c",
            "value": "â€‡2.28k/?â€‡[00:00&lt;00:00,â€‡191kB/s]"
          }
        },
        "527a701d969647558797f6f1b10766aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ffb58735ed5432c9df05fe5646b1e2e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10d25175e79f4f3b8bf3d90ac8b883ad",
            "value": 10
          }
        },
        "527f5e3d0bb44bd4962e76b1cc643fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52cb77a2def74218af8e4f2de339ccb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_447280b5d3684b32bfc66e6591e0ca95",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a8734466a8c4ed6a1ba3afbbbb68ff3",
            "value": 1
          }
        },
        "52d0960525114c0391be1eae1fee89a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6d24c4617174a1784e2226d86489949",
              "IPY_MODEL_a8403440ef3c42e19e7b7feb11eedd2d",
              "IPY_MODEL_c55c9b6114974f0a8187f30b938d247c"
            ],
            "layout": "IPY_MODEL_4aff4aeaa89c4d8580f25d7898cf242f"
          }
        },
        "52ecbab86efb4ba0b911850fa1f68526": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b26c3e2efd47ff8f022f096ac1adee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5d97c216e99847ee8d2f9cc35531ec19",
            "value": "split_20250821_194003.json:â€‡"
          }
        },
        "5396cb9938d946d6a3e911c0dcf1c38c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3706210875742a392e5507e973dd029",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_472e685d98b74fb68274eb2c893b386d",
            "value": 1
          }
        },
        "549a8b0d237b49e980984ee1a09a9f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d766177faf4f68bedee1bdf9b7d5d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ee6ff0aec2f490781066c0cab49f16b",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "54fe1627eec94605bde27ded2f171fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d609d45438534d4198916869bd32a700",
              "IPY_MODEL_012655270da748158cc040c47f973159",
              "IPY_MODEL_4b65fe6a5c184634b2e67d35af7c78db"
            ],
            "layout": "IPY_MODEL_104801c45ec047019388b3f18f243b40"
          }
        },
        "55479a07b48b48f1ac5d8c736fd051b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5555dc4a35df4672aff58f9f02f273f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20311e97bb3b45e6ae899826074bcfa4",
            "max": 145968751,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6ed5d8fb3df4fd99392f3200f27b4e2",
            "value": 145968751
          }
        },
        "557beb0d27474172905c15595ab28910": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb223591ba7241a497ba356a74d44690",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db48e8e9044847f6b5ce902584783d22",
            "value": "â€‡113M/113Mâ€‡[00:02&lt;00:00,â€‡44.6MB/s]"
          }
        },
        "5592bcbe6c614971a65754d9bd371fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "564041ced0c44d69bc35e8f9a6346e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb5e6891fa44f63a2dfe5cd5c3bdf67",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a92da005cc684a1baa963293d303f37f",
            "value": "â€‡2.28k/?â€‡[00:00&lt;00:00,â€‡162kB/s]"
          }
        },
        "564a36ad5dea4535b737feea5e3dad18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "565431068186428fb96dc61d6722f3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56c25f6e4c8b419ea020fd0dae681a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0438b53dd5742198bcc4f31990f5928",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b71cffd7f3214094b467350636f30c20",
            "value": "â€‡2.29k/?â€‡[00:00&lt;00:00,â€‡167kB/s]"
          }
        },
        "56e0a91637c9422099f67e7a91d03f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e382d1943d4990891314c98480663a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dc892b851f441d4a0d54df3d9ca20fd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_add484f9e2ce4ae2854898b88863e93b",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡225.33â€‡examples/s]"
          }
        },
        "57093d32b92c4d7c99f90924ebd94efd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5722b0115fce4f03ab8da0a0a4ddac04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a381e784e97846309d90b49e7aa17812",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_35dc3cf6fe5348b3bcff011fd269547a",
            "value": "split_20250821_200502.json:â€‡"
          }
        },
        "572a4b3599c441cd9376fc4b1bbb01f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0655360d0d60492dbbba1e8df3c5dc9d",
              "IPY_MODEL_8342d04350b14f10aea8baeabffef4dd",
              "IPY_MODEL_7c198bc26b0b4167b1cfd7bccb5e6070"
            ],
            "layout": "IPY_MODEL_0da1fbd111e14c95ad09d0d062d9e0f1"
          }
        },
        "578dbfa43d834111a89b659b6c26aa72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584ef6902c3e49458f498845787ab292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad09ccbe47224a1cb785c16e7be0d1f3",
              "IPY_MODEL_7ecdd079ffc64672b7c5b7332167e8f8",
              "IPY_MODEL_0ca6cbb4a862470bba7a7d3408f5f684"
            ],
            "layout": "IPY_MODEL_c13dd9d3ff264c3fa3c0f553c9d5f695"
          }
        },
        "58b145be5b5348a680012261428c8720": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e67342a7bb461c835888998349d662": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591cabc6e4c34d36a9870d5713f1b8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59cb9bf6ea074f74b22bf88648245013": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8bd145eea248988deefd97df1f73ed",
            "max": 101682388,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90bdb4d60ab846b89b81f4ca6441eb8c",
            "value": 101682388
          }
        },
        "5a7af02bc5b04afe840bf09e5e85d66b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7d163c2d364f6aa2b9bdc7f36ea946": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb163f12c39432683915cd4f29f8c80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bdee5065fda467585a2017d9ebb240b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c155b47fb3348c4b349185b33e6027b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c21dfbf1ae44041a348dc9176c6014e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46de432861941ba8fef64f8b10677c8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_da27c6d4c412440eb9816d73b5cda70f",
            "value": "â€‡2.26k/?â€‡[00:00&lt;00:00,â€‡150kB/s]"
          }
        },
        "5c6a04a1dce34f4da5f2d533e78c3002": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c959cff98944fcba99f3a720fadc656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cc7e66bddf54993b60ee4e6814d14cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85bb7c017edc43f2837999e17492bd1c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e15c010dd8154598aaa58014c22b015c",
            "value": 10
          }
        },
        "5d97c216e99847ee8d2f9cc35531ec19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dfd29d223304e81875b144a88fae11e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0ab1cf49504ad2afb0970d2b1f13c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e2d8f90e361410f91431e7013a6f874": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3b6a79621c48d48167036b3f7b42b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4e91713bb74892be7320691ded1773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ec8dddb4b89459e970d64ff4e72a657": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee304a35b0640f6a746def6ad48ae89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee6476343494893bee2a375c24f2040": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee6ff0aec2f490781066c0cab49f16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f2949b9ad064d09bac9b390ab5fa7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a342890e08674977b6a61706d45f055e",
              "IPY_MODEL_2da35df8f2864db4afff409abdec5e9b",
              "IPY_MODEL_6858abc8c4ff41c7acfef3e13ff2648c"
            ],
            "layout": "IPY_MODEL_c0c49ae9f76f446c86bf3502479511c2"
          }
        },
        "5f2b7466b714466282c0f90915215c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f427f67545a49989a53f70f10cf51b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbc5b5bcbf3b4a43a5ee855d48e1903c",
              "IPY_MODEL_3f8efe9366ea47e1a16eba3dfded0337",
              "IPY_MODEL_5c21dfbf1ae44041a348dc9176c6014e"
            ],
            "layout": "IPY_MODEL_69ba7d7f60af4f27803adac0dc73b8b7"
          }
        },
        "5fde4ebd394d46a4a2e5bc1a1e77ce18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "601829b443aa4b3ca7d7f93001e5d9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "607bb6a94ca14c04bd7f9c6b2ba15f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "608d8dcb31254dd1bdea275742c746dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52ecbab86efb4ba0b911850fa1f68526",
              "IPY_MODEL_cdd90ca6ef0d4b9aae3951620112291f",
              "IPY_MODEL_ee053b038dbc44bbaa0776345ea7e437"
            ],
            "layout": "IPY_MODEL_a0efe76cdc234cf29cb823aebe7b438c"
          }
        },
        "60caafd857354c7681ae35c933ff130a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7ca470386ed4b07b739d7b4172edb69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13524c0c8e9d44ff96e7ffe6b7e05c64",
            "value": 1
          }
        },
        "61069e89dada4e3399f8e30b365b4877": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a6f5088348e428594e0c6e0f1439d21",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e59a905deeee4b12bafe99a934e214e8",
            "value": "â€‡232M/232Mâ€‡[00:45&lt;00:00,â€‡3.65MB/s]"
          }
        },
        "61127971a60d4c9d963f33e3bd984153": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6168a184cc8a4771812f0e3b4ed08687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8c0134c0ac4d568e6215c71f485312",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6bc1ec17defa431cb1be6b2857b21cc1",
            "value": "videos/split_20250821_132430.tar.gz:â€‡100%"
          }
        },
        "61a24e54bfe14d50bd293eebb5299b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623b36d1793f417e97a35d02ab01fe3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6241aedf9e10400ca80a6028829f0fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6364585109b6411eb17b805e2ec3b369": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68ee46368de4b83abd5f087e5089697",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f584ce5a43bf4bfca768fc0d43597994",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡161.70â€‡examples/s]"
          }
        },
        "64b18477b46747d297eb9c09082508dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c44599ab9a4ebda6a270f5f3d938be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715b1990d3814ad29902ef7feff8c5ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e878f9f97ad452b9e0a6fa6ada7cf3c",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡190.18â€‡examples/s]"
          }
        },
        "652bf8af766c4b3b93a153ffba2da202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65535083ec1743d7a581fceada8026e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "655d3390fb0d47baaed9ee33fd13c74c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9a05907526a42eea3d00555d5765418",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_69d30e62a11544c5869cd0966ed709e0",
            "value": "videos/split_20250821_182033.tar.gz:â€‡100%"
          }
        },
        "65a4d32d4f5f4171b4baffa9fe042e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ed5f0792a0e40d996dae340ecf21f46",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3f70355aa35a4067ae35cd6d3f272dc8",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡130.77â€‡examples/s]"
          }
        },
        "65bfc9e3caec48468940d297b1b32163": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6603aa49e7944973888371d3962b57ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665ea9af71db414e80705c4fe387bfaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669a2772073a4288ad1ce136568f0e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66e29d4978d247dd8fee3b26ddcadfaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665ea9af71db414e80705c4fe387bfaa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1f5d8be639d49978e839265c04be031",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡109kB/s]"
          }
        },
        "66f31e15a37e45778ac20eef49de6b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9258682f4c44440eb3fa1f33df2655e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3a3417e229664d50b791accd5c788a63",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "672c6d9e827c41f1a0b884e2d5160ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecd475bdf2c24a23bd926ae53be192d0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b4c7fe646741437cba9292e921db8c59",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡320.39â€‡examples/s]"
          }
        },
        "674d2320910d4b8cb54085f8229e77c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae958630f99647df817229d15ae09239",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2b0d207e58204933afe05f227a86fd04",
            "value": "â€‡2.25k/?â€‡[00:00&lt;00:00,â€‡172kB/s]"
          }
        },
        "68208ef15ec64189921227c619f88b76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6858abc8c4ff41c7acfef3e13ff2648c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b5f548e91946f083ab50b5267dddd2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c42946b231f9451eaaa09c4002647076",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡160kB/s]"
          }
        },
        "6870473e7db44da7a950b2b2a1b11de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "693716292b7e4f2ca76fb2efe0ca8955": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696c607a088e4d8a924dd7e7a9c808f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c22317f7712b4efaac0daadf68535c72",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ab99f52c5bb4af6ad473104994f0f80",
            "value": "split_20250821_134435.json:â€‡"
          }
        },
        "69ba7d7f60af4f27803adac0dc73b8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d30e62a11544c5869cd0966ed709e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ad9203774a74a7ea61cf32d9621d58a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ade4aff31f54202b82204a185a6627f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b9e9ef86ed948ea83f520730eadf1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc1ec17defa431cb1be6b2857b21cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bdf888eec664801adc41955f5a1be23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bffc27dfb294a94b9902b84338fc4ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c346f76a51a4becbf2ddfcf6160e599": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d689903d59f4768abe7680480f75c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dc892b851f441d4a0d54df3d9ca20fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de60d8e82144c8a8a9767ab53f4f689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e0a9476cad04eca953789470f7f2ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e878f9f97ad452b9e0a6fa6ada7cf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ebe014bbf704c31b2aa1cd38b4eb39a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec18a56e9a24b53b6b797f81545e554": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ecae4e1107d41ca9a2587ff1f060db8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed5f0792a0e40d996dae340ecf21f46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8fc573535f457ab04d81402a8a57a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fedca28a9bd441ca9c68265fef30ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e719034506984aee8a365a98dc55ed6f",
              "IPY_MODEL_e3076e36d87c45d48ee719db0ef56b92",
              "IPY_MODEL_61069e89dada4e3399f8e30b365b4877"
            ],
            "layout": "IPY_MODEL_edd2c66315c44c4bb496283dc69dfe06"
          }
        },
        "6ffc654e12e3403fa364b52125e5a30d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffe980df3ce46afbbb9b3eb612d2787": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "706e3a95bed3413080faa2f23faf66ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eedf618b5344ba7890eb6b5c0eb9cdd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f2c7fbbaa0374caabedeebd22cdbeacf",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "7158f9ded03248e08251c11beedc0d95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715b1990d3814ad29902ef7feff8c5ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a6136d94ff45938a65ef8d69c9c3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e0a91637c9422099f67e7a91d03f87",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca66eb8bce6a45818b897c3f70b1e1c2",
            "value": 10
          }
        },
        "71abedd3ea6543bd9e02215965e1bd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43a94077573a4ba090c8f2af73215d9c",
              "IPY_MODEL_cdf662205e2d4d168694c620e368f499",
              "IPY_MODEL_8c081a391b7b40d58a1b57c7a4899e81"
            ],
            "layout": "IPY_MODEL_9e22cce8a8154ac7a37eb8d6eabd64ca"
          }
        },
        "71bb93edc2ae4a0aa239175773e40047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71dd2ac0bf1f48acb2299e01f673405f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721383979a7c45399264d6439a6b3639": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cbf4452135f4c1db96f27ed051b7346",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4a31ef1c2129427a807a1b5639f8eaa7",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡77.92â€‡examples/s]"
          }
        },
        "721455084e1b437a865bcd17d99244bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "721d22af5052436eb1ccdf9911caa074": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729e4df970634927a3d7d2b72607d000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71dd2ac0bf1f48acb2299e01f673405f",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_023f04357113484fa2936430eb888454",
            "value": 10
          }
        },
        "72f57cc38eef4739bdee12162a09db61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730cb697502b4b389b09d65d3f1789d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_075261f66e5a41b696a51553262d0d68",
              "IPY_MODEL_7d9f2c7d6576426e94919023095945a3",
              "IPY_MODEL_0c6b80f9422b487aa78d272f91285321"
            ],
            "layout": "IPY_MODEL_cae0177279f240f5a3a32d1c4c153298"
          }
        },
        "732f4c0d8ed0485caab688a89bed4fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bd3462fbc34a4e8a1dc226a0f6dc99",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd4db3a38ef343838896e1440c1ec976",
            "value": 10
          }
        },
        "74117064d79f406d9bd669a49b826a11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743c49668dd64fbe8c1a87354e80a6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b3275e844114ec2b0923fe793bfa2f7",
              "IPY_MODEL_0a5c7ffac7d84fc0b62b7496362309fd",
              "IPY_MODEL_f859cfbbfb2b4340be4c5badc2c569c0"
            ],
            "layout": "IPY_MODEL_9ab776b973c14a72a0cd8b5546ffed9b"
          }
        },
        "7467e9bde9494565bd053f153e4bbdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7481ff21352c46e1bd2bb0bf496ffec8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74a29070aa0e400bafc6f05f12fb0b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05354ab1f9254b41b9ac9e26295e83c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_439592cf8575406c9ac60c6385f17224",
            "value": "â€‡2.29k/?â€‡[00:00&lt;00:00,â€‡107kB/s]"
          }
        },
        "74b2c2f347544ad3a490924920c420b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74e83d3ea80748f4916ab91bf6e9fdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7568cd143d0f47b298a6ec83985d8b45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b90a22efc949c49ed1855b9fe2c1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7606b668154b498cb6a14faf520d1489": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "76ceadea5ec54ba3b91bc45fe7750413": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76cf2dadfdcb420e961584ea80a23261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77bbaf79cb4b4128ae11bd8d9c47c5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77e3d0726193424a808e3fa494307a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "784a48bb43514f3cb13325f2cbb330b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78edd019b89a4b3f94a058a3d7f9165b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7340af60a0249fba4494b35d189fb44",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_878634f684a24292a86e2b7d3b671416",
            "value": "split_20250821_195516.json:â€‡"
          }
        },
        "791b2f5791d3420d82af98c5d0cbb1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88bfe626212a45c08028153bbc807f36",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0060a45aba4e44d18cb406730a7cc0be",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "7920d21ab0ac489b8234fb26248c12e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79427544a58143b581438e35bf00fab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b094d5571ded4398a0eded5d1003f5dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d689903d59f4768abe7680480f75c56",
            "value": "â€‡7.41G/7.41Gâ€‡[01:32&lt;00:00,â€‡138MB/s]"
          }
        },
        "7993bb4f24f148e9893a55c443689ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dfd29d223304e81875b144a88fae11e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_880b2827b2b74777a757230ad05a2370",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "7a5fa4c0507d4ca580ea705a3db34f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ab00b6f21c641289aab6f1598754b89",
              "IPY_MODEL_4b682354338a42c69ae71167fee89580",
              "IPY_MODEL_66e29d4978d247dd8fee3b26ddcadfaf"
            ],
            "layout": "IPY_MODEL_04b6e817c3af4f63a13b9fd12684fe77"
          }
        },
        "7ac2a62e47d64e989d665d0bbe51ab97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ad50d33235f4fac9dc1f4f64960eafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c998bacc1d245d0b2298a52404d2a64",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0b6951f1264244eb8147d4e32a4dfe84",
            "value": "split_20250821_173006.json:â€‡"
          }
        },
        "7b8e7b3ca0f34eb89eb8fbcbbb409169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebe014bbf704c31b2aa1cd38b4eb39a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e4e91713bb74892be7320691ded1773",
            "value": "â€‡2.26k/?â€‡[00:00&lt;00:00,â€‡87.8kB/s]"
          }
        },
        "7c198bc26b0b4167b1cfd7bccb5e6070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e2383c7d6046a4bd4afadb033d3d45",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c9e84972fdc54df4b1615a30e2734447",
            "value": "â€‡95.0M/95.0Mâ€‡[00:03&lt;00:00,â€‡34.7MB/s]"
          }
        },
        "7c5f244f24eb466da7cf4fd3a126b484": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e021fafe2d17436a9ae9ef88d1b776e0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fed5c8542b494ae6b1add92418a2b0bc",
            "value": "videos/split_20250821_170439.tar.gz:â€‡100%"
          }
        },
        "7d0ca61de917451fa1f0b9c8372943c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d0e29dced1b4befbeec87d8398e2b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7bf4d7b392946a7aa0fa3ced1ff5043",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6870473e7db44da7a950b2b2a1b11de8",
            "value": 1
          }
        },
        "7d9f2c7d6576426e94919023095945a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba68460048014697b44e2140487e7093",
            "max": 157073724,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a356cc24f20441848d00847c91dec1c5",
            "value": 157073724
          }
        },
        "7df1f19b7c8b4d8aa65f96064f3d8353": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e067139bacc4f05a7573610abe098f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e448796fd0c401d9b2d6bcc61d189e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f73cf3d9c0d745ab8b172dc68868a946",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe692706abbb4c4d90166714db6b388f",
            "value": "split_20250821_131935.json:â€‡"
          }
        },
        "7e47973e045646ffa600958e581f3845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45cfca5b10b945f4a1ac0bb975aebc03",
              "IPY_MODEL_59cb9bf6ea074f74b22bf88648245013",
              "IPY_MODEL_b324866114b1436ea25c7bbd61b8caed"
            ],
            "layout": "IPY_MODEL_c92862695c27465abf2fb3db80714f0d"
          }
        },
        "7e5b2669a4fa4a7eab17d6db8101522a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d108c9cf11ee4b34946ba2960f212da8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_035247d2aff746e091c59883732dc1d9",
            "value": "â€‡133M/133Mâ€‡[00:03&lt;00:00,â€‡39.5MB/s]"
          }
        },
        "7e734d02abd249e781e687b7cce75c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7eaa89c3621c41628ece753c9220be5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ecdd079ffc64672b7c5b7332167e8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844925304432433db9adb51157004858",
            "max": 180382769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51c4c765782247a3bfc1e5ef2df604a0",
            "value": 180382769
          }
        },
        "7ee52f2ebfc94b638b1b443c6c284b06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eedf618b5344ba7890eb6b5c0eb9cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1910cd4ac745d09ff1bb09c66c975c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d491bc06c85b4f70a0c5d57c2de91482",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_483611d1fd7d4680b415d0b140ac2e99",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡163.76â€‡examples/s]"
          }
        },
        "7ffb58735ed5432c9df05fe5646b1e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803dc72b746547d1823f7d9d17ccf8fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "808854da31b04db29cc5b912ca57e35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18148b5b771e469dadf1d1e77371606a",
              "IPY_MODEL_71a6136d94ff45938a65ef8d69c9c3c4",
              "IPY_MODEL_b19cfe681a754a27a5f77add87547527"
            ],
            "layout": "IPY_MODEL_c40cc7ccf5af4fb5845daebe041cc873"
          }
        },
        "80e5368351ad47dbb6629cd584e05492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76b4e8438754885ad7c99327a6749b9",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf07a0b8bbb44796b3aaffdafef8a9cb",
            "value": 10
          }
        },
        "812883ffb2774195906c864aaf179d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b57055c40d4eb2b9f1f1c9e1b11503",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_481772e766534b7f962e6f476e230ea6",
            "value": "videos/split_20250821_150431.tar.gz:â€‡100%"
          }
        },
        "8136b90da8fe4cdc812d8ddcf5350ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a9a725bb4740b0a6c662428f35ae1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d8b690f4d7483e9d3b1d6ee6a16943": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39608cd9847442f3a5726bf48635ce21",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6603aa49e7944973888371d3962b57ac",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "82359936ea6a409e9ab645b92ea23853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82546c80f8504f72af066aeb750b2f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82daef89f74442f993380d83196be2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83045cbb91e04c69bb6a8345fc617590": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c46f61627dce4b61be0fcb1a5bbce6ab",
            "max": 124805307,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f655efb40dc54423a2d59aec8ac95782",
            "value": 124805307
          }
        },
        "8342d04350b14f10aea8baeabffef4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41fa98825f841a98b5e4bdd8fd9e8c5",
            "max": 95021670,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c459c0bef2e1408a8437c16300c1d0a9",
            "value": 95021670
          }
        },
        "8386e7a55cb54f3d9cdcfbe67bacf90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83abbbe2fc874dc1985edf756c4364c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee304a35b0640f6a746def6ad48ae89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_44c469928a1c4311975e23f8ba0a6775",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "83cb9e7bfcab4d099d6d29b4ef181dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "844925304432433db9adb51157004858": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84797b95c67242a6aae1c80f0141dc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eda8fcba58134affac690b01244be73b",
              "IPY_MODEL_2fccd67badc142c3931d23ff28646a01",
              "IPY_MODEL_a44e89e031e14172acb68d12d6aab78a"
            ],
            "layout": "IPY_MODEL_c6aa0f6b08394fd2a96b79808ba69b19"
          }
        },
        "847b5cf923bc40f1a4d35a0852915f22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848974a8ad4443e5b529a54d8565d2c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e25370886c4477a89ef99b71767a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85bb7c017edc43f2837999e17492bd1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b8cdba72c0487487691e7bf541b1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c787dce02b5f406692bc483d867f1b9d",
              "IPY_MODEL_edd173faeff74ad283b6b7373b9c391e",
              "IPY_MODEL_3d52615582bb4accbaccf10b62388699"
            ],
            "layout": "IPY_MODEL_eab29b89038e497ea1782cb05638243b"
          }
        },
        "86c07d6b29554e64ad5ac43bb51eefda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876ba3e195e2452fb43ac1d383a7a058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "878634f684a24292a86e2b7d3b671416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87e2326b64e04116bde463995c2ade1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4c1aa24d61540319f7f8afda4ba118d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b5bd7aa9ed8243e4a2576530ff0ecf15",
            "value": "â€‡187M/187Mâ€‡[00:03&lt;00:00,â€‡70.5MB/s]"
          }
        },
        "87f3177c15c84cbca6c8985d40350095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "880b2827b2b74777a757230ad05a2370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88bfe626212a45c08028153bbc807f36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ca8fb1b43e43fca2d1261b976e0b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa0804b6a8864dcc9a3d8f57ee80ce20",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4b0a1043763045269b43b29552d7a71c",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡249.00â€‡examples/s]"
          }
        },
        "8ac39827c83040fe86552c7e6dbcd488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_490fa60f449d4a53a7daa8faf04cdeb4",
              "IPY_MODEL_a49580a7d86c43e5a113fbbb5961c075",
              "IPY_MODEL_5192a52a52214369b17cc706dbc9f94e"
            ],
            "layout": "IPY_MODEL_012bfec4c75f4e45917e35934a70793c"
          }
        },
        "8b3877c6710d446d8e67c98bbd9af5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df1f19b7c8b4d8aa65f96064f3d8353",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_994031e2a5b94d759325417f88a61aa8",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "8bdb140957dc4cfc801c65e09b6cee8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0bed94338241a8a38122aa263167b9",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_072b7b4b4f8e4155a30344a01487ce00",
            "value": 10
          }
        },
        "8c081a391b7b40d58a1b57c7a4899e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7481ff21352c46e1bd2bb0bf496ffec8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8386e7a55cb54f3d9cdcfbe67bacf90e",
            "value": "â€‡154M/154Mâ€‡[00:02&lt;00:00,â€‡70.9MB/s]"
          }
        },
        "8c13dc62bbab41f092a3ade2d2c8c61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66f31e15a37e45778ac20eef49de6b50",
              "IPY_MODEL_d47bed531c4448ca8241cbea9866371a",
              "IPY_MODEL_c5a63cb25e514430bb9aa444a917f2b1"
            ],
            "layout": "IPY_MODEL_c9350c6b76ed42d681e390ebf5eddcd0"
          }
        },
        "8d3f436e4e9341a7b676d5d9f4328d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e38d6f60659414ab12f6fa54859908f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76cf2dadfdcb420e961584ea80a23261",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_51b3e90661da40cba555b6aba7519ee0",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡267.94â€‡examples/s]"
          }
        },
        "8ec8343e996a4a9d8f9358f5b16a4e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aae7c1e081284a11b70ce350622bf561",
              "IPY_MODEL_303d650c7fc64aee8f0660e60b0b53a4",
              "IPY_MODEL_79427544a58143b581438e35bf00fab3"
            ],
            "layout": "IPY_MODEL_37e54b1fbf2443c7a64442ab02da654a"
          }
        },
        "8ef4d90f20a64cd18349af605aa9717e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef87f87155440d58d8d81b63abef220": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8eff4035a1e747e6bbc7e37c408e699b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_696c607a088e4d8a924dd7e7a9c808f7",
              "IPY_MODEL_7d0e29dced1b4befbeec87d8398e2b9e",
              "IPY_MODEL_674d2320910d4b8cb54085f8229e77c9"
            ],
            "layout": "IPY_MODEL_e57b5ef904584d758b82f6bba122b9b8"
          }
        },
        "8f6be2f006a047a3acaacaf836d2b0ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fefddf659334043be219829f9cdd582": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d901cfd4c4ed46e48b950fe06a0479ac",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c76f6a9a5fe548aea1e49c5765cffdbc",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡225.55â€‡examples/s]"
          }
        },
        "9047f7d04c224adba2cd76ea69f565fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90bdb4d60ab846b89b81f4ca6441eb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90ce8df4723b4a0f9f3b68ab92beef2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_121deeca9bb546d28224240a3d1a6dc1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_803dc72b746547d1823f7d9d17ccf8fd",
            "value": "â€‡178M/178Mâ€‡[00:06&lt;00:00,â€‡33.8MB/s]"
          }
        },
        "91bd3462fbc34a4e8a1dc226a0f6dc99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e2ac58f18843a494fd5759f7ea2f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "921389b39fbc4e3b97956f41b6199fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_848974a8ad4443e5b529a54d8565d2c5",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f2b7466b714466282c0f90915215c25",
            "value": 10
          }
        },
        "9258682f4c44440eb3fa1f33df2655e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92b9e4ff38b343aeb50c5f50066aa267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933719bf83d747ecbf2a9d7ca8c5b5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61127971a60d4c9d963f33e3bd984153",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c45516f7e361494cba1b817ffa90f469",
            "value": "split_20250821_155434.json:â€‡"
          }
        },
        "9344c5102dde4a2da6b9ae48bded44f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3745e9cec4c4b18a725d6bd9a6a318d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1a338f9a15ad43ccbce0cf5aab1704f0",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡46.4kB/s]"
          }
        },
        "934bae43e2694a9dbc5bc03fa11a6ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93650f3e4d864114b989179929250948": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7d163c2d364f6aa2b9bdc7f36ea946",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e0ab1cf49504ad2afb0970d2b1f13c7",
            "value": "â€‡149M/149Mâ€‡[00:02&lt;00:00,â€‡67.0MB/s]"
          }
        },
        "93aebab0e618451da2d89b13722cc4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3210c647eca74de7b36817b751c5c28a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19d8e43fbd8041f3967bf8fbb1211a82",
            "value": 10
          }
        },
        "93af1c5299534a788a16ef462bb80427": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93fa188741784837be98c7cb79e8b377": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9413a45d8ee84e868bf4783b4d44bee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b60efd809e47a6be8c8be22769a8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970013d0191f4451ac109cf222c58c21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9751fb094c5f43dbb55181033dc95b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d69a2f68f04a7890924dae30ab01ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98044f2a74a145f6b74a1b9f35863a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "985d7cba227047b9845aad66b3d33e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "994031e2a5b94d759325417f88a61aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99a02d4e278648ef9a1c9fc1bf928f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a9359d785ec48a8b9e6673531fd49af",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_108dc838b7e54caca61e8b1907bb2ac9",
            "value": 500
          }
        },
        "9a95ef09ed0f4563a9952936f844e430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ab00b6f21c641289aab6f1598754b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de96312514014a969717aa18717aef90",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ac2a62e47d64e989d665d0bbe51ab97",
            "value": "split_20250821_162429.json:â€‡"
          }
        },
        "9ab776b973c14a72a0cd8b5546ffed9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b14b799c11043078a7f110920e6f32f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3275e844114ec2b0923fe793bfa2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a03a4d9af0240d299a2864c94821a7b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_050b00824a714e5eadb351e4caf2b607",
            "value": "split_20250821_170439.json:â€‡"
          }
        },
        "9b8b976a237c466ba617da7a5e616955": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be23aa970a142f3a0f96da607cbf378": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c313bcbc0f741f3b0a723da6bd5b83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_439179fe810649a8b55a0859fb38dbea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5fde4ebd394d46a4a2e5bc1a1e77ce18",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡141kB/s]"
          }
        },
        "9c3206976fce4c3f8d0bde8110e683c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d814cd564d94b389f1e12018fc08f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8c0134c0ac4d568e6215c71f485312": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9104542e7e4184aa7503d1b3a70e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c638ead9ded748c493a6c7107246ddf7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_047913100e204459aa7ffe9d03688d0b",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡292.92â€‡examples/s]"
          }
        },
        "9dbab2f5805e4ee9ad2f1e1813714116": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d695bb5cc57343c38e86a2958ec676da",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdabbfaf96e14884bf4c7c22f9abc6bd",
            "value": 1
          }
        },
        "9e1ad5401367433ab70aac38bd84aa0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a9a725bb4740b0a6c662428f35ae1b",
            "max": 96115794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_669a2772073a4288ad1ce136568f0e74",
            "value": 96115794
          }
        },
        "9e22cce8a8154ac7a37eb8d6eabd64ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e3a680752334e96a13f007c5073cce2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e9f13688d7b4ca4a126dd8f55347063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fbddcd9e35341abbe25009ea1f1303c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01012fc829d430f96f001834b45ceef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a039accf745b42f6b59da7a44857c81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a063d9706a1c4e2caa09eece741462d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06cd463710442329642d58afaba5bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_085e513ab0664259b706f91d55b33556",
              "IPY_MODEL_9e1ad5401367433ab70aac38bd84aa0b",
              "IPY_MODEL_dac71f140ad540b89429c349b1554824"
            ],
            "layout": "IPY_MODEL_bc972d28b60d4949b5e89b86232b2bc0"
          }
        },
        "a0be5ff8bfa045688d5aec49eb157b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0efe76cdc234cf29cb823aebe7b438c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f83e0acd6e4a4d8b18921ce0ae8ece": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14a469ffd9f4fce9c2b10aabba793bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c00b95fe9f436d8b390d9a31962f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cd43556384444184d49646151bbaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf21a9057e564960bda8ff4b5b3e34fc",
              "IPY_MODEL_3ef9643f5c564deab9ac6b549bc86dbd",
              "IPY_MODEL_7f1910cd4ac745d09ff1bb09c66c975c"
            ],
            "layout": "IPY_MODEL_8d3f436e4e9341a7b676d5d9f4328d6e"
          }
        },
        "a2994e2991c44db48cf94852b22b1142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f598586ab94ec49003ba4e92bc8481",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d66f62907aa045a8afcbc88fbf7a19d8",
            "value": "videos/split_20250821_134435.tar.gz:â€‡100%"
          }
        },
        "a2f482e1a9ad435aaa223463ea42e264": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8614384512b426d978e55a4f8f12b85",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0e48cbe8df5a41b9a0a4142f34859db7",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "a342890e08674977b6a61706d45f055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bdee5065fda467585a2017d9ebb240b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82546c80f8504f72af066aeb750b2f9b",
            "value": "split_20250821_202505.json:â€‡"
          }
        },
        "a356cc24f20441848d00847c91dec1c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3706210875742a392e5507e973dd029": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a381e784e97846309d90b49e7aa17812": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3c46b0e86cb4ae6a924454f04a27fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a43d408cd3604ef0aae3ad8aba861e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a44e89e031e14172acb68d12d6aab78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7af02bc5b04afe840bf09e5e85d66b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_74b2c2f347544ad3a490924920c420b3",
            "value": "â€‡2.25k/?â€‡[00:00&lt;00:00,â€‡49.8kB/s]"
          }
        },
        "a471b8d5bf0b4b92b9b45239910ee70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec8dddb4b89459e970d64ff4e72a657",
            "max": 133461565,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f14d3a97a9fe44be912e9e55c90f186b",
            "value": 133461565
          }
        },
        "a47eaed8224142899e3d195e3b12e99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a49580a7d86c43e5a113fbbb5961c075": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35ef6876004e47fbb0d43cc8306000e9",
            "max": 146551682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2473253d855d4bff885e5994f7dc11b5",
            "value": 146551682
          }
        },
        "a4ef2bb0dd2f4784846b06341f79525b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_155492956989422a97a34d9a3371a7f4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d508744b736d43bf84c49a350ca3598f",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "a59635b5aa2741418880f7f7bad1d110": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6106767a2eb40ab8ddcc6be7941a93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_168057af343348cbb415c9730da44bfb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a43d408cd3604ef0aae3ad8aba861e3e",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡282.38â€‡examples/s]"
          }
        },
        "a6139ea9dca140d88b274eff9252136a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a628453aa18d4053baca7eebcdb7009b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9d112daa4a4ec58f3980518518f394",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af9d0b21066c43fd993b8d5bde261c02",
            "value": "â€‡80.9M/80.9Mâ€‡[00:02&lt;00:00,â€‡33.2MB/s]"
          }
        },
        "a74bc62fd14b47a691c62d4617e4c107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be23aa970a142f3a0f96da607cbf378",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_deb5afc36a4448d98278b50f9011ba7c",
            "value": "â€‡91.4M/91.4Mâ€‡[00:02&lt;00:00,â€‡40.0MB/s]"
          }
        },
        "a75e30e0ddcb46678de9a7eee0bc426b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2695419ccb9f41c997972ca73ee2ff16",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65535083ec1743d7a581fceada8026e8",
            "value": 10
          }
        },
        "a76b4e8438754885ad7c99327a6749b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b1f60d75854a0fbb5a17f36af9afc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bff63d6b5246e685e7ea5e8f7af403": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ca470386ed4b07b739d7b4172edb69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a7fe13207ca842fa8c268a9b97db5b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8403440ef3c42e19e7b7feb11eedd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef87f87155440d58d8d81b63abef220",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af35af701f7742adbdb872701b802911",
            "value": 1
          }
        },
        "a8b4769d77a04ca68626ee98d12d6db0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e2383c7d6046a4bd4afadb033d3d45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92da005cc684a1baa963293d303f37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9e7fe53b70444f58c2542597dca0526": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa0804b6a8864dcc9a3d8f57ee80ce20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa1d0dd742c34c45a13552abecba153c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef20d2158fa427dad7227469a690099",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_303be2e8dcee4c97a2167652c4137529",
            "value": "split_20250821_191635.json:â€‡"
          }
        },
        "aa977cd78abf48ee99b86e86d8dc8a57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "aa9e9cc4b93f4a4d9f0af0272dc79b70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaaeed62e4054505af6ed055a4dd3af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273b09e7cc2b416eb5db565d501fa805",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eaa89c3621c41628ece753c9220be5d",
            "value": 1
          }
        },
        "aae7c1e081284a11b70ce350622bf561": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e2c3bb30fd348d890c28e35a5b3342a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d0c2d343d3e6415eab4e4f88774ce4b1",
            "value": "videos/split_20250822_142242.tar.gz:â€‡100%"
          }
        },
        "aaecebc7516f4767a096969e33879d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d715bc644949f79c78ea3a663a2176",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c4766483ab74a3baa08eba6ac03d65c",
            "value": "â€‡2.26k/?â€‡[00:00&lt;00:00,â€‡58.5kB/s]"
          }
        },
        "ab65b4ad51bc47249d842912d024d2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_721d22af5052436eb1ccdf9911caa074",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9413a45d8ee84e868bf4783b4d44bee0",
            "value": "videos/split_20250821_162429.tar.gz:â€‡100%"
          }
        },
        "ac277180d4c94f478f65f8c5aa56a37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a3744d84b04db6829425dbf791aa3e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b366897863341bc902aa35e92dfe9b2",
            "value": 10
          }
        },
        "ac2952f5d24245789649586fda6d3285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aca35e59e80f4c7f9e8b52a4d63adf1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acb5e6891fa44f63a2dfe5cd5c3bdf67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad09ccbe47224a1cb785c16e7be0d1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4874cc43aa478d80fc3d5cd999c0d9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f8fc573535f457ab04d81402a8a57a5",
            "value": "videos/split_20250821_140536.tar.gz:â€‡100%"
          }
        },
        "ad241810bb0e48e8bd01d689c793f9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7993bb4f24f148e9893a55c443689ec6",
              "IPY_MODEL_8bdb140957dc4cfc801c65e09b6cee8e",
              "IPY_MODEL_9d9104542e7e4184aa7503d1b3a70e3a"
            ],
            "layout": "IPY_MODEL_2c58e331be3142f2a9ff6b8dbb4775d7"
          }
        },
        "adbaee126343429c8343316716d48bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa1d0dd742c34c45a13552abecba153c",
              "IPY_MODEL_bbb0b513680141a88b61a92246861e3e",
              "IPY_MODEL_f6801e9fa06841b995530bd7395461f7"
            ],
            "layout": "IPY_MODEL_17b080ff18024e25b0e22d8f1522dbdf"
          }
        },
        "adca45cd41014108b0d1c2585d6dbfea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "add484f9e2ce4ae2854898b88863e93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add5c6b67a0c478daaab25c4308339da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adf4047907c44fdaab9283e6f191a8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae42c739afad45389d3bbe630c32cc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4d5da567a32485ab0d8210c58fa4a05",
              "IPY_MODEL_fc8d9920e31f4eedb1bbddb82275c12d",
              "IPY_MODEL_002e98318e1f4188873f9148083a441a"
            ],
            "layout": "IPY_MODEL_ee6e5ae9953a4dd79659df4010d5f0e4"
          }
        },
        "ae958630f99647df817229d15ae09239": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0122a3fee84cbb85424407cf7a7581": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af35af701f7742adbdb872701b802911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af571e291ba9431cabf40c0fe8b5c91e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af9d0b21066c43fd993b8d5bde261c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afc13849d4d74e74a10d8b472f81eaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b094d5571ded4398a0eded5d1003f5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0fc373b7a914d87b472958fcd0cb91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e448796fd0c401d9b2d6bcc61d189e1",
              "IPY_MODEL_40b50e03915142c2b68f9ddc6d12ad21",
              "IPY_MODEL_7b8e7b3ca0f34eb89eb8fbcbbb409169"
            ],
            "layout": "IPY_MODEL_d28e4c0863204f5296604e016af9418d"
          }
        },
        "b19cfe681a754a27a5f77add87547527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6c2fd82f3a04af189259caea928afb7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30e60f278b20411eb481c38300d258f9",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡169.35â€‡examples/s]"
          }
        },
        "b2d64a0497da4b4c8cf0f5deb5f5f0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eca679cd1924d6fbd7292698fc76279",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27f7d5a291cb4cca8b2a600673c401d8",
            "value": "split_20250823_072111.json:â€‡"
          }
        },
        "b324866114b1436ea25c7bbd61b8caed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf0e5c1bff84b08900159f20a47e504",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_254053cdb8a042df91004671bd08a4ca",
            "value": "â€‡102M/102Mâ€‡[00:02&lt;00:00,â€‡52.4MB/s]"
          }
        },
        "b374cc48c3574d27b03e02fc5250d047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ee52f2ebfc94b638b1b443c6c284b06",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e755447389744658b4f44c518069eed",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡177.14â€‡examples/s]"
          }
        },
        "b455ce518b02401d95c2dfed5ef14810": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46de432861941ba8fef64f8b10677c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4766cdd10724ac3826f4caf25c40099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c7fe646741437cba9292e921db8c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4fb41df76ba47a3a5a035252b3fc965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa977cd78abf48ee99b86e86d8dc8a57",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4605a8c9aef649b894a57dac614163f4",
            "value": 1
          }
        },
        "b50f14fa80b3428f9951007d60c9c41d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5bd7aa9ed8243e4a2576530ff0ecf15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b61fa56772cb42428d3d9cb848f966b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68ee46368de4b83abd5f087e5089697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b4ec790f3349b1877a35f0cf8d581d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3988f39e200242f9b6a1bc2595295c01",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2593a1f747d8427c8cdfa489e2a87b6b",
            "value": "â€‡233M/233Mâ€‡[00:04&lt;00:00,â€‡63.2MB/s]"
          }
        },
        "b6c5d635f7174548b7a28264c2bb2ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b71cffd7f3214094b467350636f30c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7340af60a0249fba4494b35d189fb44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7bf4d7b392946a7aa0fa3ced1ff5043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b890fbb77e024761b917c79dc6047b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693716292b7e4f2ca76fb2efe0ca8955",
            "max": 148671619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08f9e20118164693b1161f56f82a888d",
            "value": 148671619
          }
        },
        "b8ec81cf4df340dba0362657e12586e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba16508ffd3a4433be3fce6ababb307d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba46758b66e24061b2026d911d88e33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ad50d33235f4fac9dc1f4f64960eafc",
              "IPY_MODEL_aaaeed62e4054505af6ed055a4dd3af3",
              "IPY_MODEL_038af559a4df4dff867cc3eeab6f866a"
            ],
            "layout": "IPY_MODEL_58e67342a7bb461c835888998349d662"
          }
        },
        "ba68460048014697b44e2140487e7093": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb0ea45f8c7949cbb567676fac340962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb18395af3b2403c8094f5d057257b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb0b513680141a88b61a92246861e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b50620b43c54d79aefb14de592936d8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_add5c6b67a0c478daaab25c4308339da",
            "value": 1
          }
        },
        "bc972d28b60d4949b5e89b86232b2bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4db3a38ef343838896e1440c1ec976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdabbfaf96e14884bf4c7c22f9abc6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdc3ceda0c354b2c92bc7b57d21ba996": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdd4746cb35a4e1fa3f1ed0d896ba415": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bdfba7a5d6ef4028972097fcc5be8fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd0800131eac4e96a03c2a7bc1f3211e",
              "IPY_MODEL_5cc7e66bddf54993b60ee4e6814d14cd",
              "IPY_MODEL_c8979407dc4849bfb94475a8e754311c"
            ],
            "layout": "IPY_MODEL_c2186525a96b456b9996e1c88f6b76ca"
          }
        },
        "beac7da3b3f2441590daefb331289256": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec8233b54db429399b930d408fe8804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b455ce518b02401d95c2dfed5ef14810",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ff0e1feee78e41c7a8667f559ead63df",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "befd6da40dee497bb4ae3ad948dc391d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4ef2bb0dd2f4784846b06341f79525b",
              "IPY_MODEL_c7f2cb1e0f304f70abebaa4d73ab802e",
              "IPY_MODEL_64c44599ab9a4ebda6a270f5f3d938be"
            ],
            "layout": "IPY_MODEL_55479a07b48b48f1ac5d8c736fd051b9"
          }
        },
        "bf07a0b8bbb44796b3aaffdafef8a9cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf82f0110b8c458d9547b0253ed15347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1af2e45500f14a8087dc014df39e2fbd",
              "IPY_MODEL_f023ac40c6cb4a30b6a2209bbdf1927e",
              "IPY_MODEL_42c65d5520884173b3dd3fe0eaa68a98"
            ],
            "layout": "IPY_MODEL_6bffc27dfb294a94b9902b84338fc4ae"
          }
        },
        "c0438b53dd5742198bcc4f31990f5928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c071e066e11d4da1b646b83d4b1870db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32bda37c20024511a5042bb4c2a48b67",
              "IPY_MODEL_ac277180d4c94f478f65f8c5aa56a37b",
              "IPY_MODEL_56e382d1943d4990891314c98480663a"
            ],
            "layout": "IPY_MODEL_9047f7d04c224adba2cd76ea69f565fa"
          }
        },
        "c0c49ae9f76f446c86bf3502479511c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13dd9d3ff264c3fa3c0f553c9d5f695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d98febf1f449eeaa0a24e439b0f7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_933719bf83d747ecbf2a9d7ca8c5b5b4",
              "IPY_MODEL_f0a3e7b7eb0447c4b04d1829cd19d88e",
              "IPY_MODEL_9344c5102dde4a2da6b9ae48bded44f5"
            ],
            "layout": "IPY_MODEL_4f9b90fc3132458b8253dfe59c622d4d"
          }
        },
        "c2186525a96b456b9996e1c88f6b76ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22317f7712b4efaac0daadf68535c72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22b9c86a5454ec88ea803b72ea2f2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2ae98e8c1ec40cb96850dd0ecd2be9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2dd711adab9463bbdea128a4439a138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35726c91afc4a1b84f3d4ee2411acd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c37e85323d5c40e99eda9dc7d324c32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f279f39c74e84b0dbcbe0e3cc307f019",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a6139ea9dca140d88b274eff9252136a",
            "value": "â€‡114k/?â€‡[00:00&lt;00:00,â€‡6.16MB/s]"
          }
        },
        "c40cc7ccf5af4fb5845daebe041cc873": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42946b231f9451eaaa09c4002647076": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c45516f7e361494cba1b817ffa90f469": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c459c0bef2e1408a8437c16300c1d0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c46f61627dce4b61be0fcb1a5bbce6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51d43c099884903867e2b1086fe7ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c55c9b6114974f0a8187f30b938d247c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2d8f90e361410f91431e7013a6f874",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_784a48bb43514f3cb13325f2cbb330b6",
            "value": "â€‡2.29k/?â€‡[00:00&lt;00:00,â€‡156kB/s]"
          }
        },
        "c5a63cb25e514430bb9aa444a917f2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb18395af3b2403c8094f5d057257b8f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2311373a86c645cc8da4e90bde6f295e",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡238.55â€‡examples/s]"
          }
        },
        "c638ead9ded748c493a6c7107246ddf7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6aa0f6b08394fd2a96b79808ba69b19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6bd9b2c59884fb79c33a9eb08fefe05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d8e2c35a03420cb285e7cefe8cce1d",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f431d07e04c4de98c19ce92553defd8",
            "value": 10
          }
        },
        "c6c2fd82f3a04af189259caea928afb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c749f822062b47b1a5f44c5b1bf80a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c76f6a9a5fe548aea1e49c5765cffdbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77e1316bdde48679e54314e3f658196": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_706e3a95bed3413080faa2f23faf66ea",
              "IPY_MODEL_a75e30e0ddcb46678de9a7eee0bc426b",
              "IPY_MODEL_672c6d9e827c41f1a0b884e2d5160ab2"
            ],
            "layout": "IPY_MODEL_f9eb26f0971c4e01b088419e8bab2840"
          }
        },
        "c787dce02b5f406692bc483d867f1b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33785dea9e2142078e20af4cb7430256",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7d0ca61de917451fa1f0b9c8372943c4",
            "value": "split_20250821_123446.json:â€‡"
          }
        },
        "c7bcd64db42f44b5813024715e3a954f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7f2cb1e0f304f70abebaa4d73ab802e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8de4d61278842c897adc77126e1bf66",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e85db8aaae73437e885af6f832cbbe85",
            "value": 10
          }
        },
        "c88fdbc5af5049d290b25ac9956c8016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b1f60d75854a0fbb5a17f36af9afc4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9e9f13688d7b4ca4a126dd8f55347063",
            "value": "split_20250821_184950.json:â€‡"
          }
        },
        "c8979407dc4849bfb94475a8e754311c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf205ffe7e84ac8ba550e950d73b414",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_565431068186428fb96dc61d6722f3ea",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡189.85â€‡examples/s]"
          }
        },
        "c92862695c27465abf2fb3db80714f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9350c6b76ed42d681e390ebf5eddcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d8e2c35a03420cb285e7cefe8cce1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e84972fdc54df4b1615a30e2734447": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca2da5fbdb994a9a9a2d06f192084c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca66eb8bce6a45818b897c3f70b1e1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caa698276a7748eea26bce40b44512c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cad8999a891d4311bc8e99be3999c01b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cae0177279f240f5a3a32d1c4c153298": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caf71772ade44d5bae5ea14af062a2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b61fa56772cb42428d3d9cb848f966b8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afc13849d4d74e74a10d8b472f81eaa0",
            "value": 10
          }
        },
        "cb223591ba7241a497ba356a74d44690": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbc5b5bcbf3b4a43a5ee855d48e1903c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b145be5b5348a680012261428c8720",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_87f3177c15c84cbca6c8985d40350095",
            "value": "split_20250821_142935.json:â€‡"
          }
        },
        "cd0238bd8f974863aff2505bf4753adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd90ca6ef0d4b9aae3951620112291f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e781128c74641b4b55bb4d37f1cb347",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4748a6a6b6641d3836effe6a57d4d69",
            "value": 1
          }
        },
        "cde2755689a344bbbb0c19d11cbcbd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b4769d77a04ca68626ee98d12d6db0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_75b90a22efc949c49ed1855b9fe2c1d9",
            "value": "â€‡219M/219Mâ€‡[00:05&lt;00:00,â€‡56.5MB/s]"
          }
        },
        "cdf662205e2d4d168694c620e368f499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df381598e4ee4e92bba3f8dd7aac040f",
            "max": 154268160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31904a1d1cee42328afe1a25706a7667",
            "value": 154268160
          }
        },
        "cedd38e0768f4fc3a530aa2203ad849d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f57cc38eef4739bdee12162a09db61",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_256bbb1035ea4d29970a47d55c1d6cee",
            "value": "videos/split_20250821_184950.tar.gz:â€‡100%"
          }
        },
        "cf21a9057e564960bda8ff4b5b3e34fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e013cb5f81ea41c38267a1f8b875344e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_97d69a2f68f04a7890924dae30ab01ef",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "cf308a88b5ab4dd4900ba23adf8f8331": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af0122a3fee84cbb85424407cf7a7581",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f0707922911a416d8ae86a39637184ca",
            "value": "split_20250821_175929.json:â€‡"
          }
        },
        "cf557807219b4f7abd0d8bc3ccb41702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b60efd809e47a6be8c8be22769a8ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76ceadea5ec54ba3b91bc45fe7750413",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡194.68â€‡examples/s]"
          }
        },
        "d04f1f15c7f648bf965660b95f2bec65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3521cf83dc6140d48cb085eb5a877b81",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_578dbfa43d834111a89b659b6c26aa72",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "d054666105634112a70bd3aa59713a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549a8b0d237b49e980984ee1a09a9f08",
              "IPY_MODEL_17b9dbe38e1741308ffa09dba84a7861",
              "IPY_MODEL_65a4d32d4f5f4171b4baffa9fe042e1a"
            ],
            "layout": "IPY_MODEL_7158f9ded03248e08251c11beedc0d95"
          }
        },
        "d062b042c92b4a8197ab115bb61d7158": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d075fffe056e4aa9902435f2552f9e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2293f9dbdc4ff7bdf9ce757b4e41b4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f33ccec5b77447a790714a51bc388d8a",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "d0b5f548e91946f083ab50b5267dddd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c2d343d3e6415eab4e4f88774ce4b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0e7e707eb8f401d91bb5771036cb83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7920d21ab0ac489b8234fb26248c12e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c155b47fb3348c4b349185b33e6027b",
            "value": "split_20250821_144939.json:â€‡"
          }
        },
        "d108c9cf11ee4b34946ba2960f212da8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d284a716e42d425e984b87f78e4f644d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d28e4c0863204f5296604e016af9418d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d5d97dbbaa4a5bb1a65ec42ff18c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d32bd5192e964190b96c4ac5993bef17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13104e2205654db6bef189fc2314d20d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c35726c91afc4a1b84f3d4ee2411acd8",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡84.10â€‡examples/s]"
          }
        },
        "d3745e9cec4c4b18a725d6bd9a6a318d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b838e2e4a04977be608f22b6b858db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3bcdc103bb646678383751221c6dfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7606b668154b498cb6a14faf520d1489",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4321ea6fb103495ea155e50adb4e79a1",
            "value": 1
          }
        },
        "d41ca7bf21d74aedbcae34a69b7d0bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47bed531c4448ca8241cbea9866371a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6be2f006a047a3acaacaf836d2b0ae",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_601829b443aa4b3ca7d7f93001e5d9ce",
            "value": 10
          }
        },
        "d47c808876e4471e8932d97d540c27ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a384672df804e2eb302f2c262ac1b34",
              "IPY_MODEL_d88b2a209b7c4e5aa827923100b360e2",
              "IPY_MODEL_48bd381f73554708a85608ab8333e639"
            ],
            "layout": "IPY_MODEL_6bdf888eec664801adc41955f5a1be23"
          }
        },
        "d491bc06c85b4f70a0c5d57c2de91482": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d5da567a32485ab0d8210c58fa4a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a6501c92294442ea4462114695906c2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a0be5ff8bfa045688d5aec49eb157b3a",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "d508744b736d43bf84c49a350ca3598f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d59037e3864545bd8492c2f0c4bce614": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d609d45438534d4198916869bd32a700": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d5144411c74c44ac65999ddc12ce63",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_40a5a19395ab4371a429438892caab54",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "d6516ffc11974feab2959b88551ea028": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b3877c6710d446d8e67c98bbd9af5b2",
              "IPY_MODEL_729e4df970634927a3d7d2b72607d000",
              "IPY_MODEL_dca0c4d39e6f49fa841d5fc583b2ce3d"
            ],
            "layout": "IPY_MODEL_934bae43e2694a9dbc5bc03fa11a6ab0"
          }
        },
        "d66f62907aa045a8afcbc88fbf7a19d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d695bb5cc57343c38e86a2958ec676da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d69b5359deaf412f999cd83cd380a358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7c5b0b124b74388a7a059a0a1d69560",
              "IPY_MODEL_044f21ed61424b1195823013d68d680f",
              "IPY_MODEL_5207724128d44f60bced38302e130961"
            ],
            "layout": "IPY_MODEL_b8ec81cf4df340dba0362657e12586e3"
          }
        },
        "d6cbff4a212f4da79a32d97dfef6c53b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d24c4617174a1784e2226d86489949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65bfc9e3caec48468940d297b1b32163",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aca35e59e80f4c7f9e8b52a4d63adf1b",
            "value": "split_20250821_140536.json:â€‡"
          }
        },
        "d6ed5d8fb3df4fd99392f3200f27b4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7001087bf05439182343a1927ca553a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d92c221d8c91439793757d4f04012b2e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c1f53b43ffa44b492ccac077d69f74a",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "d7c5b0b124b74388a7a059a0a1d69560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ecae4e1107d41ca9a2587ff1f060db8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_83cb9e7bfcab4d099d6d29b4ef181dde",
            "value": "split_20250821_182033.json:â€‡"
          }
        },
        "d84b4bf0fdbd44878afd843ab639cfd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8554998c9154dcc9d22dcc7579a255d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_791b2f5791d3420d82af98c5d0cbb1a5",
              "IPY_MODEL_99a02d4e278648ef9a1c9fc1bf928f71",
              "IPY_MODEL_101ac4c7ccc64a22ac2b9459240087cf"
            ],
            "layout": "IPY_MODEL_37e25cbbd7604619a5b588cebf2efd80"
          }
        },
        "d856e6038687496f83319882f62cc188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c346f76a51a4becbf2ddfcf6160e599",
            "max": 187234189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51368a1bdc574e5b91d4a64446851127",
            "value": 187234189
          }
        },
        "d88b2a209b7c4e5aa827923100b360e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_400408cf4b064cff8f114132d923cc7c",
            "max": 5802030065,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a039accf745b42f6b59da7a44857c81c",
            "value": 5802030065
          }
        },
        "d901cfd4c4ed46e48b950fe06a0479ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d92c221d8c91439793757d4f04012b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96cc789a364484cacbf109a98238709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e012111d634802b9c654cb09aab716": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da220582afd74ce7a86147528d7819bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bb03acb2e244e94975548accc82d063",
              "IPY_MODEL_04ead14343a747e1a24cd7c78a2cfaef",
              "IPY_MODEL_a74bc62fd14b47a691c62d4617e4c107"
            ],
            "layout": "IPY_MODEL_188f9b7af1044717b0de58efc8567e00"
          }
        },
        "da27c6d4c412440eb9816d73b5cda70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac71f140ad540b89429c349b1554824": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec18a56e9a24b53b6b797f81545e554",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a3c46b0e86cb4ae6a924454f04a27fd6",
            "value": "â€‡96.1M/96.1Mâ€‡[00:03&lt;00:00,â€‡35.0MB/s]"
          }
        },
        "db48e8e9044847f6b5ce902584783d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbcc0c23d94742a88c00553261dbfd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7568cd143d0f47b298a6ec83985d8b45",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77bbaf79cb4b4128ae11bd8d9c47c5ec",
            "value": 10
          }
        },
        "dc1edb786a4b43fcbd8963ffd9c95757": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca0c4d39e6f49fa841d5fc583b2ce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6cbff4a212f4da79a32d97dfef6c53b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ee8d9603a8e4118a2888ca4bad6889f",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡163.67â€‡examples/s]"
          }
        },
        "dcfff566048840c08210805e690c8b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0238bd8f974863aff2505bf4753adb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca2da5fbdb994a9a9a2d06f192084c7d",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡226.22â€‡examples/s]"
          }
        },
        "dd121d51834c4adf94fff006c0922817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd1f47f01e17425c8c16dfaa774bd412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd402f789cb54479ae12f347fa4bc2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad6ac6f264e4ebbb6ffd856a00d4043",
            "max": 80901588,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74e83d3ea80748f4916ab91bf6e9fdb2",
            "value": 80901588
          }
        },
        "ddae853cd6c142eb9a5632c5af9362e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde1173b23e440c0ab7087d8f58bf261": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de8473b916c2499f9c72f8154190bd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d075fffe056e4aa9902435f2552f9e2f",
              "IPY_MODEL_527a701d969647558797f6f1b10766aa",
              "IPY_MODEL_8e38d6f60659414ab12f6fa54859908f"
            ],
            "layout": "IPY_MODEL_1d1ce762b0f24629856d91b148503e73"
          }
        },
        "de96312514014a969717aa18717aef90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb5afc36a4448d98278b50f9011ba7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dec69aa4fd654d5bb23b5f6229a7f357": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "df381598e4ee4e92bba3f8dd7aac040f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3bb6c7935d430b9f3f3b29144dd391": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0027836565e4d30a6e4ed539a7d5f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81d8b690f4d7483e9d3b1d6ee6a16943",
              "IPY_MODEL_caf71772ade44d5bae5ea14af062a2e1",
              "IPY_MODEL_d32bd5192e964190b96c4ac5993bef17"
            ],
            "layout": "IPY_MODEL_f5ce8be99c12477da37c829d529ecad6"
          }
        },
        "e013cb5f81ea41c38267a1f8b875344e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e021fafe2d17436a9ae9ef88d1b776e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e090a1b5185044bca0aa3c23a9ce9f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e15c010dd8154598aaa58014c22b015c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e18db17aa91c4c5594648b1d0bcd276c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1f5d8be639d49978e839265c04be031": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e299a2733da44b9793e031da8bc3bd9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eab081484424f40b03a241b68d70584",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91e2ac58f18843a494fd5759f7ea2f5a",
            "value": 1
          }
        },
        "e3076e36d87c45d48ee719db0ef56b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd196ad3cc64212a310a52ba497d5fb",
            "max": 232029868,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe6e414b5596422c97771e3212cb9f8c",
            "value": 232029868
          }
        },
        "e3de5bac352a4ef1b393d8bd54f22497": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e41caf959306471ebb32bf31c6e665b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef4d90f20a64cd18349af605aa9717e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_607bb6a94ca14c04bd7f9c6b2ba15f77",
            "value": "â€‡2.26k/?â€‡[00:00&lt;00:00,â€‡114kB/s]"
          }
        },
        "e47062a0f9da44cdb8071a7f5c7a1c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4748a6a6b6641d3836effe6a57d4d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e47860f69cad401abf1799ba4a6710b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2d64a0497da4b4c8cf0f5deb5f5f0a0",
              "IPY_MODEL_9dbab2f5805e4ee9ad2f1e1813714116",
              "IPY_MODEL_c37e85323d5c40e99eda9dc7d324c32f"
            ],
            "layout": "IPY_MODEL_d284a716e42d425e984b87f78e4f644d"
          }
        },
        "e4c1aa24d61540319f7f8afda4ba118d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e57b5ef904584d758b82f6bba122b9b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5998566c5564751aa2c8234ebefc554": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b692e76478e4d11842900add1448c72",
              "IPY_MODEL_4c39c23af87e4daebb8779671dbb67fc",
              "IPY_MODEL_557beb0d27474172905c15595ab28910"
            ],
            "layout": "IPY_MODEL_072cae87b93a4d5c8dc445dd4e3d6e88"
          }
        },
        "e59a905deeee4b12bafe99a934e214e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69857e738b24aa5ae36559fea20504b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6c3ec3956ad4d18bfe1cf00d9462f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2f482e1a9ad435aaa223463ea42e264",
              "IPY_MODEL_3d07736d18f74795b45e3c86fb32fcdd",
              "IPY_MODEL_cf557807219b4f7abd0d8bc3ccb41702"
            ],
            "layout": "IPY_MODEL_f5e35e0a28804c41a217d43d9e12fd58"
          }
        },
        "e6f213a2a6184f3ab3401731d6d9690b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e719034506984aee8a365a98dc55ed6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a063d9706a1c4e2caa09eece741462d0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd1f47f01e17425c8c16dfaa774bd412",
            "value": "videos/split_20250821_123446.tar.gz:â€‡100%"
          }
        },
        "e78e2ce18e4c436991ada9b4f790ce58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f8c93897044bad874a79494d296647": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e85db8aaae73437e885af6f832cbbe85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8b84c0daac54d319d63cbfcaae75f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8de4d61278842c897adc77126e1bf66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91d0b48af9d43ae9aa19007deef2a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78edd019b89a4b3f94a058a3d7f9165b",
              "IPY_MODEL_60caafd857354c7681ae35c933ff130a",
              "IPY_MODEL_9c313bcbc0f741f3b0a723da6bd5b83e"
            ],
            "layout": "IPY_MODEL_5178cd7c967b40f6aa06b326ff65534b"
          }
        },
        "e9977e277b664d8dbd5f51210ec43f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9a05907526a42eea3d00555d5765418": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9bae617bf534efab9c5a82fb4605942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e090a1b5185044bca0aa3c23a9ce9f2d",
            "max": 107888722,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba16508ffd3a4433be3fce6ababb307d",
            "value": 107888722
          }
        },
        "e9e75f26184e4b69bdfc11e7627a90d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecac637d93774b53b7c8f9c8b6dce4ed",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0dd2e9fbbd5a45cb919cb53b63968f65",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡31.0kB/s]"
          }
        },
        "eab29b89038e497ea1782cb05638243b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb1b45c3e48c46aa92844961abd15913": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb46e718676d4b4591e2d0bd3963eeff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8136b90da8fe4cdc812d8ddcf5350ba3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4d06949235904c56add415de70e827bb",
            "value": "videos/split_20250821_144939.tar.gz:â€‡100%"
          }
        },
        "ecac637d93774b53b7c8f9c8b6dce4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd475bdf2c24a23bd926ae53be192d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf054e57eac449e8378892f85419c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_304112ad685a45f5a29ff23e870d5def",
            "max": 219284496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e18db17aa91c4c5594648b1d0bcd276c",
            "value": 219284496
          }
        },
        "eda8fcba58134affac690b01244be73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad9203774a74a7ea61cf32d9621d58a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_876ba3e195e2452fb43ac1d383a7a058",
            "value": "split_20250821_204955.json:â€‡"
          }
        },
        "edd173faeff74ad283b6b7373b9c391e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e734d02abd249e781e687b7cce75c4b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5592bcbe6c614971a65754d9bd371fc7",
            "value": 1
          }
        },
        "edd2c66315c44c4bb496283dc69dfe06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edea68cdf5d942098f18e162fd996a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf205ffe7e84ac8ba550e950d73b414": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee053b038dbc44bbaa0776345ea7e437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c3206976fce4c3f8d0bde8110e683c4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a7fe13207ca842fa8c268a9b97db5b67",
            "value": "â€‡2.27k/?â€‡[00:00&lt;00:00,â€‡86.5kB/s]"
          }
        },
        "ee29d82a1fd04d24961b8a1ca95b82ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee6e5ae9953a4dd79659df4010d5f0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff6e3c4dc694ca6bc4366b6fe234fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f023ac40c6cb4a30b6a2209bbdf1927e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591cabc6e4c34d36a9870d5713f1b8e9",
            "max": 163495813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6de60d8e82144c8a8a9767ab53f4f689",
            "value": 163495813
          }
        },
        "f0707922911a416d8ae86a39637184ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a3e7b7eb0447c4b04d1829cd19d88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c0a809e783e483fbc3387b51b2a5abe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8b84c0daac54d319d63cbfcaae75f58",
            "value": 1
          }
        },
        "f0b26c3e2efd47ff8f022f096ac1adee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10dc417c644442789840938328a84a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f14d3a97a9fe44be912e9e55c90f186b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1c7f7183465492d803e47b58b59a3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_349a2cd6777940c5806354c6cefa1f49",
              "IPY_MODEL_732f4c0d8ed0485caab688a89bed4fd7",
              "IPY_MODEL_b374cc48c3574d27b03e02fc5250d047"
            ],
            "layout": "IPY_MODEL_3198cbacd8a9400680ee4a9e138b9721"
          }
        },
        "f1eed59e2a9b49f6a0f392d65e7133d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_812883ffb2774195906c864aaf179d38",
              "IPY_MODEL_e9bae617bf534efab9c5a82fb4605942",
              "IPY_MODEL_45148cf2f5ea4a708125fdc409395950"
            ],
            "layout": "IPY_MODEL_d59037e3864545bd8492c2f0c4bce614"
          }
        },
        "f232eddf0b474cf39b3fa29ccc3c4681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cedd38e0768f4fc3a530aa2203ad849d",
              "IPY_MODEL_51c6a4a4ab9a4500b07c8019f86d29e6",
              "IPY_MODEL_b6b4ec790f3349b1877a35f0cf8d581d"
            ],
            "layout": "IPY_MODEL_9e3a680752334e96a13f007c5073cce2"
          }
        },
        "f279f39c74e84b0dbcbe0e3cc307f019": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b57055c40d4eb2b9f1f1c9e1b11503": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c7fbbaa0374caabedeebd22cdbeacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e447c0b7a045808bff215b13e87a38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f33ccec5b77447a790714a51bc388d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3590b7cb91044f0b84ec8b4d8240065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb46e718676d4b4591e2d0bd3963eeff",
              "IPY_MODEL_ecf054e57eac449e8378892f85419c0f",
              "IPY_MODEL_cde2755689a344bbbb0c19d11cbcbd0c"
            ],
            "layout": "IPY_MODEL_527f5e3d0bb44bd4962e76b1cc643fc4"
          }
        },
        "f3dfb245de7f43baa44d9ff261712f86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3f0ca91814b4b4aa466a88a084b0e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41fa98825f841a98b5e4bdd8fd9e8c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5047f7a0fe9409cb40abe6ef3404ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f584ce5a43bf4bfca768fc0d43597994": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5ce8be99c12477da37c829d529ecad6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5e35e0a28804c41a217d43d9e12fd58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5fa316359404931b06425a616dd754c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f655efb40dc54423a2d59aec8ac95782": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6739cce2d8843a8a48691717abb2277": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6801e9fa06841b995530bd7395461f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7bff63d6b5246e685e7ea5e8f7af403",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46286a88b8ca4cbebe99594e05163c05",
            "value": "â€‡2.28k/?â€‡[00:00&lt;00:00,â€‡155kB/s]"
          }
        },
        "f69dfa9253cd4e0e966868a569954ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2aee65d7bf304ee8ac88eb1550217c56",
              "IPY_MODEL_17846ac1d87e4acb9db044dd96197345",
              "IPY_MODEL_2a15cadc1c9b4eeb9b40da6925b3fa45"
            ],
            "layout": "IPY_MODEL_fcc512fd88d946648dbd81ec99f2e5a5"
          }
        },
        "f69ec6544e9046f898a943d0ac6a0ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f713aefb8c2f40f7abe60e0ab4b2068d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6168a184cc8a4771812f0e3b4ed08687",
              "IPY_MODEL_dd402f789cb54479ae12f347fa4bc2ae",
              "IPY_MODEL_a628453aa18d4053baca7eebcdb7009b"
            ],
            "layout": "IPY_MODEL_20fe5430471d4eb39a4e0d03530e0079"
          }
        },
        "f73cf3d9c0d745ab8b172dc68868a946": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f752b8d2e55d41a4910541ea6803879c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f859cfbbfb2b4340be4c5badc2c569c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7fbcc3469f48e1a9ebc73f7aee4f33",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_623b36d1793f417e97a35d02ab01fe3f",
            "value": "â€‡2.29k/?â€‡[00:00&lt;00:00,â€‡154kB/s]"
          }
        },
        "f8614384512b426d978e55a4f8f12b85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d65df63bdf43ffbc834cf3d8379487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9eb26f0971c4e01b088419e8bab2840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa0f1c7af95c473dbd17efa8177edd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fad6ac6f264e4ebbb6ffd856a00d4043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb601496f3684f138e379a7ef0f159e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbc3f1de86a94757abc915710f06de49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d04f1f15c7f648bf965660b95f2bec65",
              "IPY_MODEL_0b5ec895ecd547f19fb6bcddfe5c6c80",
              "IPY_MODEL_6364585109b6411eb17b805e2ec3b369"
            ],
            "layout": "IPY_MODEL_a0f83e0acd6e4a4d8b18921ce0ae8ece"
          }
        },
        "fc8d9920e31f4eedb1bbddb82275c12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6f213a2a6184f3ab3401731d6d9690b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5fa316359404931b06425a616dd754c",
            "value": 10
          }
        },
        "fcc512fd88d946648dbd81ec99f2e5a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0800131eac4e96a03c2a7bc1f3211e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_970013d0191f4451ac109cf222c58c21",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_12cb178f70fe43108dddfcf88bcd1329",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "fd1ef440f6654de19c627d85213556c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf308a88b5ab4dd4900ba23adf8f8331",
              "IPY_MODEL_e299a2733da44b9793e031da8bc3bd9f",
              "IPY_MODEL_74a29070aa0e400bafc6f05f12fb0b7e"
            ],
            "layout": "IPY_MODEL_0aaf47e231ac4d798d55e1ae6493879e"
          }
        },
        "fd4e22dd29154990bd22fc05936b6471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe2911ab94654deab8eefa851a84d992": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad8999a891d4311bc8e99be3999c01b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c2ae98e8c1ec40cb96850dd0ecd2be9b",
            "value": "split_20250821_125931.json:â€‡"
          }
        },
        "fe692706abbb4c4d90166714db6b388f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe6e414b5596422c97771e3212cb9f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fed5c8542b494ae6b1add92418a2b0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff0e1feee78e41c7a8667f559ead63df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff4eef111cf84098a02006a002a3e8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "682074df702d4d3795e274fa505ee326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bed3d2411d845519ad123b1ce88f851",
              "IPY_MODEL_a997e5adfdab40e297039c2de8d5ffe9",
              "IPY_MODEL_5211205ba45b4fa7aa4bdfe283630da2"
            ],
            "layout": "IPY_MODEL_da7ed0baf11b490fbb8ac28af7565c1d"
          }
        },
        "7bed3d2411d845519ad123b1ce88f851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8385ba69874f2085a94b27ddf7d66e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04c3800e6bff49928b7103059f1b2142",
            "value": "config.json:â€‡"
          }
        },
        "a997e5adfdab40e297039c2de8d5ffe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c24ef86e254453aa5036c0ce03a314",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4a6beca900249ce934c99e857a2800a",
            "value": 1
          }
        },
        "5211205ba45b4fa7aa4bdfe283630da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8a20c44a6e4286863cd2e0dd0b8972",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c3970a5f776461f989c99b2402302ac",
            "value": "â€‡22.7k/?â€‡[00:00&lt;00:00,â€‡2.49MB/s]"
          }
        },
        "da7ed0baf11b490fbb8ac28af7565c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8385ba69874f2085a94b27ddf7d66e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c3800e6bff49928b7103059f1b2142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87c24ef86e254453aa5036c0ce03a314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a4a6beca900249ce934c99e857a2800a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca8a20c44a6e4286863cd2e0dd0b8972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c3970a5f776461f989c99b2402302ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c6743e8627466688900eee7fc14232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ce747e04020442ca9a2b5e971158bb3",
              "IPY_MODEL_26a4f5d7961d4e66819a4c87d50fbd74",
              "IPY_MODEL_f8f1d8d88fc44fb585abfe4dad945995"
            ],
            "layout": "IPY_MODEL_c41472a91a4b4926973fb2ba23f1dfd7"
          }
        },
        "9ce747e04020442ca9a2b5e971158bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d4adcc05dd4db7ab9c116fce296607",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8838fc22c20f4e07871e9145108951e7",
            "value": "pytorch_model.bin:â€‡100%"
          }
        },
        "26a4f5d7961d4e66819a4c87d50fbd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_297bdcd3b8474ef4af49f14894ecdf49",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_248dd61e709843e78b7bcf18f81502b3",
            "value": 486348721
          }
        },
        "f8f1d8d88fc44fb585abfe4dad945995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d05f8c034144fb2a6b3f53ab4b51232",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_74b3562c68694e33b2b3437af9fef979",
            "value": "â€‡486M/486Mâ€‡[00:02&lt;00:00,â€‡283MB/s]"
          }
        },
        "c41472a91a4b4926973fb2ba23f1dfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d4adcc05dd4db7ab9c116fce296607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8838fc22c20f4e07871e9145108951e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "297bdcd3b8474ef4af49f14894ecdf49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248dd61e709843e78b7bcf18f81502b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d05f8c034144fb2a6b3f53ab4b51232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b3562c68694e33b2b3437af9fef979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928db6d483464f60b9eef1b0d7888f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ccdf8c51fcb487097fa35a8ea2c570a",
              "IPY_MODEL_0ec462c589d34d66a5b98f1163ebb9b3",
              "IPY_MODEL_c21cc744ceb04b8ab3f9c52a7071d2aa"
            ],
            "layout": "IPY_MODEL_fd8927427a2e438984d01be2ec8a3876"
          }
        },
        "5ccdf8c51fcb487097fa35a8ea2c570a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a798b64eb2d8429e8b6a3970f9c00f87",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8795b643af4d46ebb901a248ebdfad99",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "0ec462c589d34d66a5b98f1163ebb9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d484cba1615f45418294fdf5ad5f622d",
            "max": 486296528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b67a6ed8e47e4165a3ee1c00807be46c",
            "value": 486296528
          }
        },
        "c21cc744ceb04b8ab3f9c52a7071d2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b97716577e01445c9c731c6edafd0249",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ba94841c8c1407f8a904ca7a9a21f72",
            "value": "â€‡486M/486Mâ€‡[00:01&lt;00:00,â€‡519MB/s]"
          }
        },
        "fd8927427a2e438984d01be2ec8a3876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a798b64eb2d8429e8b6a3970f9c00f87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8795b643af4d46ebb901a248ebdfad99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d484cba1615f45418294fdf5ad5f622d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67a6ed8e47e4165a3ee1c00807be46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b97716577e01445c9c731c6edafd0249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba94841c8c1407f8a904ca7a9a21f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}