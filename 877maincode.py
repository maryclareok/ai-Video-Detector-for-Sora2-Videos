# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rdiI9iZisyI4zhJKWXaGs2lvcXqHbK_Y

main code
"""

# ============================================================
# PAPER FINAL RUN (MID-ONLY) â€” R2Res (R2+1D + ResNet18) + MLP
# Full dataset target: 1200 AI (AI_CORE+SORA) + 1200 REAL
# FIXED: no oversample requirement on REAL (you only have 1200)
# ============================================================

!pip -q install opencv-python-headless scikit-learn pandas tqdm matplotlib joblib

import os, cv2, random, hashlib, subprocess, json
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from torchvision import models as tvm
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve, confusion_matrix, brier_score_loss
)

import matplotlib.pyplot as plt

# -----------------------------
# 0) CONFIG
# -----------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PIN_MEMORY = (DEVICE == "cuda")
print("Torch device:", DEVICE)
if DEVICE == "cuda":
    print("GPU:", torch.cuda.get_device_name(0))

# Input directories (your setup)
AI_CORE_DIR = "/content/drive/MyDrive/AudioModel/AI"
SORA_DIR    = "/content/drive/MyDrive/soravideo/sora2aivideos"
REAL_DIR    = "/content/drive/MyDrive/genvid_ucf_1200/real_1200"

# Output
OUT_ROOT = "/content/drive/MyDrive/ai_detection_eval"
RUN_NAME = "PAPER_FINAL_R2RES_MID_ONLY_FULLDATA"
RUN_DIR  = os.path.join(OUT_ROOT, RUN_NAME)
os.makedirs(RUN_DIR, exist_ok=True)
print("RUN_DIR:", RUN_DIR)

# Targets (paper run)
N_EACH_TARGET = 1200  # AI=1200, REAL=1200

# Oversample settings:
# - REAL cannot be oversampled beyond what exists (you have 1200)
# - AI can be oversampled because you have 2450 total
AI_RAW_MULT = 1.10     # oversample AI only
REAL_RAW_MULT = 1.00   # DO NOT oversample REAL

# Metadata-neutral normalization
NORM_FPS   = 25
NORM_W     = 256
NORM_H     = 256
NORM_CODEC = "libx264"
NORM_CRF   = 23
NORM_PIX   = "yuv420p"
NORM_AUDIO = False

# Clip sampling
T = 8
FRAME_STRIDE = 3
TARGET_SIZE  = 224

# Training
BATCH_SIZE   = 6        # lower if OOM
EPOCHS       = 12
LR           = 3e-4
WEIGHT_DECAY = 1e-4
NUM_WORKERS  = 2

TRAIN_CLIPS_PER_VIDEO = 3
TEST_CLIPS_PER_VIDEO  = 1

# Dedup controls
NEAR_DUP_HAMMING_MAX = 3

SAVE_EVERY_EPOCH = True
BEST_ON = "f1"

# -----------------------------
# 1) HELPERS: list videos
# -----------------------------
def list_videos(root):
    vids = []
    for r, _, files in os.walk(root):
        for f in files:
            if f.lower().endswith((".mp4",".avi",".mov",".mkv",".webm")):
                vids.append(os.path.join(r, f))
    return vids

ai_core = list_videos(AI_CORE_DIR)
sora    = list_videos(SORA_DIR)
real    = list_videos(REAL_DIR)

print("AI_CORE videos:", len(ai_core))
print("SORA videos   :", len(sora))
print("REAL videos   :", len(real))

AI_ALL = ai_core + sora

# Determine how many we can actually target safely
N_EACH = min(N_EACH_TARGET, len(real), len(AI_ALL))
if N_EACH < N_EACH_TARGET:
    print(f"[WARN] Not enough videos to hit 1200/1200. Using N_EACH={N_EACH} instead.")

AI_RAW_EACH   = min(int(np.ceil(N_EACH * AI_RAW_MULT)), len(AI_ALL))
REAL_RAW_EACH = min(int(np.ceil(N_EACH * REAL_RAW_MULT)), len(real))

# -----------------------------
# 2) SAMPLE RAW POOL
# -----------------------------
AI_raw   = random.sample(AI_ALL, AI_RAW_EACH)
REAL_raw = random.sample(real,   REAL_RAW_EACH)

raw_df = pd.DataFrame({
    "path": AI_raw + REAL_raw,
    "y":    [1]*len(AI_raw) + [0]*len(REAL_raw),
    "src":  ["AI"]*len(AI_raw) + ["REAL"]*len(REAL_raw),
}).sample(frac=1.0, random_state=SEED).reset_index(drop=True)

raw_manifest = os.path.join(RUN_DIR, f"raw_manifest_AI{AI_RAW_EACH}_REAL{REAL_RAW_EACH}.csv")
raw_df.to_csv(raw_manifest, index=False)
print("\nRAW pool:", len(raw_df), "total | AI=", (raw_df.y==1).sum(), "REAL=", (raw_df.y==0).sum())
print("Saved:", raw_manifest)

# -----------------------------
# 3) HASHING + DEDUP
# -----------------------------
def get_first_frame(path, size=(128,128)):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        cap.release()
        return None
    ok, frame = cap.read()
    cap.release()
    if not ok or frame is None:
        return None
    return cv2.resize(frame, size, interpolation=cv2.INTER_AREA)

def sha1_first_frame(path):
    fr = get_first_frame(path)
    if fr is None:
        return None
    return hashlib.sha1(fr.tobytes()).hexdigest()

def dhash_first_frame(path, hash_size=8):
    fr = get_first_frame(path, size=(hash_size+1, hash_size))
    if fr is None:
        return None
    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)
    diff = gray[:,1:] > gray[:,:-1]
    h = 0
    for b in diff.flatten():
        h = (h << 1) | int(b)
    return h

def hamming(a, b):
    return (a ^ b).bit_count()

def uf_make(n):
    parent = list(range(n))
    rank = [0]*n
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(a,b):
        ra, rb = find(a), find(b)
        if ra == rb: return
        if rank[ra] < rank[rb]:
            parent[ra] = rb
        elif rank[ra] > rank[rb]:
            parent[rb] = ra
        else:
            parent[rb] = ra
            rank[ra] += 1
    return find, union

def bucket_key(h, bits=12):
    return int(h) >> (64 - bits)

print("\nComputing hashes for dedup...")
sha_list, dh_list = [], []
for p in tqdm(raw_df.path.tolist(), desc="Hashing"):
    sha_list.append(sha1_first_frame(p))
    dh_list.append(dhash_first_frame(p))

df = raw_df.copy()
df["sha1"] = sha_list
df["dhash"] = dh_list
df = df.dropna(subset=["sha1","dhash"]).reset_index(drop=True)

before = len(df)
df = df.drop_duplicates(subset=["sha1"]).reset_index(drop=True)
print("Exact dupes removed:", before - len(df))

bucket = defaultdict(list)
for i, h in enumerate(df.dhash.values):
    bucket[bucket_key(h)].append(i)

find, union = uf_make(len(df))
for _, idxs in bucket.items():
    if len(idxs) < 2:
        continue
    if len(idxs) > 350:
        idxs = random.sample(idxs, 350)
    for ii in range(len(idxs)):
        a = idxs[ii]
        ha = int(df.loc[a,"dhash"])
        for jj in range(ii+1, len(idxs)):
            b = idxs[jj]
            hb = int(df.loc[b,"dhash"])
            if hamming(ha, hb) <= NEAR_DUP_HAMMING_MAX:
                union(a, b)

df["cluster"] = [find(i) for i in range(len(df))]
before = len(df)
df = df.drop_duplicates(subset=["cluster"]).reset_index(drop=True)
print("Near-dupes removed:", before - len(df))
print("After dedup:", len(df))

# Rebalance to exact N_EACH, but if dedup removed too many REAL/AI,
# fall back to the maximum possible common size.
df_ai = df[df.y==1]
df_rl = df[df.y==0]
N_EACH_FINAL = min(N_EACH, len(df_ai), len(df_rl))
if N_EACH_FINAL < N_EACH:
    print(f"[WARN] After dedup, cannot keep N_EACH={N_EACH}. Using N_EACH_FINAL={N_EACH_FINAL}.")

df_ai = df_ai.sample(n=N_EACH_FINAL, random_state=SEED)
df_rl = df_rl.sample(n=N_EACH_FINAL, random_state=SEED)
df = pd.concat([df_ai, df_rl], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)

dedup_manifest = os.path.join(RUN_DIR, "dedup_manifest.csv")
df.to_csv(dedup_manifest, index=False)
print("Rebalanced:", len(df), "total | AI=", (df.y==1).sum(), "REAL=", (df.y==0).sum())
print("Saved:", dedup_manifest)

# -----------------------------
# 4) NORMALIZE VIDEOS (metadata-neutral)
# -----------------------------
NORM_DIR = os.path.join(RUN_DIR, "normalized")
os.makedirs(NORM_DIR, exist_ok=True)

def norm_path_for(i):
    return os.path.join(NORM_DIR, f"v_{i:05d}.mp4")

def ffmpeg_normalize(in_path, out_path):
    aflag = ["-an"] if not NORM_AUDIO else []
    cmd = [
        "ffmpeg","-y","-hide_banner","-loglevel","error",
        "-i", in_path,
        "-vf", f"scale={NORM_W}:{NORM_H}:force_original_aspect_ratio=decrease,"
               f"pad={NORM_W}:{NORM_H}:(ow-iw)/2:(oh-ih)/2,"
               f"fps={NORM_FPS}",
        "-vsync","cfr",
        *aflag,
        "-c:v", NORM_CODEC,
        "-crf", str(NORM_CRF),
        "-pix_fmt", NORM_PIX,
        out_path
    ]
    subprocess.run(cmd, check=False)

print("\nNormalizing videos...")
norm_paths = []
for i, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc="Normalize")):
    outp = norm_path_for(i)
    if not os.path.exists(outp):
        ffmpeg_normalize(row.path, outp)
    norm_paths.append(outp if (os.path.exists(outp) and os.path.getsize(outp) > 1000) else None)

ndf = df.copy()
ndf["norm_path"] = norm_paths
ndf = ndf.dropna(subset=["norm_path"]).reset_index(drop=True)

norm_manifest = os.path.join(RUN_DIR, "normalized_manifest.csv")
ndf.to_csv(norm_manifest, index=False)
print("Normalized:", len(ndf), "Saved:", norm_manifest)

# -----------------------------
# 5) CLUSTER-SAFE SPLIT (video-level)
# -----------------------------
clusters = ndf.cluster.values
cluster_to_rows = defaultdict(list)
for i,c in enumerate(clusters):
    cluster_to_rows[int(c)].append(i)

cluster_ids = list(cluster_to_rows.keys())
cluster_labels = []
for c in cluster_ids:
    ys = [int(ndf.y.iloc[i]) for i in cluster_to_rows[c]]
    cluster_labels.append(1 if sum(ys) >= (len(ys)/2) else 0)

c_train, c_test = train_test_split(
    cluster_ids, test_size=0.30, random_state=SEED, stratify=cluster_labels
)

train_vid_idx, test_vid_idx = [], []
for c in c_train: train_vid_idx.extend(cluster_to_rows[c])
for c in c_test:  test_vid_idx.extend(cluster_to_rows[c])

train_videos = ndf.iloc[train_vid_idx].reset_index(drop=True)
test_videos  = ndf.iloc[test_vid_idx].reset_index(drop=True)

print("\nCluster-safe split (video-level):")
print(" Train videos:", len(train_videos), " Test videos:", len(test_videos))
print(" Train AI/REAL:", int((train_videos.y==1).sum()), "/", int((train_videos.y==0).sum()))
print(" Test  AI/REAL:", int((test_videos.y==1).sum()), "/", int((test_videos.y==0).sum()))

train_videos.to_csv(os.path.join(RUN_DIR, "train_videos.csv"), index=False)
test_videos.to_csv(os.path.join(RUN_DIR, "test_videos.csv"), index=False)

# -----------------------------
# 6) FIXED CLIP PLAN
# -----------------------------
def get_frame_count(path):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        cap.release()
        return 0
    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    cap.release()
    return n

def make_clip_plan(videos_df, clips_per_video, T, stride, seed):
    rng = random.Random(seed)
    rows = []
    for r in videos_df.itertuples(index=False):
        path = r.norm_path
        n = get_frame_count(path)
        max_start = max(0, n - (T-1)*stride - 1)
        for _ in range(clips_per_video):
            start = rng.randint(0, max_start) if max_start > 0 else 0
            rows.append({
                "norm_path": path,
                "y": int(r.y),
                "src": r.src,
                "cluster": int(r.cluster),
                "start": int(start),
                "T": int(T),
                "stride": int(stride),
            })
    return pd.DataFrame(rows)

clip_train = make_clip_plan(train_videos, TRAIN_CLIPS_PER_VIDEO, T, FRAME_STRIDE, seed=SEED+11)
clip_test  = make_clip_plan(test_videos,  TEST_CLIPS_PER_VIDEO,  T, FRAME_STRIDE, seed=SEED+99)

clip_train_path = os.path.join(RUN_DIR, "clip_plan_train.csv")
clip_test_path  = os.path.join(RUN_DIR, "clip_plan_test.csv")
clip_train.to_csv(clip_train_path, index=False)
clip_test.to_csv(clip_test_path, index=False)

print("\nSaved fixed clip plans:")
print(" Train:", clip_train_path, "| clips:", len(clip_train))
print(" Test :", clip_test_path,  "| clips:", len(clip_test))

# -----------------------------
# 7) DATASET (mid stream only)
# -----------------------------
def resize_center_bgr(frame_bgr, size):
    return cv2.resize(frame_bgr, (size, size), interpolation=cv2.INTER_AREA)

def to_rgb(frame_bgr):
    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)

def read_frames_fixed(path, start, T, stride, size):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        cap.release()
        return None
    frames = []
    for i in range(T):
        idx = start + i*stride
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ok, fr = cap.read()
        if not ok or fr is None:
            cap.release()
            return None
        fr = resize_center_bgr(fr, size)
        frames.append(fr)
    cap.release()
    return frames

class FixedClipDataset(torch.utils.data.Dataset):
    def __init__(self, clip_df, size=224):
        self.df = clip_df.reset_index(drop=True)
        self.size = size

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        r = self.df.iloc[idx]
        frames = read_frames_fixed(
            r["norm_path"],
            int(r["start"]),
            T=int(r["T"]),
            stride=int(r["stride"]),
            size=self.size
        )
        y = int(r["y"])
        if frames is None:
            x = np.zeros((int(r["T"]), 3, self.size, self.size), dtype=np.float32)
            return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)

        arr = np.stack([to_rgb(fr) for fr in frames], axis=0).astype(np.float32) / 255.0
        arr = np.transpose(arr, (0,3,1,2))  # T,C,H,W
        return torch.from_numpy(arr), torch.tensor(y, dtype=torch.long)

def make_loader(ds, shuffle):
    return DataLoader(
        ds,
        batch_size=BATCH_SIZE,
        shuffle=shuffle,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        drop_last=False
    )

train_loader = make_loader(FixedClipDataset(clip_train, TARGET_SIZE), shuffle=True)
test_loader  = make_loader(FixedClipDataset(clip_test,  TARGET_SIZE), shuffle=False)

# -----------------------------
# 8) MODEL (R2+1D + ResNet18 fusion + MLP)
# -----------------------------
class R2ResNetFusion_MLP(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.r2 = tvm.video.r2plus1d_18(weights=tvm.video.R2Plus1D_18_Weights.DEFAULT)
        self.r2.fc = nn.Identity()

        self.res = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)
        self.res.fc = nn.Identity()

        self.head = nn.Sequential(
            nn.Linear(512 + 512, 256),
            nn.ReLU(),
            nn.Dropout(0.25),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        B,T,C,H,W = x.shape
        xr2 = x.permute(0,2,1,3,4)
        f_r2 = self.r2(xr2)  # B,512

        xf = x.reshape(B*T, C, H, W)
        f = self.res(xf).view(B, T, -1).mean(dim=1)  # B,512

        feat = torch.cat([f_r2, f], dim=1)
        return self.head(feat)

# -----------------------------
# 9) METRICS + PLOTS
# -----------------------------
def compute_metrics(y_true, proba):
    pred = (proba >= 0.5).astype(int)
    return {
        "acc": float(accuracy_score(y_true, pred)),
        "f1": float(f1_score(y_true, pred)),
        "roc_auc": float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float("nan"),
        "ap": float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float("nan"),
    }

@torch.no_grad()
def eval_model(model, loader):
    model.eval()
    all_y, all_p = [], []
    for x, y in loader:
        x = x.to(DEVICE)
        logits = model(x)
        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()
        all_y.append(y.numpy()); all_p.append(p)
    y_true = np.concatenate(all_y, axis=0)
    proba  = np.concatenate(all_p, axis=0)
    return y_true, proba, compute_metrics(y_true, proba)

def save_pr_roc_plots(y_true, proba, out_dir, prefix="best_test"):
    fpr, tpr, _ = roc_curve(y_true, proba)
    plt.figure(); plt.plot(fpr, tpr)
    plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title(f"ROC ({prefix})")
    roc_path = os.path.join(out_dir, f"{prefix}_roc.png")
    plt.savefig(roc_path, dpi=200, bbox_inches="tight"); plt.close()

    prec, rec, _ = precision_recall_curve(y_true, proba)
    plt.figure(); plt.plot(rec, prec)
    plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title(f"PR ({prefix})")
    pr_path = os.path.join(out_dir, f"{prefix}_pr.png")
    plt.savefig(pr_path, dpi=200, bbox_inches="tight"); plt.close()
    return roc_path, pr_path

def save_confusion(y_true, proba, out_dir, prefix="best_test"):
    pred = (proba >= 0.5).astype(int)
    cm = confusion_matrix(y_true, pred)
    plt.figure(); plt.imshow(cm, interpolation="nearest"); plt.title(f"CM ({prefix})")
    plt.colorbar()
    plt.xticks([0,1], ["REAL","AI"]); plt.yticks([0,1], ["REAL","AI"])
    for i in range(2):
        for j in range(2):
            plt.text(j, i, str(cm[i,j]), ha="center", va="center")
    plt.xlabel("Pred"); plt.ylabel("True")
    cm_path = os.path.join(out_dir, f"{prefix}_confusion.png")
    plt.savefig(cm_path, dpi=200, bbox_inches="tight"); plt.close()
    return cm_path

def save_calibration_plot(y_true, proba, out_dir, prefix="best_test", bins=10):
    proba = np.asarray(proba); y_true = np.asarray(y_true)
    edges = np.linspace(0,1,bins+1)
    bin_ids = np.clip(np.digitize(proba, edges)-1, 0, bins-1)
    conf, acc = [], []
    for b in range(bins):
        m = (bin_ids==b)
        if m.sum()==0:
            conf.append(np.nan); acc.append(np.nan)
        else:
            conf.append(proba[m].mean()); acc.append(y_true[m].mean())
    plt.figure()
    plt.plot([0,1],[0,1])
    plt.scatter(conf, acc)
    plt.xlabel("Mean predicted prob"); plt.ylabel("Empirical positives")
    plt.title(f"Calibration ({prefix})")
    cal_path = os.path.join(out_dir, f"{prefix}_calibration.png")
    plt.savefig(cal_path, dpi=200, bbox_inches="tight"); plt.close()
    return cal_path, float(brier_score_loss(y_true, proba))

# -----------------------------
# 10) TRAIN LOOP
# -----------------------------
def train_main(model, train_loader, test_loader, epochs, name):
    model = model.to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    ckpt_dir = os.path.join(RUN_DIR, "checkpoints")
    os.makedirs(ckpt_dir, exist_ok=True)
    plot_dir = os.path.join(RUN_DIR, "plots")
    os.makedirs(plot_dir, exist_ok=True)

    history = []
    best_score = -1e9
    best_state = None
    best_epoch = -1

    for ep in range(1, epochs+1):
        model.train()
        losses = []
        for x, y in train_loader:
            x = x.to(DEVICE); y = y.to(DEVICE)
            opt.zero_grad(set_to_none=True)
            logits = model(x)
            loss = F.cross_entropy(logits, y)
            loss.backward()
            opt.step()
            losses.append(loss.item())

        y_true, proba, m = eval_model(model, test_loader)
        row = {"epoch": ep, "train_loss": float(np.mean(losses)), **m}
        history.append(row)

        print(f"{name} | ep {ep}/{epochs} | loss={row['train_loss']:.4f} | "
              f"test_f1={row['f1']:.3f} auc={row['roc_auc']:.3f} acc={row['acc']:.3f} ap={row['ap']:.3f}")

        if SAVE_EVERY_EPOCH:
            torch.save({
                "epoch": ep,
                "model_state": model.state_dict(),
                "opt_state": opt.state_dict(),
                "metrics": row,
            }, os.path.join(ckpt_dir, f"{name}_epoch{ep:03d}.pt"))

        score = row[BEST_ON]
        if score > best_score:
            best_score = score
            best_epoch = ep
            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}

    if best_state is not None:
        model.load_state_dict(best_state)

    y_true, proba, final_m = eval_model(model, test_loader)

    best_path = os.path.join(RUN_DIR, f"{name}_BEST.pt")
    torch.save({
        "best_epoch": best_epoch,
        "model_state": model.state_dict(),
        "final_metrics": final_m,
        "history": history,
        "manifests": {
            "normalized_manifest": norm_manifest,
            "clip_train": clip_train_path,
            "clip_test": clip_test_path
        }
    }, best_path)

    hist_df = pd.DataFrame(history)
    hist_csv = os.path.join(RUN_DIR, f"{name}_history.csv")
    hist_df.to_csv(hist_csv, index=False)

    # plots
    plt.figure(); plt.plot(hist_df["epoch"], hist_df["train_loss"])
    plt.xlabel("Epoch"); plt.ylabel("Train loss"); plt.title("Train loss")
    plt.savefig(os.path.join(plot_dir, "train_loss.png"), dpi=200, bbox_inches="tight"); plt.close()

    for k in ["acc","f1","roc_auc","ap"]:
        plt.figure(); plt.plot(hist_df["epoch"], hist_df[k])
        plt.xlabel("Epoch"); plt.ylabel(k); plt.title(f"Test {k}")
        plt.savefig(os.path.join(plot_dir, f"test_{k}.png"), dpi=200, bbox_inches="tight"); plt.close()

    roc_path, pr_path = save_pr_roc_plots(y_true, proba, plot_dir, "best_test")
    cm_path = save_confusion(y_true, proba, plot_dir, "best_test")
    cal_path, brier = save_calibration_plot(y_true, proba, plot_dir, "best_test")

    final_metrics_path = os.path.join(RUN_DIR, f"{name}_final_metrics.json")
    with open(final_metrics_path, "w") as f:
        json.dump({**final_m, "best_epoch": best_epoch, "brier": brier}, f, indent=2)

    print("\nSaved BEST:", best_path)
    print("Saved history:", hist_csv)
    print("Saved final metrics:", final_metrics_path)
    print("Saved plots in:", plot_dir)
    print(" -", roc_path)
    print(" -", pr_path)
    print(" -", cm_path)
    print(" -", cal_path)

    return model, final_m, best_epoch

# -----------------------------
# 11) RUN PAPER TRAINING
# -----------------------------
model = R2ResNetFusion_MLP()
trained_model, final_metrics, best_epoch = train_main(
    model,
    train_loader,
    test_loader,
    epochs=EPOCHS,
    name="R2Res_mid_MLP_FULLDATA"
)

print("\n==== FINAL (BEST) TEST METRICS ====")
print("Best epoch:", best_epoch)
print(final_metrics)
print("\nRun folder:", RUN_DIR)